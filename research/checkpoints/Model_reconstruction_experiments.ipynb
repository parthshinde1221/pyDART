{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# PyDart Library â€“ Third Testing Checkpoint\n",
        "\n",
        "## Overview\n",
        "\n",
        "In this checkpoint, I focused on reconstructing the DAG, a critical milestone in the development of PyDart. Success at this stage was essential for validating the correctness of the DAG-based execution approach.\n",
        "\n",
        "## Main Contributions\n",
        "\n",
        "1. **Exploring DAG Reconstruction**  \n",
        "   - Investigated the `torch.fx` module for extracting computational graphs.  \n",
        "   - Studied topological sorting algorithms to determine execution order.  \n",
        "\n",
        "2. **Implementing Basic Scheduling**  \n",
        "   - Designed a simple Round-Robin (RR) scheme for stage allocation.  \n",
        "   - Implemented a preliminary grouping mechanism for:  \n",
        "     - Layers (referred to as **modules** in the DAG representation of the DNN forward pass).  \n",
        "     - Functions (operators).  \n",
        "     - I grouped them using a constant.(experimented with this as well.)\n",
        "\n",
        "3. **Validating DAG-Based Execution**  \n",
        "   - Successfully reconstructed the graph using topological sorting.  \n",
        "   - Verified correct node allocation within the execution framework.  \n",
        "\n",
        "4. **Output Matching and Evaluation**  \n",
        "   - Compared outputs against a standard native PyTorch implementation of the `Evaluator` class.  \n",
        "   - Confirmed that a DAG-based approach would preserve correctness.  \n",
        "\n",
        "## Key Insights and Next Steps\n",
        "\n",
        "- This approach  **confirmed that DAG-based execution would produce correct outputs**, reinforcing the feasibility of this methodology for future iterations.  \n",
        "- This validation was crucial for refining DAG-based scheduling and improving execution efficiency in subsequent notebooks.\n",
        "\n",
        "---\n",
        "\n",
        "**Note**: Multiple iterations were performed during the development of these classes. The key checkpoints included here highlight the most significant developments. Subsequent iterations followed a similar approach.\n"
      ],
      "metadata": {
        "id": "lysdAHcradzz"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UtM6jSY_qyYM",
        "outputId": "7d62c83f-16a6-45c2-8565-0c2beb5c13db"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "============================================================\n",
            "Tracing and Extracting Information for SimpleCNN\n",
            "============================================================\n",
            "\n",
            "Topological Execution Order:\n",
            "1: x\n",
            "2: conv1\n",
            "3: relu\n",
            "4: conv2\n",
            "5: relu_1\n",
            "6: cat\n",
            "7: flatten\n",
            "8: fc\n",
            "9: output\n",
            "\n",
            "Dependency List:\n",
            "x: []\n",
            "conv1: ['x']\n",
            "relu: ['conv1']\n",
            "conv2: ['x']\n",
            "relu_1: ['conv2']\n",
            "cat: ['relu', 'relu_1']\n",
            "flatten: ['cat']\n",
            "fc: ['flatten']\n",
            "output: ['fc']\n",
            "\n",
            "============================================================\n",
            "Tracing and Extracting Information for ResNet18\n",
            "============================================================\n",
            "\n",
            "Topological Execution Order:\n",
            "1: x\n",
            "2: conv1\n",
            "3: bn1\n",
            "4: relu\n",
            "5: maxpool\n",
            "6: layer1_0_conv1\n",
            "7: layer1_0_bn1\n",
            "8: layer1_0_relu\n",
            "9: layer1_0_conv2\n",
            "10: layer1_0_bn2\n",
            "11: add\n",
            "12: layer1_0_relu_1\n",
            "13: layer1_1_conv1\n",
            "14: layer1_1_bn1\n",
            "15: layer1_1_relu\n",
            "16: layer1_1_conv2\n",
            "17: layer1_1_bn2\n",
            "18: add_1\n",
            "19: layer1_1_relu_1\n",
            "20: layer2_0_conv1\n",
            "21: layer2_0_bn1\n",
            "22: layer2_0_relu\n",
            "23: layer2_0_conv2\n",
            "24: layer2_0_bn2\n",
            "25: layer2_0_downsample_0\n",
            "26: layer2_0_downsample_1\n",
            "27: add_2\n",
            "28: layer2_0_relu_1\n",
            "29: layer2_1_conv1\n",
            "30: layer2_1_bn1\n",
            "31: layer2_1_relu\n",
            "32: layer2_1_conv2\n",
            "33: layer2_1_bn2\n",
            "34: add_3\n",
            "35: layer2_1_relu_1\n",
            "36: layer3_0_conv1\n",
            "37: layer3_0_bn1\n",
            "38: layer3_0_relu\n",
            "39: layer3_0_conv2\n",
            "40: layer3_0_bn2\n",
            "41: layer3_0_downsample_0\n",
            "42: layer3_0_downsample_1\n",
            "43: add_4\n",
            "44: layer3_0_relu_1\n",
            "45: layer3_1_conv1\n",
            "46: layer3_1_bn1\n",
            "47: layer3_1_relu\n",
            "48: layer3_1_conv2\n",
            "49: layer3_1_bn2\n",
            "50: add_5\n",
            "51: layer3_1_relu_1\n",
            "52: layer4_0_conv1\n",
            "53: layer4_0_bn1\n",
            "54: layer4_0_relu\n",
            "55: layer4_0_conv2\n",
            "56: layer4_0_bn2\n",
            "57: layer4_0_downsample_0\n",
            "58: layer4_0_downsample_1\n",
            "59: add_6\n",
            "60: layer4_0_relu_1\n",
            "61: layer4_1_conv1\n",
            "62: layer4_1_bn1\n",
            "63: layer4_1_relu\n",
            "64: layer4_1_conv2\n",
            "65: layer4_1_bn2\n",
            "66: add_7\n",
            "67: layer4_1_relu_1\n",
            "68: avgpool\n",
            "69: flatten\n",
            "70: fc\n",
            "71: output\n",
            "\n",
            "Dependency List:\n",
            "x: []\n",
            "conv1: ['x']\n",
            "bn1: ['conv1']\n",
            "relu: ['bn1']\n",
            "maxpool: ['relu']\n",
            "layer1_0_conv1: ['maxpool']\n",
            "layer1_0_bn1: ['layer1_0_conv1']\n",
            "layer1_0_relu: ['layer1_0_bn1']\n",
            "layer1_0_conv2: ['layer1_0_relu']\n",
            "layer1_0_bn2: ['layer1_0_conv2']\n",
            "add: ['layer1_0_bn2', 'maxpool']\n",
            "layer1_0_relu_1: ['add']\n",
            "layer1_1_conv1: ['layer1_0_relu_1']\n",
            "layer1_1_bn1: ['layer1_1_conv1']\n",
            "layer1_1_relu: ['layer1_1_bn1']\n",
            "layer1_1_conv2: ['layer1_1_relu']\n",
            "layer1_1_bn2: ['layer1_1_conv2']\n",
            "add_1: ['layer1_1_bn2', 'layer1_0_relu_1']\n",
            "layer1_1_relu_1: ['add_1']\n",
            "layer2_0_conv1: ['layer1_1_relu_1']\n",
            "layer2_0_bn1: ['layer2_0_conv1']\n",
            "layer2_0_relu: ['layer2_0_bn1']\n",
            "layer2_0_conv2: ['layer2_0_relu']\n",
            "layer2_0_bn2: ['layer2_0_conv2']\n",
            "layer2_0_downsample_0: ['layer1_1_relu_1']\n",
            "layer2_0_downsample_1: ['layer2_0_downsample_0']\n",
            "add_2: ['layer2_0_bn2', 'layer2_0_downsample_1']\n",
            "layer2_0_relu_1: ['add_2']\n",
            "layer2_1_conv1: ['layer2_0_relu_1']\n",
            "layer2_1_bn1: ['layer2_1_conv1']\n",
            "layer2_1_relu: ['layer2_1_bn1']\n",
            "layer2_1_conv2: ['layer2_1_relu']\n",
            "layer2_1_bn2: ['layer2_1_conv2']\n",
            "add_3: ['layer2_1_bn2', 'layer2_0_relu_1']\n",
            "layer2_1_relu_1: ['add_3']\n",
            "layer3_0_conv1: ['layer2_1_relu_1']\n",
            "layer3_0_bn1: ['layer3_0_conv1']\n",
            "layer3_0_relu: ['layer3_0_bn1']\n",
            "layer3_0_conv2: ['layer3_0_relu']\n",
            "layer3_0_bn2: ['layer3_0_conv2']\n",
            "layer3_0_downsample_0: ['layer2_1_relu_1']\n",
            "layer3_0_downsample_1: ['layer3_0_downsample_0']\n",
            "add_4: ['layer3_0_bn2', 'layer3_0_downsample_1']\n",
            "layer3_0_relu_1: ['add_4']\n",
            "layer3_1_conv1: ['layer3_0_relu_1']\n",
            "layer3_1_bn1: ['layer3_1_conv1']\n",
            "layer3_1_relu: ['layer3_1_bn1']\n",
            "layer3_1_conv2: ['layer3_1_relu']\n",
            "layer3_1_bn2: ['layer3_1_conv2']\n",
            "add_5: ['layer3_1_bn2', 'layer3_0_relu_1']\n",
            "layer3_1_relu_1: ['add_5']\n",
            "layer4_0_conv1: ['layer3_1_relu_1']\n",
            "layer4_0_bn1: ['layer4_0_conv1']\n",
            "layer4_0_relu: ['layer4_0_bn1']\n",
            "layer4_0_conv2: ['layer4_0_relu']\n",
            "layer4_0_bn2: ['layer4_0_conv2']\n",
            "layer4_0_downsample_0: ['layer3_1_relu_1']\n",
            "layer4_0_downsample_1: ['layer4_0_downsample_0']\n",
            "add_6: ['layer4_0_bn2', 'layer4_0_downsample_1']\n",
            "layer4_0_relu_1: ['add_6']\n",
            "layer4_1_conv1: ['layer4_0_relu_1']\n",
            "layer4_1_bn1: ['layer4_1_conv1']\n",
            "layer4_1_relu: ['layer4_1_bn1']\n",
            "layer4_1_conv2: ['layer4_1_relu']\n",
            "layer4_1_bn2: ['layer4_1_conv2']\n",
            "add_7: ['layer4_1_bn2', 'layer4_0_relu_1']\n",
            "layer4_1_relu_1: ['add_7']\n",
            "avgpool: ['layer4_1_relu_1']\n",
            "flatten: ['avgpool']\n",
            "fc: ['flatten']\n",
            "output: ['fc']\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.fx import symbolic_trace\n",
        "from torchvision import models\n",
        "\n",
        "# Define a SimpleCNN with torch.cat in its forward pass\n",
        "class SimpleCNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(SimpleCNN, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=16, kernel_size=3, padding=1)\n",
        "        self.conv2 = nn.Conv2d(in_channels=3, out_channels=16, kernel_size=5, padding=2)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.flatten = nn.Flatten()\n",
        "        self.fc = nn.Linear(32 * 28 * 28, 10)  # Assuming input images are 28x28\n",
        "\n",
        "    def forward(self, x):\n",
        "        # First path\n",
        "        x1 = self.conv1(x)\n",
        "        x1 = self.relu(x1)\n",
        "\n",
        "        # Second path\n",
        "        x2 = self.conv2(x)\n",
        "        x2 = self.relu(x2)\n",
        "\n",
        "        # Concatenate along the channel dimension\n",
        "        x = torch.cat((x1, x2), dim=1)\n",
        "\n",
        "        x = self.flatten(x)\n",
        "        x = self.fc(x)\n",
        "        return x\n",
        "\n",
        "def trace_and_extract_info(model, model_name=\"Model\"):\n",
        "    \"\"\"\n",
        "    Traces the given model using torch.fx and extracts:\n",
        "    - Topological Execution Order\n",
        "    - Dependency List\n",
        "    \"\"\"\n",
        "    print(f\"\\n{'='*60}\\nTracing and Extracting Information for {model_name}\\n{'='*60}\")\n",
        "\n",
        "    # Trace the model\n",
        "    traced_model = symbolic_trace(model)\n",
        "\n",
        "    # Extract Topological Order (execution order)\n",
        "    topological_order = []\n",
        "    for node in traced_model.graph.nodes:\n",
        "        topological_order.append(node.name)\n",
        "\n",
        "    # Extract Dependency List\n",
        "    dependency_list = {}\n",
        "    for node in traced_model.graph.nodes:\n",
        "        dependencies = [arg.name for arg in node.all_input_nodes]\n",
        "        dependency_list[node.name] = dependencies\n",
        "\n",
        "    # Print Topological Execution Order\n",
        "    print(\"\\nTopological Execution Order:\")\n",
        "    for idx, node_name in enumerate(topological_order, 1):\n",
        "        print(f\"{idx}: {node_name}\")\n",
        "\n",
        "    # Print Dependency List\n",
        "    print(\"\\nDependency List:\")\n",
        "    for node_name in topological_order:\n",
        "        deps = dependency_list[node_name]\n",
        "        print(f\"{node_name}: {deps}\")\n",
        "\n",
        "def main():\n",
        "    # Instantiate the SimpleCNN\n",
        "    simple_cnn = SimpleCNN()\n",
        "    simple_cnn.eval()  # Set to evaluation mode\n",
        "\n",
        "    # Instantiate the ResNet18 model from torchvision\n",
        "    resnet18 = models.resnet18(pretrained=False)  # Set pretrained=True if you want pretrained weights\n",
        "    resnet18.eval()  # Set to evaluation mode\n",
        "\n",
        "    # Trace and extract information for SimpleCNN\n",
        "    trace_and_extract_info(simple_cnn, model_name=\"SimpleCNN\")\n",
        "\n",
        "    # Trace and extract information for ResNet18\n",
        "    trace_and_extract_info(resnet18, model_name=\"ResNet18\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.fx import symbolic_trace\n",
        "from torchvision import models\n",
        "import warnings\n",
        "\n",
        "# Suppress specific deprecation warnings from torchvision\n",
        "warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"torchvision.models._utils\")\n",
        "\n",
        "# Define a SimpleCNN with torch.cat in its forward pass\n",
        "class SimpleCNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(SimpleCNN, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=16, kernel_size=3, padding=1)\n",
        "        self.conv2 = nn.Conv2d(in_channels=3, out_channels=16, kernel_size=5, padding=2)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.flatten = nn.Flatten()\n",
        "        self.fc = nn.Linear(32 * 28 * 28, 10)  # Assuming input images are 28x28\n",
        "\n",
        "    def forward(self, x):\n",
        "        # First path\n",
        "        x1 = self.conv1(x)\n",
        "        x1 = self.relu(x1)\n",
        "\n",
        "        # Second path\n",
        "        x2 = self.conv2(x)\n",
        "        x2 = self.relu(x2)\n",
        "\n",
        "        # Concatenate along the channel dimension\n",
        "        x = torch.cat((x1, x2), dim=1)\n",
        "\n",
        "        x = self.flatten(x)\n",
        "        x = self.fc(x)\n",
        "        return x\n",
        "\n",
        "def trace_and_extract_info(model, model_name=\"Model\"):\n",
        "    \"\"\"\n",
        "    Traces the given model using torch.fx and extracts:\n",
        "    - Topological Execution Order\n",
        "    - Dependency List\n",
        "    - Interaction Details: How outputs of dependencies are used as inputs\n",
        "    \"\"\"\n",
        "    print(f\"\\n{'='*60}\\nTracing and Extracting Information for {model_name}\\n{'='*60}\")\n",
        "\n",
        "    # Trace the model\n",
        "    traced_model = symbolic_trace(model)\n",
        "\n",
        "    # Extract Topological Order (execution order)\n",
        "    topological_order = [node.name for node in traced_model.graph.nodes]\n",
        "\n",
        "    # Extract Dependency List\n",
        "    dependency_list = {node.name: [arg.name for arg in node.all_input_nodes] for node in traced_model.graph.nodes}\n",
        "\n",
        "    # Extract Interaction Details\n",
        "    interaction_details = {node.name: [arg.name for arg in node.all_input_nodes] for node in traced_model.graph.nodes}\n",
        "\n",
        "    # Print Topological Execution Order\n",
        "    print(\"\\nTopological Execution Order:\")\n",
        "    for idx, node_name in enumerate(topological_order, 1):\n",
        "        print(f\"{idx}: {node_name}\")\n",
        "\n",
        "    # Print Dependency List\n",
        "    print(\"\\nDependency List:\")\n",
        "    for node_name in topological_order:\n",
        "        deps = dependency_list[node_name]\n",
        "        print(f\"{node_name}: {deps}\")\n",
        "\n",
        "    # Print Interaction Details\n",
        "    print(\"\\nInteraction Details:\")\n",
        "    for node_name in topological_order:\n",
        "        inputs = interaction_details[node_name]\n",
        "        if inputs:\n",
        "            print(f\"{node_name} receives inputs from: {', '.join(inputs)}\")\n",
        "        else:\n",
        "            print(f\"{node_name} receives inputs from: None (Input Placeholder)\")\n",
        "\n",
        "    return traced_model  # Return the traced model for further use\n",
        "\n",
        "def resolve_arg(arg, node_outputs):\n",
        "    \"\"\"\n",
        "    Recursively replaces Node references in args and kwargs with their actual outputs.\n",
        "    \"\"\"\n",
        "    if isinstance(arg, torch.fx.Node):\n",
        "        return node_outputs[arg.name]\n",
        "    elif isinstance(arg, (list, tuple)):\n",
        "        return type(arg)(resolve_arg(a, node_outputs) for a in arg)\n",
        "    elif isinstance(arg, dict):\n",
        "        return {k: resolve_arg(v, node_outputs) for k, v in arg.items()}\n",
        "    else:\n",
        "        return arg\n",
        "\n",
        "def reconstruct_forward(traced_model, input_tensor):\n",
        "    \"\"\"\n",
        "    Reconstructs the forward pass based on the traced graph's topological order and dependencies.\n",
        "    Returns the reconstructed output.\n",
        "    \"\"\"\n",
        "    print(\"\\nReconstructing Forward Pass Based on Topological Order and Dependencies...\\n\")\n",
        "\n",
        "    # Dictionary to hold the outputs of each node\n",
        "    node_outputs = {}\n",
        "\n",
        "    # Iterate through nodes in topological order\n",
        "    for node in traced_model.graph.nodes:\n",
        "        if node.op == 'placeholder':\n",
        "            # Assign the input tensor to the node\n",
        "            node_outputs[node.name] = input_tensor\n",
        "            print(f\"Executing Placeholder: {node.name}\")\n",
        "        elif node.op == 'call_module':\n",
        "            # Get the submodule\n",
        "            submodule = dict(traced_model.named_modules())[node.target]\n",
        "            # Resolve the arguments\n",
        "            args = resolve_arg(node.args, node_outputs)\n",
        "            kwargs = resolve_arg(node.kwargs, node_outputs)\n",
        "            # Execute the submodule\n",
        "            print(f\"Executing Module: {node.target} with inputs {[arg.name for arg in node.all_input_nodes]}\")\n",
        "            node_outputs[node.name] = submodule(*args, **kwargs)\n",
        "        elif node.op == 'call_function':\n",
        "            # Get the function\n",
        "            func = node.target\n",
        "            # Resolve the arguments\n",
        "            args = resolve_arg(node.args, node_outputs)\n",
        "            kwargs = resolve_arg(node.kwargs, node_outputs)\n",
        "            # Execute the function\n",
        "            print(f\"Executing Function: {func.__name__} with inputs {[arg.name for arg in node.all_input_nodes]}\")\n",
        "            node_outputs[node.name] = func(*args, **kwargs)\n",
        "        elif node.op == 'call_method':\n",
        "            # Get the method\n",
        "            method = getattr(resolve_arg(node.args[0], node_outputs), node.target)\n",
        "            # Resolve the arguments (excluding the first argument which is 'self')\n",
        "            args = resolve_arg(node.args[1:], node_outputs)\n",
        "            kwargs = resolve_arg(node.kwargs, node_outputs)\n",
        "            # Execute the method\n",
        "            print(f\"Executing Method: {node.target} on {node.args[0].name} with inputs {[arg.name for arg in node.all_input_nodes[1:]]}\")\n",
        "            node_outputs[node.name] = method(*args, **kwargs)\n",
        "        elif node.op == 'output':\n",
        "            # Assign the final output\n",
        "            output = resolve_arg(node.args[0], node_outputs)\n",
        "            print(f\"Final Output: {node.name}\")\n",
        "            node_outputs[node.name] = output\n",
        "        else:\n",
        "            raise NotImplementedError(f\"Operation {node.op} is not supported.\")\n",
        "\n",
        "    # The output node holds the final output\n",
        "    return node_outputs['output']\n",
        "\n",
        "def main():\n",
        "    # Instantiate the SimpleCNN\n",
        "    simple_cnn = SimpleCNN()\n",
        "    simple_cnn.eval()  # Set to evaluation mode\n",
        "\n",
        "    # Instantiate the ResNet18 model from torchvision\n",
        "    # Note: 'pretrained' is deprecated, use 'weights' instead if using a newer torchvision version\n",
        "    try:\n",
        "        resnet18 = models.resnet18(weights=None)  # Set weights=models.ResNet18_Weights.DEFAULT for pretrained\n",
        "    except TypeError:\n",
        "        # For older torchvision versions\n",
        "        resnet18 = models.resnet18(pretrained=False)\n",
        "    resnet18.eval()  # Set to evaluation mode\n",
        "\n",
        "    # Trace and extract information for SimpleCNN\n",
        "    traced_simple_cnn = trace_and_extract_info(simple_cnn, model_name=\"SimpleCNN\")\n",
        "\n",
        "    # Create a sample input tensor for SimpleCNN\n",
        "    # Assuming input images are 28x28 with 3 channels\n",
        "    simple_cnn_input = torch.randn(1, 3, 28, 28)\n",
        "\n",
        "    # Reconstruct forward pass for SimpleCNN within no_grad context\n",
        "    with torch.no_grad():\n",
        "        reconstructed_simple_cnn_output = reconstruct_forward(traced_simple_cnn, simple_cnn_input)\n",
        "\n",
        "        # Get actual output from SimpleCNN\n",
        "        actual_simple_cnn_output = simple_cnn(simple_cnn_input)\n",
        "\n",
        "    # Compare the outputs\n",
        "    print(\"\\nComparing Reconstructed Output with Actual Output for SimpleCNN:\")\n",
        "    if torch.allclose(reconstructed_simple_cnn_output, actual_simple_cnn_output, atol=1e-6):\n",
        "        print(\"Success: The reconstructed output matches the actual output.\")\n",
        "    else:\n",
        "        print(\"Warning: The reconstructed output does not match the actual output.\")\n",
        "\n",
        "    # Trace and extract information for ResNet18\n",
        "    traced_resnet18 = trace_and_extract_info(resnet18, model_name=\"ResNet18\")\n",
        "\n",
        "    # Create a sample input tensor for ResNet18\n",
        "    # ResNet18 typically expects 224x224 images with 3 channels\n",
        "    resnet18_input = torch.randn(1, 3, 224, 224)\n",
        "\n",
        "    # Reconstruct forward pass for ResNet18 within no_grad context\n",
        "    with torch.no_grad():\n",
        "        reconstructed_resnet18_output = reconstruct_forward(traced_resnet18, resnet18_input)\n",
        "\n",
        "        # Get actual output from ResNet18\n",
        "        actual_resnet18_output = resnet18(resnet18_input)\n",
        "\n",
        "    # Compare the outputs\n",
        "    print(\"\\nComparing Reconstructed Output with Actual Output for ResNet18:\")\n",
        "    if torch.allclose(reconstructed_resnet18_output, actual_resnet18_output, atol=1e-6):\n",
        "        print(\"Success: The reconstructed output matches the actual output.\")\n",
        "    else:\n",
        "        print(\"Warning: The reconstructed output does not match the actual output.\")\n",
        "\n",
        "    print(reconstructed_simple_cnn_output, actual_simple_cnn_output)\n",
        "    print(reconstructed_resnet18_output.shape, actual_resnet18_output.shape)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "id": "syNC5QMeqzwk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f3f8fe51-5206-43f7-fc4d-f9a46e738c3b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "============================================================\n",
            "Tracing and Extracting Information for SimpleCNN\n",
            "============================================================\n",
            "\n",
            "Topological Execution Order:\n",
            "1: x\n",
            "2: conv1\n",
            "3: relu\n",
            "4: conv2\n",
            "5: relu_1\n",
            "6: cat\n",
            "7: flatten\n",
            "8: fc\n",
            "9: output\n",
            "\n",
            "Dependency List:\n",
            "x: []\n",
            "conv1: ['x']\n",
            "relu: ['conv1']\n",
            "conv2: ['x']\n",
            "relu_1: ['conv2']\n",
            "cat: ['relu', 'relu_1']\n",
            "flatten: ['cat']\n",
            "fc: ['flatten']\n",
            "output: ['fc']\n",
            "\n",
            "Interaction Details:\n",
            "x receives inputs from: None (Input Placeholder)\n",
            "conv1 receives inputs from: x\n",
            "relu receives inputs from: conv1\n",
            "conv2 receives inputs from: x\n",
            "relu_1 receives inputs from: conv2\n",
            "cat receives inputs from: relu, relu_1\n",
            "flatten receives inputs from: cat\n",
            "fc receives inputs from: flatten\n",
            "output receives inputs from: fc\n",
            "\n",
            "Reconstructing Forward Pass Based on Topological Order and Dependencies...\n",
            "\n",
            "Executing Placeholder: x\n",
            "Executing Module: conv1 with inputs ['x']\n",
            "Executing Module: relu with inputs ['conv1']\n",
            "Executing Module: conv2 with inputs ['x']\n",
            "Executing Module: relu with inputs ['conv2']\n",
            "Executing Function: cat with inputs ['relu', 'relu_1']\n",
            "Executing Module: flatten with inputs ['cat']\n",
            "Executing Module: fc with inputs ['flatten']\n",
            "Final Output: output\n",
            "\n",
            "Comparing Reconstructed Output with Actual Output for SimpleCNN:\n",
            "Success: The reconstructed output matches the actual output.\n",
            "\n",
            "============================================================\n",
            "Tracing and Extracting Information for ResNet18\n",
            "============================================================\n",
            "\n",
            "Topological Execution Order:\n",
            "1: x\n",
            "2: conv1\n",
            "3: bn1\n",
            "4: relu\n",
            "5: maxpool\n",
            "6: layer1_0_conv1\n",
            "7: layer1_0_bn1\n",
            "8: layer1_0_relu\n",
            "9: layer1_0_conv2\n",
            "10: layer1_0_bn2\n",
            "11: add\n",
            "12: layer1_0_relu_1\n",
            "13: layer1_1_conv1\n",
            "14: layer1_1_bn1\n",
            "15: layer1_1_relu\n",
            "16: layer1_1_conv2\n",
            "17: layer1_1_bn2\n",
            "18: add_1\n",
            "19: layer1_1_relu_1\n",
            "20: layer2_0_conv1\n",
            "21: layer2_0_bn1\n",
            "22: layer2_0_relu\n",
            "23: layer2_0_conv2\n",
            "24: layer2_0_bn2\n",
            "25: layer2_0_downsample_0\n",
            "26: layer2_0_downsample_1\n",
            "27: add_2\n",
            "28: layer2_0_relu_1\n",
            "29: layer2_1_conv1\n",
            "30: layer2_1_bn1\n",
            "31: layer2_1_relu\n",
            "32: layer2_1_conv2\n",
            "33: layer2_1_bn2\n",
            "34: add_3\n",
            "35: layer2_1_relu_1\n",
            "36: layer3_0_conv1\n",
            "37: layer3_0_bn1\n",
            "38: layer3_0_relu\n",
            "39: layer3_0_conv2\n",
            "40: layer3_0_bn2\n",
            "41: layer3_0_downsample_0\n",
            "42: layer3_0_downsample_1\n",
            "43: add_4\n",
            "44: layer3_0_relu_1\n",
            "45: layer3_1_conv1\n",
            "46: layer3_1_bn1\n",
            "47: layer3_1_relu\n",
            "48: layer3_1_conv2\n",
            "49: layer3_1_bn2\n",
            "50: add_5\n",
            "51: layer3_1_relu_1\n",
            "52: layer4_0_conv1\n",
            "53: layer4_0_bn1\n",
            "54: layer4_0_relu\n",
            "55: layer4_0_conv2\n",
            "56: layer4_0_bn2\n",
            "57: layer4_0_downsample_0\n",
            "58: layer4_0_downsample_1\n",
            "59: add_6\n",
            "60: layer4_0_relu_1\n",
            "61: layer4_1_conv1\n",
            "62: layer4_1_bn1\n",
            "63: layer4_1_relu\n",
            "64: layer4_1_conv2\n",
            "65: layer4_1_bn2\n",
            "66: add_7\n",
            "67: layer4_1_relu_1\n",
            "68: avgpool\n",
            "69: flatten\n",
            "70: fc\n",
            "71: output\n",
            "\n",
            "Dependency List:\n",
            "x: []\n",
            "conv1: ['x']\n",
            "bn1: ['conv1']\n",
            "relu: ['bn1']\n",
            "maxpool: ['relu']\n",
            "layer1_0_conv1: ['maxpool']\n",
            "layer1_0_bn1: ['layer1_0_conv1']\n",
            "layer1_0_relu: ['layer1_0_bn1']\n",
            "layer1_0_conv2: ['layer1_0_relu']\n",
            "layer1_0_bn2: ['layer1_0_conv2']\n",
            "add: ['layer1_0_bn2', 'maxpool']\n",
            "layer1_0_relu_1: ['add']\n",
            "layer1_1_conv1: ['layer1_0_relu_1']\n",
            "layer1_1_bn1: ['layer1_1_conv1']\n",
            "layer1_1_relu: ['layer1_1_bn1']\n",
            "layer1_1_conv2: ['layer1_1_relu']\n",
            "layer1_1_bn2: ['layer1_1_conv2']\n",
            "add_1: ['layer1_1_bn2', 'layer1_0_relu_1']\n",
            "layer1_1_relu_1: ['add_1']\n",
            "layer2_0_conv1: ['layer1_1_relu_1']\n",
            "layer2_0_bn1: ['layer2_0_conv1']\n",
            "layer2_0_relu: ['layer2_0_bn1']\n",
            "layer2_0_conv2: ['layer2_0_relu']\n",
            "layer2_0_bn2: ['layer2_0_conv2']\n",
            "layer2_0_downsample_0: ['layer1_1_relu_1']\n",
            "layer2_0_downsample_1: ['layer2_0_downsample_0']\n",
            "add_2: ['layer2_0_bn2', 'layer2_0_downsample_1']\n",
            "layer2_0_relu_1: ['add_2']\n",
            "layer2_1_conv1: ['layer2_0_relu_1']\n",
            "layer2_1_bn1: ['layer2_1_conv1']\n",
            "layer2_1_relu: ['layer2_1_bn1']\n",
            "layer2_1_conv2: ['layer2_1_relu']\n",
            "layer2_1_bn2: ['layer2_1_conv2']\n",
            "add_3: ['layer2_1_bn2', 'layer2_0_relu_1']\n",
            "layer2_1_relu_1: ['add_3']\n",
            "layer3_0_conv1: ['layer2_1_relu_1']\n",
            "layer3_0_bn1: ['layer3_0_conv1']\n",
            "layer3_0_relu: ['layer3_0_bn1']\n",
            "layer3_0_conv2: ['layer3_0_relu']\n",
            "layer3_0_bn2: ['layer3_0_conv2']\n",
            "layer3_0_downsample_0: ['layer2_1_relu_1']\n",
            "layer3_0_downsample_1: ['layer3_0_downsample_0']\n",
            "add_4: ['layer3_0_bn2', 'layer3_0_downsample_1']\n",
            "layer3_0_relu_1: ['add_4']\n",
            "layer3_1_conv1: ['layer3_0_relu_1']\n",
            "layer3_1_bn1: ['layer3_1_conv1']\n",
            "layer3_1_relu: ['layer3_1_bn1']\n",
            "layer3_1_conv2: ['layer3_1_relu']\n",
            "layer3_1_bn2: ['layer3_1_conv2']\n",
            "add_5: ['layer3_1_bn2', 'layer3_0_relu_1']\n",
            "layer3_1_relu_1: ['add_5']\n",
            "layer4_0_conv1: ['layer3_1_relu_1']\n",
            "layer4_0_bn1: ['layer4_0_conv1']\n",
            "layer4_0_relu: ['layer4_0_bn1']\n",
            "layer4_0_conv2: ['layer4_0_relu']\n",
            "layer4_0_bn2: ['layer4_0_conv2']\n",
            "layer4_0_downsample_0: ['layer3_1_relu_1']\n",
            "layer4_0_downsample_1: ['layer4_0_downsample_0']\n",
            "add_6: ['layer4_0_bn2', 'layer4_0_downsample_1']\n",
            "layer4_0_relu_1: ['add_6']\n",
            "layer4_1_conv1: ['layer4_0_relu_1']\n",
            "layer4_1_bn1: ['layer4_1_conv1']\n",
            "layer4_1_relu: ['layer4_1_bn1']\n",
            "layer4_1_conv2: ['layer4_1_relu']\n",
            "layer4_1_bn2: ['layer4_1_conv2']\n",
            "add_7: ['layer4_1_bn2', 'layer4_0_relu_1']\n",
            "layer4_1_relu_1: ['add_7']\n",
            "avgpool: ['layer4_1_relu_1']\n",
            "flatten: ['avgpool']\n",
            "fc: ['flatten']\n",
            "output: ['fc']\n",
            "\n",
            "Interaction Details:\n",
            "x receives inputs from: None (Input Placeholder)\n",
            "conv1 receives inputs from: x\n",
            "bn1 receives inputs from: conv1\n",
            "relu receives inputs from: bn1\n",
            "maxpool receives inputs from: relu\n",
            "layer1_0_conv1 receives inputs from: maxpool\n",
            "layer1_0_bn1 receives inputs from: layer1_0_conv1\n",
            "layer1_0_relu receives inputs from: layer1_0_bn1\n",
            "layer1_0_conv2 receives inputs from: layer1_0_relu\n",
            "layer1_0_bn2 receives inputs from: layer1_0_conv2\n",
            "add receives inputs from: layer1_0_bn2, maxpool\n",
            "layer1_0_relu_1 receives inputs from: add\n",
            "layer1_1_conv1 receives inputs from: layer1_0_relu_1\n",
            "layer1_1_bn1 receives inputs from: layer1_1_conv1\n",
            "layer1_1_relu receives inputs from: layer1_1_bn1\n",
            "layer1_1_conv2 receives inputs from: layer1_1_relu\n",
            "layer1_1_bn2 receives inputs from: layer1_1_conv2\n",
            "add_1 receives inputs from: layer1_1_bn2, layer1_0_relu_1\n",
            "layer1_1_relu_1 receives inputs from: add_1\n",
            "layer2_0_conv1 receives inputs from: layer1_1_relu_1\n",
            "layer2_0_bn1 receives inputs from: layer2_0_conv1\n",
            "layer2_0_relu receives inputs from: layer2_0_bn1\n",
            "layer2_0_conv2 receives inputs from: layer2_0_relu\n",
            "layer2_0_bn2 receives inputs from: layer2_0_conv2\n",
            "layer2_0_downsample_0 receives inputs from: layer1_1_relu_1\n",
            "layer2_0_downsample_1 receives inputs from: layer2_0_downsample_0\n",
            "add_2 receives inputs from: layer2_0_bn2, layer2_0_downsample_1\n",
            "layer2_0_relu_1 receives inputs from: add_2\n",
            "layer2_1_conv1 receives inputs from: layer2_0_relu_1\n",
            "layer2_1_bn1 receives inputs from: layer2_1_conv1\n",
            "layer2_1_relu receives inputs from: layer2_1_bn1\n",
            "layer2_1_conv2 receives inputs from: layer2_1_relu\n",
            "layer2_1_bn2 receives inputs from: layer2_1_conv2\n",
            "add_3 receives inputs from: layer2_1_bn2, layer2_0_relu_1\n",
            "layer2_1_relu_1 receives inputs from: add_3\n",
            "layer3_0_conv1 receives inputs from: layer2_1_relu_1\n",
            "layer3_0_bn1 receives inputs from: layer3_0_conv1\n",
            "layer3_0_relu receives inputs from: layer3_0_bn1\n",
            "layer3_0_conv2 receives inputs from: layer3_0_relu\n",
            "layer3_0_bn2 receives inputs from: layer3_0_conv2\n",
            "layer3_0_downsample_0 receives inputs from: layer2_1_relu_1\n",
            "layer3_0_downsample_1 receives inputs from: layer3_0_downsample_0\n",
            "add_4 receives inputs from: layer3_0_bn2, layer3_0_downsample_1\n",
            "layer3_0_relu_1 receives inputs from: add_4\n",
            "layer3_1_conv1 receives inputs from: layer3_0_relu_1\n",
            "layer3_1_bn1 receives inputs from: layer3_1_conv1\n",
            "layer3_1_relu receives inputs from: layer3_1_bn1\n",
            "layer3_1_conv2 receives inputs from: layer3_1_relu\n",
            "layer3_1_bn2 receives inputs from: layer3_1_conv2\n",
            "add_5 receives inputs from: layer3_1_bn2, layer3_0_relu_1\n",
            "layer3_1_relu_1 receives inputs from: add_5\n",
            "layer4_0_conv1 receives inputs from: layer3_1_relu_1\n",
            "layer4_0_bn1 receives inputs from: layer4_0_conv1\n",
            "layer4_0_relu receives inputs from: layer4_0_bn1\n",
            "layer4_0_conv2 receives inputs from: layer4_0_relu\n",
            "layer4_0_bn2 receives inputs from: layer4_0_conv2\n",
            "layer4_0_downsample_0 receives inputs from: layer3_1_relu_1\n",
            "layer4_0_downsample_1 receives inputs from: layer4_0_downsample_0\n",
            "add_6 receives inputs from: layer4_0_bn2, layer4_0_downsample_1\n",
            "layer4_0_relu_1 receives inputs from: add_6\n",
            "layer4_1_conv1 receives inputs from: layer4_0_relu_1\n",
            "layer4_1_bn1 receives inputs from: layer4_1_conv1\n",
            "layer4_1_relu receives inputs from: layer4_1_bn1\n",
            "layer4_1_conv2 receives inputs from: layer4_1_relu\n",
            "layer4_1_bn2 receives inputs from: layer4_1_conv2\n",
            "add_7 receives inputs from: layer4_1_bn2, layer4_0_relu_1\n",
            "layer4_1_relu_1 receives inputs from: add_7\n",
            "avgpool receives inputs from: layer4_1_relu_1\n",
            "flatten receives inputs from: avgpool\n",
            "fc receives inputs from: flatten\n",
            "output receives inputs from: fc\n",
            "\n",
            "Reconstructing Forward Pass Based on Topological Order and Dependencies...\n",
            "\n",
            "Executing Placeholder: x\n",
            "Executing Module: conv1 with inputs ['x']\n",
            "Executing Module: bn1 with inputs ['conv1']\n",
            "Executing Module: relu with inputs ['bn1']\n",
            "Executing Module: maxpool with inputs ['relu']\n",
            "Executing Module: layer1.0.conv1 with inputs ['maxpool']\n",
            "Executing Module: layer1.0.bn1 with inputs ['layer1_0_conv1']\n",
            "Executing Module: layer1.0.relu with inputs ['layer1_0_bn1']\n",
            "Executing Module: layer1.0.conv2 with inputs ['layer1_0_relu']\n",
            "Executing Module: layer1.0.bn2 with inputs ['layer1_0_conv2']\n",
            "Executing Function: add with inputs ['layer1_0_bn2', 'maxpool']\n",
            "Executing Module: layer1.0.relu with inputs ['add']\n",
            "Executing Module: layer1.1.conv1 with inputs ['layer1_0_relu_1']\n",
            "Executing Module: layer1.1.bn1 with inputs ['layer1_1_conv1']\n",
            "Executing Module: layer1.1.relu with inputs ['layer1_1_bn1']\n",
            "Executing Module: layer1.1.conv2 with inputs ['layer1_1_relu']\n",
            "Executing Module: layer1.1.bn2 with inputs ['layer1_1_conv2']\n",
            "Executing Function: add with inputs ['layer1_1_bn2', 'layer1_0_relu_1']\n",
            "Executing Module: layer1.1.relu with inputs ['add_1']\n",
            "Executing Module: layer2.0.conv1 with inputs ['layer1_1_relu_1']\n",
            "Executing Module: layer2.0.bn1 with inputs ['layer2_0_conv1']\n",
            "Executing Module: layer2.0.relu with inputs ['layer2_0_bn1']\n",
            "Executing Module: layer2.0.conv2 with inputs ['layer2_0_relu']\n",
            "Executing Module: layer2.0.bn2 with inputs ['layer2_0_conv2']\n",
            "Executing Module: layer2.0.downsample.0 with inputs ['layer1_1_relu_1']\n",
            "Executing Module: layer2.0.downsample.1 with inputs ['layer2_0_downsample_0']\n",
            "Executing Function: add with inputs ['layer2_0_bn2', 'layer2_0_downsample_1']\n",
            "Executing Module: layer2.0.relu with inputs ['add_2']\n",
            "Executing Module: layer2.1.conv1 with inputs ['layer2_0_relu_1']\n",
            "Executing Module: layer2.1.bn1 with inputs ['layer2_1_conv1']\n",
            "Executing Module: layer2.1.relu with inputs ['layer2_1_bn1']\n",
            "Executing Module: layer2.1.conv2 with inputs ['layer2_1_relu']\n",
            "Executing Module: layer2.1.bn2 with inputs ['layer2_1_conv2']\n",
            "Executing Function: add with inputs ['layer2_1_bn2', 'layer2_0_relu_1']\n",
            "Executing Module: layer2.1.relu with inputs ['add_3']\n",
            "Executing Module: layer3.0.conv1 with inputs ['layer2_1_relu_1']\n",
            "Executing Module: layer3.0.bn1 with inputs ['layer3_0_conv1']\n",
            "Executing Module: layer3.0.relu with inputs ['layer3_0_bn1']\n",
            "Executing Module: layer3.0.conv2 with inputs ['layer3_0_relu']\n",
            "Executing Module: layer3.0.bn2 with inputs ['layer3_0_conv2']\n",
            "Executing Module: layer3.0.downsample.0 with inputs ['layer2_1_relu_1']\n",
            "Executing Module: layer3.0.downsample.1 with inputs ['layer3_0_downsample_0']\n",
            "Executing Function: add with inputs ['layer3_0_bn2', 'layer3_0_downsample_1']\n",
            "Executing Module: layer3.0.relu with inputs ['add_4']\n",
            "Executing Module: layer3.1.conv1 with inputs ['layer3_0_relu_1']\n",
            "Executing Module: layer3.1.bn1 with inputs ['layer3_1_conv1']\n",
            "Executing Module: layer3.1.relu with inputs ['layer3_1_bn1']\n",
            "Executing Module: layer3.1.conv2 with inputs ['layer3_1_relu']\n",
            "Executing Module: layer3.1.bn2 with inputs ['layer3_1_conv2']\n",
            "Executing Function: add with inputs ['layer3_1_bn2', 'layer3_0_relu_1']\n",
            "Executing Module: layer3.1.relu with inputs ['add_5']\n",
            "Executing Module: layer4.0.conv1 with inputs ['layer3_1_relu_1']\n",
            "Executing Module: layer4.0.bn1 with inputs ['layer4_0_conv1']\n",
            "Executing Module: layer4.0.relu with inputs ['layer4_0_bn1']\n",
            "Executing Module: layer4.0.conv2 with inputs ['layer4_0_relu']\n",
            "Executing Module: layer4.0.bn2 with inputs ['layer4_0_conv2']\n",
            "Executing Module: layer4.0.downsample.0 with inputs ['layer3_1_relu_1']\n",
            "Executing Module: layer4.0.downsample.1 with inputs ['layer4_0_downsample_0']\n",
            "Executing Function: add with inputs ['layer4_0_bn2', 'layer4_0_downsample_1']\n",
            "Executing Module: layer4.0.relu with inputs ['add_6']\n",
            "Executing Module: layer4.1.conv1 with inputs ['layer4_0_relu_1']\n",
            "Executing Module: layer4.1.bn1 with inputs ['layer4_1_conv1']\n",
            "Executing Module: layer4.1.relu with inputs ['layer4_1_bn1']\n",
            "Executing Module: layer4.1.conv2 with inputs ['layer4_1_relu']\n",
            "Executing Module: layer4.1.bn2 with inputs ['layer4_1_conv2']\n",
            "Executing Function: add with inputs ['layer4_1_bn2', 'layer4_0_relu_1']\n",
            "Executing Module: layer4.1.relu with inputs ['add_7']\n",
            "Executing Module: avgpool with inputs ['layer4_1_relu_1']\n",
            "Executing Function: flatten with inputs ['avgpool']\n",
            "Executing Module: fc with inputs ['flatten']\n",
            "Final Output: output\n",
            "\n",
            "Comparing Reconstructed Output with Actual Output for ResNet18:\n",
            "Success: The reconstructed output matches the actual output.\n",
            "tensor([[-0.0209, -0.1199, -0.0577, -0.3201, -0.3333,  0.0187,  0.2808, -0.3235,\n",
            "          0.0067,  0.0792]]) tensor([[-0.0209, -0.1199, -0.0577, -0.3201, -0.3333,  0.0187,  0.2808, -0.3235,\n",
            "          0.0067,  0.0792]])\n",
            "torch.Size([1, 1000]) torch.Size([1, 1000])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.fx import symbolic_trace\n",
        "from torchvision import models\n",
        "import warnings\n",
        "\n",
        "# Suppress specific deprecation warnings from torchvision\n",
        "warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"torchvision.models._utils\")\n",
        "\n",
        "# Define a SimpleCNN with torch.cat in its forward pass\n",
        "class SimpleCNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(SimpleCNN, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=16, kernel_size=3, padding=1)\n",
        "        self.conv2 = nn.Conv2d(in_channels=3, out_channels=16, kernel_size=5, padding=2)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.flatten = nn.Flatten()\n",
        "        self.fc = nn.Linear(32 * 28 * 28, 10)  # Assuming input images are 28x28\n",
        "\n",
        "    def forward(self, x):\n",
        "        # First path\n",
        "        x1 = self.conv1(x)\n",
        "        x1 = self.relu(x1)\n",
        "\n",
        "        # Second path\n",
        "        x2 = self.conv2(x)\n",
        "        x2 = self.relu(x2)\n",
        "\n",
        "        # Concatenate along the channel dimension\n",
        "        x = torch.cat((x1, x2), dim=1)\n",
        "\n",
        "        x = self.flatten(x)\n",
        "        x = self.fc(x)\n",
        "        return x\n",
        "\n",
        "def trace_and_extract_info(model, model_name=\"Model\"):\n",
        "    \"\"\"\n",
        "    Traces the given model using torch.fx and extracts:\n",
        "    - Topological Execution Order\n",
        "    - Dependency List\n",
        "    - Interaction Details: How outputs of dependencies are used as inputs\n",
        "    \"\"\"\n",
        "    print(f\"\\n{'='*60}\\nTracing and Extracting Information for {model_name}\\n{'='*60}\")\n",
        "\n",
        "    # Trace the model\n",
        "    traced_model = symbolic_trace(model)\n",
        "\n",
        "    # Extract Topological Order (execution order)\n",
        "    topological_order = [node.name for node in traced_model.graph.nodes]\n",
        "\n",
        "    # Extract Dependency List\n",
        "    dependency_list = {node.name: [arg.name for arg in node.all_input_nodes] for node in traced_model.graph.nodes}\n",
        "\n",
        "    # Extract Interaction Details\n",
        "    interaction_details = {node.name: [arg.name for arg in node.all_input_nodes] for node in traced_model.graph.nodes}\n",
        "\n",
        "    # Print Topological Execution Order\n",
        "    print(\"\\nTopological Execution Order:\")\n",
        "    for idx, node_name in enumerate(topological_order, 1):\n",
        "        print(f\"{idx}: {node_name}\")\n",
        "\n",
        "    # Print Dependency List\n",
        "    print(\"\\nDependency List:\")\n",
        "    for node_name in topological_order:\n",
        "        deps = dependency_list[node_name]\n",
        "        print(f\"{node_name}: {deps}\")\n",
        "\n",
        "    # Print Interaction Details\n",
        "    print(\"\\nInteraction Details:\")\n",
        "    for node_name in topological_order:\n",
        "        inputs = interaction_details[node_name]\n",
        "        if inputs:\n",
        "            print(f\"{node_name} receives inputs from: {', '.join(inputs)}\")\n",
        "        else:\n",
        "            print(f\"{node_name} receives inputs from: None (Input Placeholder)\")\n",
        "\n",
        "    return traced_model  # Return the traced model for further use\n",
        "\n",
        "def resolve_arg(arg, node_outputs):\n",
        "    \"\"\"\n",
        "    Recursively replaces Node references in args and kwargs with their actual outputs.\n",
        "    \"\"\"\n",
        "    if isinstance(arg, torch.fx.Node):\n",
        "        return node_outputs[arg.name]\n",
        "    elif isinstance(arg, (list, tuple)):\n",
        "        return type(arg)(resolve_arg(a, node_outputs) for a in arg)\n",
        "    elif isinstance(arg, dict):\n",
        "        return {k: resolve_arg(v, node_outputs) for k, v in arg.items()}\n",
        "    else:\n",
        "        return arg\n",
        "\n",
        "def group_topological_order(topological_order, group_size=2):\n",
        "    \"\"\"\n",
        "    Groups the topological order list into fixed-size groups.\n",
        "    Each group is named as 'stage-1', 'stage-2', etc.\n",
        "    \"\"\"\n",
        "    groups = {}\n",
        "    num_groups = (len(topological_order) + group_size - 1) // group_size  # Ceiling division\n",
        "\n",
        "    for i in range(num_groups):\n",
        "        start_idx = i * group_size\n",
        "        end_idx = start_idx + group_size\n",
        "        group_nodes = topological_order[start_idx:end_idx]\n",
        "        stage_name = f\"stage-{i+1}\"\n",
        "        groups[stage_name] = group_nodes\n",
        "\n",
        "    return groups\n",
        "\n",
        "def reconstruct_forward_with_groups(traced_model, input_tensor, group_size=2):\n",
        "    \"\"\"\n",
        "    Reconstructs the forward pass based on grouped topological order.\n",
        "    Each group of operations is executed sequentially as a stage.\n",
        "    Also prints dependencies between stages.\n",
        "    \"\"\"\n",
        "    print(\"\\nReconstructing Forward Pass Using Grouped Stages...\\n\")\n",
        "\n",
        "    # Get the topological order\n",
        "    topological_order = [node.name for node in traced_model.graph.nodes]\n",
        "    print(\"Topological Order:\", topological_order)\n",
        "\n",
        "    # Group the operations\n",
        "    grouped_operations = group_topological_order(topological_order, group_size=group_size)\n",
        "\n",
        "    # Print the grouped stages\n",
        "    print(\"Grouped Stages:\")\n",
        "    for stage, nodes in grouped_operations.items():\n",
        "        print(f\"{stage}: {nodes}\")\n",
        "\n",
        "    # Map each node to its stage for dependency analysis\n",
        "    node_to_stage = {}\n",
        "    for stage, nodes in grouped_operations.items():\n",
        "        for node in nodes:\n",
        "            node_to_stage[node] = stage\n",
        "\n",
        "    # Analyze dependencies between stages\n",
        "    stage_dependencies = {stage: set() for stage in grouped_operations.keys()}\n",
        "\n",
        "    for stage, nodes in grouped_operations.items():\n",
        "        for node in nodes:\n",
        "            # Find the node object by name\n",
        "            node_obj = next(n for n in traced_model.graph.nodes if n.name == node)\n",
        "            dependencies = node_obj.all_input_nodes\n",
        "            for dep in dependencies:\n",
        "                dep_stage = node_to_stage.get(dep.name, None)\n",
        "                if dep_stage and dep_stage != stage:\n",
        "                    stage_dependencies[stage].add(dep_stage)\n",
        "\n",
        "    # Print dependencies between stages\n",
        "    print(\"\\nDependencies Between Stages:\")\n",
        "    for stage, deps in stage_dependencies.items():\n",
        "        if deps:\n",
        "            deps_formatted = ', '.join(sorted(deps))\n",
        "            print(f\"{stage} depends on: {deps_formatted}\")\n",
        "        else:\n",
        "            print(f\"{stage} has no dependencies on other stages.\")\n",
        "\n",
        "    # Dictionary to hold the outputs of each node\n",
        "    node_outputs = {}\n",
        "\n",
        "    # Iterate through each stage\n",
        "    for stage, nodes in grouped_operations.items():\n",
        "        print(f\"\\n--- Executing {stage} ---\")\n",
        "        for node_name in nodes:\n",
        "            node = next(n for n in traced_model.graph.nodes if n.name == node_name)\n",
        "            print(node.name,node.op)\n",
        "\n",
        "            if node.op == 'placeholder':\n",
        "                # Assign the input tensor to the node\n",
        "                node_outputs[node.name] = input_tensor\n",
        "                print(input_tensor)\n",
        "                print(f\"[{stage}] Executing Placeholder: {node.name}\")\n",
        "            elif node.op == 'call_module':\n",
        "                # Get the submodule\n",
        "                submodule = dict(traced_model.named_modules())[node.target]\n",
        "                # Resolve the arguments\n",
        "                args = resolve_arg(node.args, node_outputs)\n",
        "                kwargs = resolve_arg(node.kwargs, node_outputs)\n",
        "                # Execute the submodule\n",
        "                print(f\"[{stage}] Executing Module: {node.target} with inputs {[arg.name for arg in node.all_input_nodes]}\")\n",
        "                node_outputs[node.name] = submodule(*args, **kwargs)\n",
        "            elif node.op == 'call_function':\n",
        "                # Get the function\n",
        "                func = node.target\n",
        "                # Resolve the arguments\n",
        "                args = resolve_arg(node.args, node_outputs)\n",
        "                kwargs = resolve_arg(node.kwargs, node_outputs)\n",
        "                # Execute the function\n",
        "                input_names = ', '.join([arg.name for arg in node.all_input_nodes])\n",
        "                print(f\"[{stage}] Executing Function: {func.__name__} with inputs {input_names}\")\n",
        "                node_outputs[node.name] = func(*args, **kwargs)\n",
        "            elif node.op == 'call_method':\n",
        "                # Get the method\n",
        "                method = getattr(resolve_arg(node.args[0], node_outputs), node.target)\n",
        "                # Resolve the arguments (excluding the first argument which is 'self')\n",
        "                args = resolve_arg(node.args[1:], node_outputs)\n",
        "                kwargs = resolve_arg(node.kwargs, node_outputs)\n",
        "                # Execute the method\n",
        "                input_names = ', '.join([arg.name for arg in node.all_input_nodes[1:]])\n",
        "                print(f\"[{stage}] Executing Method: {node.target} on {node.args[0].name} with inputs {input_names}\")\n",
        "                node_outputs[node.name] = method(*args, **kwargs)\n",
        "            elif node.op == 'output':\n",
        "                # Assign the final output\n",
        "                output = resolve_arg(node.args[0], node_outputs)\n",
        "                print(f\"[{stage}] Final Output: {node.name}\")\n",
        "                node_outputs[node.name] = output\n",
        "            else:\n",
        "                raise NotImplementedError(f\"Operation {node.op} is not supported.\")\n",
        "\n",
        "    # The output node holds the final output\n",
        "    return node_outputs['output']\n",
        "\n",
        "def main():\n",
        "    # Instantiate the SimpleCNN\n",
        "    simple_cnn = SimpleCNN()\n",
        "    simple_cnn.eval()  # Set to evaluation mode\n",
        "\n",
        "    # Instantiate the ResNet18 model from torchvision\n",
        "    # Note: 'pretrained' is deprecated, use 'weights' instead if using a newer torchvision version\n",
        "    try:\n",
        "        resnet18 = models.resnet18(weights=None)  # Set weights=models.ResNet18_Weights.DEFAULT for pretrained\n",
        "    except TypeError:\n",
        "        # For older torchvision versions\n",
        "        resnet18 = models.resnet18(pretrained=False)\n",
        "    resnet18.eval()  # Set to evaluation mode\n",
        "\n",
        "    # Trace and extract information for SimpleCNN\n",
        "    traced_simple_cnn = trace_and_extract_info(simple_cnn, model_name=\"SimpleCNN\")\n",
        "\n",
        "    # Create a sample input tensor for SimpleCNN\n",
        "    # Assuming input images are 28x28 with 3 channels\n",
        "    simple_cnn_input = torch.randn(1, 3, 28, 28)\n",
        "\n",
        "    # Reconstruct forward pass for SimpleCNN within no_grad context\n",
        "    with torch.no_grad():\n",
        "        reconstructed_simple_cnn_output = reconstruct_forward_with_groups(traced_simple_cnn, simple_cnn_input, group_size=2)\n",
        "\n",
        "        # Get actual output from SimpleCNN\n",
        "        actual_simple_cnn_output = simple_cnn(simple_cnn_input)\n",
        "\n",
        "    # Compare the outputs\n",
        "    print(reconstructed_simple_cnn_output,actual_simple_cnn_output)\n",
        "    print(\"\\nComparing Reconstructed Output with Actual Output for SimpleCNN:\")\n",
        "    if torch.allclose(reconstructed_simple_cnn_output, actual_simple_cnn_output, atol=1e-6):\n",
        "        print(\"Success: The reconstructed output matches the actual output.\")\n",
        "    else:\n",
        "        print(\"Warning: The reconstructed output does not match the actual output.\")\n",
        "\n",
        "    # Trace and extract information for ResNet18\n",
        "    traced_resnet18 = trace_and_extract_info(resnet18, model_name=\"ResNet18\")\n",
        "\n",
        "    # Create a sample input tensor for ResNet18\n",
        "    # ResNet18 typically expects 224x224 images with 3 channels\n",
        "    resnet18_input = torch.randn(1, 3, 224, 224)\n",
        "\n",
        "    # Reconstruct forward pass for ResNet18 within no_grad context\n",
        "    with torch.no_grad():\n",
        "        reconstructed_resnet18_output = reconstruct_forward_with_groups(traced_resnet18, resnet18_input, group_size=4)\n",
        "\n",
        "        # Get actual output from ResNet18\n",
        "        actual_resnet18_output = resnet18(resnet18_input)\n",
        "\n",
        "    # Compare the outputs\n",
        "    print(\"\\nComparing Reconstructed Output with Actual Output for ResNet18:\")\n",
        "    if torch.allclose(reconstructed_resnet18_output, actual_resnet18_output, atol=1e-6):\n",
        "        print(\"Success: The reconstructed output matches the actual output.\")\n",
        "    else:\n",
        "        print(\"Warning: The reconstructed output does not match the actual output.\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sd1q4DTyU_On",
        "outputId": "caa5cc1f-6494-4adf-af21-36e83e38ba9a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "============================================================\n",
            "Tracing and Extracting Information for SimpleCNN\n",
            "============================================================\n",
            "\n",
            "Topological Execution Order:\n",
            "1: x\n",
            "2: conv1\n",
            "3: relu\n",
            "4: conv2\n",
            "5: relu_1\n",
            "6: cat\n",
            "7: flatten\n",
            "8: fc\n",
            "9: output\n",
            "\n",
            "Dependency List:\n",
            "x: []\n",
            "conv1: ['x']\n",
            "relu: ['conv1']\n",
            "conv2: ['x']\n",
            "relu_1: ['conv2']\n",
            "cat: ['relu', 'relu_1']\n",
            "flatten: ['cat']\n",
            "fc: ['flatten']\n",
            "output: ['fc']\n",
            "\n",
            "Interaction Details:\n",
            "x receives inputs from: None (Input Placeholder)\n",
            "conv1 receives inputs from: x\n",
            "relu receives inputs from: conv1\n",
            "conv2 receives inputs from: x\n",
            "relu_1 receives inputs from: conv2\n",
            "cat receives inputs from: relu, relu_1\n",
            "flatten receives inputs from: cat\n",
            "fc receives inputs from: flatten\n",
            "output receives inputs from: fc\n",
            "\n",
            "Reconstructing Forward Pass Using Grouped Stages...\n",
            "\n",
            "Topological Order: ['x', 'conv1', 'relu', 'conv2', 'relu_1', 'cat', 'flatten', 'fc', 'output']\n",
            "Grouped Stages:\n",
            "stage-1: ['x', 'conv1']\n",
            "stage-2: ['relu', 'conv2']\n",
            "stage-3: ['relu_1', 'cat']\n",
            "stage-4: ['flatten', 'fc']\n",
            "stage-5: ['output']\n",
            "\n",
            "Dependencies Between Stages:\n",
            "stage-1 has no dependencies on other stages.\n",
            "stage-2 depends on: stage-1\n",
            "stage-3 depends on: stage-2\n",
            "stage-4 depends on: stage-3\n",
            "stage-5 depends on: stage-4\n",
            "\n",
            "--- Executing stage-1 ---\n",
            "x placeholder\n",
            "tensor([[[[ 0.9919,  1.0639, -0.5639,  ..., -1.5714,  0.1351,  0.1515],\n",
            "          [-0.2606,  1.9897,  0.4685,  ...,  0.0305, -1.3308,  2.3423],\n",
            "          [-2.2118,  0.4520,  0.3165,  ...,  1.6263,  0.7599, -0.4755],\n",
            "          ...,\n",
            "          [-1.4329, -0.7499, -1.4569,  ...,  0.5411,  0.1705, -0.5221],\n",
            "          [ 0.1084,  0.3030, -0.8780,  ..., -0.2789,  0.2259, -1.1325],\n",
            "          [ 0.7858,  0.9327,  0.9149,  ...,  1.3730,  0.9768,  0.5517]],\n",
            "\n",
            "         [[-0.0974, -1.4023,  0.2468,  ..., -0.6523, -1.0786,  1.1026],\n",
            "          [ 0.7781,  0.5047, -0.0637,  ..., -0.2004, -1.2169, -0.2631],\n",
            "          [-0.6066, -0.4095,  0.7302,  ..., -0.2150, -0.8733,  1.2538],\n",
            "          ...,\n",
            "          [ 0.7698,  2.0837, -0.0907,  ...,  1.4351,  0.9587,  1.2799],\n",
            "          [ 0.7808, -0.8014,  0.5080,  ..., -0.0249,  1.0910, -0.4041],\n",
            "          [-0.2332,  0.3465, -1.2147,  ...,  1.2580,  1.3064,  0.0030]],\n",
            "\n",
            "         [[ 0.5545,  0.8461, -1.0732,  ...,  1.3303, -0.0332,  0.4195],\n",
            "          [ 0.5610,  0.2842, -1.2778,  ..., -0.0272, -0.0348, -0.3498],\n",
            "          [ 0.5926,  0.6793, -1.5405,  ..., -0.8728,  0.1523,  0.3544],\n",
            "          ...,\n",
            "          [-0.7703, -0.2707,  1.0436,  ..., -0.0488, -0.9709,  0.9040],\n",
            "          [-0.1626, -0.3782,  0.1393,  ..., -0.7071,  0.2978, -0.2423],\n",
            "          [-0.8257,  1.3647,  2.0830,  ..., -0.9263, -0.0810,  0.3245]]]])\n",
            "[stage-1] Executing Placeholder: x\n",
            "conv1 call_module\n",
            "[stage-1] Executing Module: conv1 with inputs ['x']\n",
            "\n",
            "--- Executing stage-2 ---\n",
            "relu call_module\n",
            "[stage-2] Executing Module: relu with inputs ['conv1']\n",
            "conv2 call_module\n",
            "[stage-2] Executing Module: conv2 with inputs ['x']\n",
            "\n",
            "--- Executing stage-3 ---\n",
            "relu_1 call_module\n",
            "[stage-3] Executing Module: relu with inputs ['conv2']\n",
            "cat call_function\n",
            "[stage-3] Executing Function: cat with inputs relu, relu_1\n",
            "\n",
            "--- Executing stage-4 ---\n",
            "flatten call_module\n",
            "[stage-4] Executing Module: flatten with inputs ['cat']\n",
            "fc call_module\n",
            "[stage-4] Executing Module: fc with inputs ['flatten']\n",
            "\n",
            "--- Executing stage-5 ---\n",
            "output output\n",
            "[stage-5] Final Output: output\n",
            "tensor([[ 0.2758, -0.3937, -0.2410, -0.0586,  0.0326, -0.0256, -0.1149,  0.1407,\n",
            "          0.2241, -0.4601]]) tensor([[ 0.2758, -0.3937, -0.2410, -0.0586,  0.0326, -0.0256, -0.1149,  0.1407,\n",
            "          0.2241, -0.4601]])\n",
            "\n",
            "Comparing Reconstructed Output with Actual Output for SimpleCNN:\n",
            "Success: The reconstructed output matches the actual output.\n",
            "\n",
            "============================================================\n",
            "Tracing and Extracting Information for ResNet18\n",
            "============================================================\n",
            "\n",
            "Topological Execution Order:\n",
            "1: x\n",
            "2: conv1\n",
            "3: bn1\n",
            "4: relu\n",
            "5: maxpool\n",
            "6: layer1_0_conv1\n",
            "7: layer1_0_bn1\n",
            "8: layer1_0_relu\n",
            "9: layer1_0_conv2\n",
            "10: layer1_0_bn2\n",
            "11: add\n",
            "12: layer1_0_relu_1\n",
            "13: layer1_1_conv1\n",
            "14: layer1_1_bn1\n",
            "15: layer1_1_relu\n",
            "16: layer1_1_conv2\n",
            "17: layer1_1_bn2\n",
            "18: add_1\n",
            "19: layer1_1_relu_1\n",
            "20: layer2_0_conv1\n",
            "21: layer2_0_bn1\n",
            "22: layer2_0_relu\n",
            "23: layer2_0_conv2\n",
            "24: layer2_0_bn2\n",
            "25: layer2_0_downsample_0\n",
            "26: layer2_0_downsample_1\n",
            "27: add_2\n",
            "28: layer2_0_relu_1\n",
            "29: layer2_1_conv1\n",
            "30: layer2_1_bn1\n",
            "31: layer2_1_relu\n",
            "32: layer2_1_conv2\n",
            "33: layer2_1_bn2\n",
            "34: add_3\n",
            "35: layer2_1_relu_1\n",
            "36: layer3_0_conv1\n",
            "37: layer3_0_bn1\n",
            "38: layer3_0_relu\n",
            "39: layer3_0_conv2\n",
            "40: layer3_0_bn2\n",
            "41: layer3_0_downsample_0\n",
            "42: layer3_0_downsample_1\n",
            "43: add_4\n",
            "44: layer3_0_relu_1\n",
            "45: layer3_1_conv1\n",
            "46: layer3_1_bn1\n",
            "47: layer3_1_relu\n",
            "48: layer3_1_conv2\n",
            "49: layer3_1_bn2\n",
            "50: add_5\n",
            "51: layer3_1_relu_1\n",
            "52: layer4_0_conv1\n",
            "53: layer4_0_bn1\n",
            "54: layer4_0_relu\n",
            "55: layer4_0_conv2\n",
            "56: layer4_0_bn2\n",
            "57: layer4_0_downsample_0\n",
            "58: layer4_0_downsample_1\n",
            "59: add_6\n",
            "60: layer4_0_relu_1\n",
            "61: layer4_1_conv1\n",
            "62: layer4_1_bn1\n",
            "63: layer4_1_relu\n",
            "64: layer4_1_conv2\n",
            "65: layer4_1_bn2\n",
            "66: add_7\n",
            "67: layer4_1_relu_1\n",
            "68: avgpool\n",
            "69: flatten\n",
            "70: fc\n",
            "71: output\n",
            "\n",
            "Dependency List:\n",
            "x: []\n",
            "conv1: ['x']\n",
            "bn1: ['conv1']\n",
            "relu: ['bn1']\n",
            "maxpool: ['relu']\n",
            "layer1_0_conv1: ['maxpool']\n",
            "layer1_0_bn1: ['layer1_0_conv1']\n",
            "layer1_0_relu: ['layer1_0_bn1']\n",
            "layer1_0_conv2: ['layer1_0_relu']\n",
            "layer1_0_bn2: ['layer1_0_conv2']\n",
            "add: ['layer1_0_bn2', 'maxpool']\n",
            "layer1_0_relu_1: ['add']\n",
            "layer1_1_conv1: ['layer1_0_relu_1']\n",
            "layer1_1_bn1: ['layer1_1_conv1']\n",
            "layer1_1_relu: ['layer1_1_bn1']\n",
            "layer1_1_conv2: ['layer1_1_relu']\n",
            "layer1_1_bn2: ['layer1_1_conv2']\n",
            "add_1: ['layer1_1_bn2', 'layer1_0_relu_1']\n",
            "layer1_1_relu_1: ['add_1']\n",
            "layer2_0_conv1: ['layer1_1_relu_1']\n",
            "layer2_0_bn1: ['layer2_0_conv1']\n",
            "layer2_0_relu: ['layer2_0_bn1']\n",
            "layer2_0_conv2: ['layer2_0_relu']\n",
            "layer2_0_bn2: ['layer2_0_conv2']\n",
            "layer2_0_downsample_0: ['layer1_1_relu_1']\n",
            "layer2_0_downsample_1: ['layer2_0_downsample_0']\n",
            "add_2: ['layer2_0_bn2', 'layer2_0_downsample_1']\n",
            "layer2_0_relu_1: ['add_2']\n",
            "layer2_1_conv1: ['layer2_0_relu_1']\n",
            "layer2_1_bn1: ['layer2_1_conv1']\n",
            "layer2_1_relu: ['layer2_1_bn1']\n",
            "layer2_1_conv2: ['layer2_1_relu']\n",
            "layer2_1_bn2: ['layer2_1_conv2']\n",
            "add_3: ['layer2_1_bn2', 'layer2_0_relu_1']\n",
            "layer2_1_relu_1: ['add_3']\n",
            "layer3_0_conv1: ['layer2_1_relu_1']\n",
            "layer3_0_bn1: ['layer3_0_conv1']\n",
            "layer3_0_relu: ['layer3_0_bn1']\n",
            "layer3_0_conv2: ['layer3_0_relu']\n",
            "layer3_0_bn2: ['layer3_0_conv2']\n",
            "layer3_0_downsample_0: ['layer2_1_relu_1']\n",
            "layer3_0_downsample_1: ['layer3_0_downsample_0']\n",
            "add_4: ['layer3_0_bn2', 'layer3_0_downsample_1']\n",
            "layer3_0_relu_1: ['add_4']\n",
            "layer3_1_conv1: ['layer3_0_relu_1']\n",
            "layer3_1_bn1: ['layer3_1_conv1']\n",
            "layer3_1_relu: ['layer3_1_bn1']\n",
            "layer3_1_conv2: ['layer3_1_relu']\n",
            "layer3_1_bn2: ['layer3_1_conv2']\n",
            "add_5: ['layer3_1_bn2', 'layer3_0_relu_1']\n",
            "layer3_1_relu_1: ['add_5']\n",
            "layer4_0_conv1: ['layer3_1_relu_1']\n",
            "layer4_0_bn1: ['layer4_0_conv1']\n",
            "layer4_0_relu: ['layer4_0_bn1']\n",
            "layer4_0_conv2: ['layer4_0_relu']\n",
            "layer4_0_bn2: ['layer4_0_conv2']\n",
            "layer4_0_downsample_0: ['layer3_1_relu_1']\n",
            "layer4_0_downsample_1: ['layer4_0_downsample_0']\n",
            "add_6: ['layer4_0_bn2', 'layer4_0_downsample_1']\n",
            "layer4_0_relu_1: ['add_6']\n",
            "layer4_1_conv1: ['layer4_0_relu_1']\n",
            "layer4_1_bn1: ['layer4_1_conv1']\n",
            "layer4_1_relu: ['layer4_1_bn1']\n",
            "layer4_1_conv2: ['layer4_1_relu']\n",
            "layer4_1_bn2: ['layer4_1_conv2']\n",
            "add_7: ['layer4_1_bn2', 'layer4_0_relu_1']\n",
            "layer4_1_relu_1: ['add_7']\n",
            "avgpool: ['layer4_1_relu_1']\n",
            "flatten: ['avgpool']\n",
            "fc: ['flatten']\n",
            "output: ['fc']\n",
            "\n",
            "Interaction Details:\n",
            "x receives inputs from: None (Input Placeholder)\n",
            "conv1 receives inputs from: x\n",
            "bn1 receives inputs from: conv1\n",
            "relu receives inputs from: bn1\n",
            "maxpool receives inputs from: relu\n",
            "layer1_0_conv1 receives inputs from: maxpool\n",
            "layer1_0_bn1 receives inputs from: layer1_0_conv1\n",
            "layer1_0_relu receives inputs from: layer1_0_bn1\n",
            "layer1_0_conv2 receives inputs from: layer1_0_relu\n",
            "layer1_0_bn2 receives inputs from: layer1_0_conv2\n",
            "add receives inputs from: layer1_0_bn2, maxpool\n",
            "layer1_0_relu_1 receives inputs from: add\n",
            "layer1_1_conv1 receives inputs from: layer1_0_relu_1\n",
            "layer1_1_bn1 receives inputs from: layer1_1_conv1\n",
            "layer1_1_relu receives inputs from: layer1_1_bn1\n",
            "layer1_1_conv2 receives inputs from: layer1_1_relu\n",
            "layer1_1_bn2 receives inputs from: layer1_1_conv2\n",
            "add_1 receives inputs from: layer1_1_bn2, layer1_0_relu_1\n",
            "layer1_1_relu_1 receives inputs from: add_1\n",
            "layer2_0_conv1 receives inputs from: layer1_1_relu_1\n",
            "layer2_0_bn1 receives inputs from: layer2_0_conv1\n",
            "layer2_0_relu receives inputs from: layer2_0_bn1\n",
            "layer2_0_conv2 receives inputs from: layer2_0_relu\n",
            "layer2_0_bn2 receives inputs from: layer2_0_conv2\n",
            "layer2_0_downsample_0 receives inputs from: layer1_1_relu_1\n",
            "layer2_0_downsample_1 receives inputs from: layer2_0_downsample_0\n",
            "add_2 receives inputs from: layer2_0_bn2, layer2_0_downsample_1\n",
            "layer2_0_relu_1 receives inputs from: add_2\n",
            "layer2_1_conv1 receives inputs from: layer2_0_relu_1\n",
            "layer2_1_bn1 receives inputs from: layer2_1_conv1\n",
            "layer2_1_relu receives inputs from: layer2_1_bn1\n",
            "layer2_1_conv2 receives inputs from: layer2_1_relu\n",
            "layer2_1_bn2 receives inputs from: layer2_1_conv2\n",
            "add_3 receives inputs from: layer2_1_bn2, layer2_0_relu_1\n",
            "layer2_1_relu_1 receives inputs from: add_3\n",
            "layer3_0_conv1 receives inputs from: layer2_1_relu_1\n",
            "layer3_0_bn1 receives inputs from: layer3_0_conv1\n",
            "layer3_0_relu receives inputs from: layer3_0_bn1\n",
            "layer3_0_conv2 receives inputs from: layer3_0_relu\n",
            "layer3_0_bn2 receives inputs from: layer3_0_conv2\n",
            "layer3_0_downsample_0 receives inputs from: layer2_1_relu_1\n",
            "layer3_0_downsample_1 receives inputs from: layer3_0_downsample_0\n",
            "add_4 receives inputs from: layer3_0_bn2, layer3_0_downsample_1\n",
            "layer3_0_relu_1 receives inputs from: add_4\n",
            "layer3_1_conv1 receives inputs from: layer3_0_relu_1\n",
            "layer3_1_bn1 receives inputs from: layer3_1_conv1\n",
            "layer3_1_relu receives inputs from: layer3_1_bn1\n",
            "layer3_1_conv2 receives inputs from: layer3_1_relu\n",
            "layer3_1_bn2 receives inputs from: layer3_1_conv2\n",
            "add_5 receives inputs from: layer3_1_bn2, layer3_0_relu_1\n",
            "layer3_1_relu_1 receives inputs from: add_5\n",
            "layer4_0_conv1 receives inputs from: layer3_1_relu_1\n",
            "layer4_0_bn1 receives inputs from: layer4_0_conv1\n",
            "layer4_0_relu receives inputs from: layer4_0_bn1\n",
            "layer4_0_conv2 receives inputs from: layer4_0_relu\n",
            "layer4_0_bn2 receives inputs from: layer4_0_conv2\n",
            "layer4_0_downsample_0 receives inputs from: layer3_1_relu_1\n",
            "layer4_0_downsample_1 receives inputs from: layer4_0_downsample_0\n",
            "add_6 receives inputs from: layer4_0_bn2, layer4_0_downsample_1\n",
            "layer4_0_relu_1 receives inputs from: add_6\n",
            "layer4_1_conv1 receives inputs from: layer4_0_relu_1\n",
            "layer4_1_bn1 receives inputs from: layer4_1_conv1\n",
            "layer4_1_relu receives inputs from: layer4_1_bn1\n",
            "layer4_1_conv2 receives inputs from: layer4_1_relu\n",
            "layer4_1_bn2 receives inputs from: layer4_1_conv2\n",
            "add_7 receives inputs from: layer4_1_bn2, layer4_0_relu_1\n",
            "layer4_1_relu_1 receives inputs from: add_7\n",
            "avgpool receives inputs from: layer4_1_relu_1\n",
            "flatten receives inputs from: avgpool\n",
            "fc receives inputs from: flatten\n",
            "output receives inputs from: fc\n",
            "\n",
            "Reconstructing Forward Pass Using Grouped Stages...\n",
            "\n",
            "Topological Order: ['x', 'conv1', 'bn1', 'relu', 'maxpool', 'layer1_0_conv1', 'layer1_0_bn1', 'layer1_0_relu', 'layer1_0_conv2', 'layer1_0_bn2', 'add', 'layer1_0_relu_1', 'layer1_1_conv1', 'layer1_1_bn1', 'layer1_1_relu', 'layer1_1_conv2', 'layer1_1_bn2', 'add_1', 'layer1_1_relu_1', 'layer2_0_conv1', 'layer2_0_bn1', 'layer2_0_relu', 'layer2_0_conv2', 'layer2_0_bn2', 'layer2_0_downsample_0', 'layer2_0_downsample_1', 'add_2', 'layer2_0_relu_1', 'layer2_1_conv1', 'layer2_1_bn1', 'layer2_1_relu', 'layer2_1_conv2', 'layer2_1_bn2', 'add_3', 'layer2_1_relu_1', 'layer3_0_conv1', 'layer3_0_bn1', 'layer3_0_relu', 'layer3_0_conv2', 'layer3_0_bn2', 'layer3_0_downsample_0', 'layer3_0_downsample_1', 'add_4', 'layer3_0_relu_1', 'layer3_1_conv1', 'layer3_1_bn1', 'layer3_1_relu', 'layer3_1_conv2', 'layer3_1_bn2', 'add_5', 'layer3_1_relu_1', 'layer4_0_conv1', 'layer4_0_bn1', 'layer4_0_relu', 'layer4_0_conv2', 'layer4_0_bn2', 'layer4_0_downsample_0', 'layer4_0_downsample_1', 'add_6', 'layer4_0_relu_1', 'layer4_1_conv1', 'layer4_1_bn1', 'layer4_1_relu', 'layer4_1_conv2', 'layer4_1_bn2', 'add_7', 'layer4_1_relu_1', 'avgpool', 'flatten', 'fc', 'output']\n",
            "Grouped Stages:\n",
            "stage-1: ['x', 'conv1', 'bn1', 'relu']\n",
            "stage-2: ['maxpool', 'layer1_0_conv1', 'layer1_0_bn1', 'layer1_0_relu']\n",
            "stage-3: ['layer1_0_conv2', 'layer1_0_bn2', 'add', 'layer1_0_relu_1']\n",
            "stage-4: ['layer1_1_conv1', 'layer1_1_bn1', 'layer1_1_relu', 'layer1_1_conv2']\n",
            "stage-5: ['layer1_1_bn2', 'add_1', 'layer1_1_relu_1', 'layer2_0_conv1']\n",
            "stage-6: ['layer2_0_bn1', 'layer2_0_relu', 'layer2_0_conv2', 'layer2_0_bn2']\n",
            "stage-7: ['layer2_0_downsample_0', 'layer2_0_downsample_1', 'add_2', 'layer2_0_relu_1']\n",
            "stage-8: ['layer2_1_conv1', 'layer2_1_bn1', 'layer2_1_relu', 'layer2_1_conv2']\n",
            "stage-9: ['layer2_1_bn2', 'add_3', 'layer2_1_relu_1', 'layer3_0_conv1']\n",
            "stage-10: ['layer3_0_bn1', 'layer3_0_relu', 'layer3_0_conv2', 'layer3_0_bn2']\n",
            "stage-11: ['layer3_0_downsample_0', 'layer3_0_downsample_1', 'add_4', 'layer3_0_relu_1']\n",
            "stage-12: ['layer3_1_conv1', 'layer3_1_bn1', 'layer3_1_relu', 'layer3_1_conv2']\n",
            "stage-13: ['layer3_1_bn2', 'add_5', 'layer3_1_relu_1', 'layer4_0_conv1']\n",
            "stage-14: ['layer4_0_bn1', 'layer4_0_relu', 'layer4_0_conv2', 'layer4_0_bn2']\n",
            "stage-15: ['layer4_0_downsample_0', 'layer4_0_downsample_1', 'add_6', 'layer4_0_relu_1']\n",
            "stage-16: ['layer4_1_conv1', 'layer4_1_bn1', 'layer4_1_relu', 'layer4_1_conv2']\n",
            "stage-17: ['layer4_1_bn2', 'add_7', 'layer4_1_relu_1', 'avgpool']\n",
            "stage-18: ['flatten', 'fc', 'output']\n",
            "\n",
            "Dependencies Between Stages:\n",
            "stage-1 has no dependencies on other stages.\n",
            "stage-2 depends on: stage-1\n",
            "stage-3 depends on: stage-2\n",
            "stage-4 depends on: stage-3\n",
            "stage-5 depends on: stage-3, stage-4\n",
            "stage-6 depends on: stage-5\n",
            "stage-7 depends on: stage-5, stage-6\n",
            "stage-8 depends on: stage-7\n",
            "stage-9 depends on: stage-7, stage-8\n",
            "stage-10 depends on: stage-9\n",
            "stage-11 depends on: stage-10, stage-9\n",
            "stage-12 depends on: stage-11\n",
            "stage-13 depends on: stage-11, stage-12\n",
            "stage-14 depends on: stage-13\n",
            "stage-15 depends on: stage-13, stage-14\n",
            "stage-16 depends on: stage-15\n",
            "stage-17 depends on: stage-15, stage-16\n",
            "stage-18 depends on: stage-17\n",
            "\n",
            "--- Executing stage-1 ---\n",
            "x placeholder\n",
            "tensor([[[[-1.9916, -1.6159, -0.6323,  ...,  1.2343,  0.0760, -0.6541],\n",
            "          [ 2.0741,  0.5248, -1.3879,  ...,  1.8489, -2.0756,  0.6348],\n",
            "          [ 0.6308,  1.4218, -0.0519,  ..., -0.1087,  0.0743,  0.6090],\n",
            "          ...,\n",
            "          [-1.4760, -0.7342, -0.6019,  ...,  1.2797, -0.1113, -1.2105],\n",
            "          [ 0.4718,  0.4163, -1.1822,  ..., -1.6489,  0.3783,  0.2588],\n",
            "          [ 0.5120, -0.9951,  0.5429,  ..., -0.6240, -0.7476,  0.9420]],\n",
            "\n",
            "         [[-0.4704, -0.2112, -1.0052,  ...,  1.4486,  0.2967, -0.2682],\n",
            "          [-1.5597, -1.4723, -0.6200,  ...,  1.1191, -0.7212,  2.5328],\n",
            "          [ 0.4779, -0.7869, -0.2234,  ...,  0.9535,  0.9866, -0.9869],\n",
            "          ...,\n",
            "          [-0.2464,  0.4099, -0.0903,  ...,  0.1454, -2.2647, -1.0832],\n",
            "          [-1.3167,  0.3896,  0.2385,  ..., -0.3963, -1.0218, -2.1751],\n",
            "          [-0.4952,  1.3906,  0.0988,  ...,  1.1938,  1.2785, -1.7576]],\n",
            "\n",
            "         [[ 0.9322, -0.5466,  1.3588,  ..., -2.4067,  1.3694,  0.1177],\n",
            "          [ 0.3859,  2.1093, -1.2749,  ...,  0.8815,  0.7237, -0.7301],\n",
            "          [-1.6163, -0.1320, -0.6925,  ..., -1.4260,  0.2402, -0.6307],\n",
            "          ...,\n",
            "          [-0.4046,  1.1183,  0.7176,  ..., -0.5285,  2.0784,  0.0455],\n",
            "          [ 1.6583, -0.1169, -1.0437,  ...,  1.4808, -1.1794,  1.3128],\n",
            "          [-1.4135, -0.5074,  0.3177,  ..., -0.4200,  0.6958,  2.1753]]]])\n",
            "[stage-1] Executing Placeholder: x\n",
            "conv1 call_module\n",
            "[stage-1] Executing Module: conv1 with inputs ['x']\n",
            "bn1 call_module\n",
            "[stage-1] Executing Module: bn1 with inputs ['conv1']\n",
            "relu call_module\n",
            "[stage-1] Executing Module: relu with inputs ['bn1']\n",
            "\n",
            "--- Executing stage-2 ---\n",
            "maxpool call_module\n",
            "[stage-2] Executing Module: maxpool with inputs ['relu']\n",
            "layer1_0_conv1 call_module\n",
            "[stage-2] Executing Module: layer1.0.conv1 with inputs ['maxpool']\n",
            "layer1_0_bn1 call_module\n",
            "[stage-2] Executing Module: layer1.0.bn1 with inputs ['layer1_0_conv1']\n",
            "layer1_0_relu call_module\n",
            "[stage-2] Executing Module: layer1.0.relu with inputs ['layer1_0_bn1']\n",
            "\n",
            "--- Executing stage-3 ---\n",
            "layer1_0_conv2 call_module\n",
            "[stage-3] Executing Module: layer1.0.conv2 with inputs ['layer1_0_relu']\n",
            "layer1_0_bn2 call_module\n",
            "[stage-3] Executing Module: layer1.0.bn2 with inputs ['layer1_0_conv2']\n",
            "add call_function\n",
            "[stage-3] Executing Function: add with inputs layer1_0_bn2, maxpool\n",
            "layer1_0_relu_1 call_module\n",
            "[stage-3] Executing Module: layer1.0.relu with inputs ['add']\n",
            "\n",
            "--- Executing stage-4 ---\n",
            "layer1_1_conv1 call_module\n",
            "[stage-4] Executing Module: layer1.1.conv1 with inputs ['layer1_0_relu_1']\n",
            "layer1_1_bn1 call_module\n",
            "[stage-4] Executing Module: layer1.1.bn1 with inputs ['layer1_1_conv1']\n",
            "layer1_1_relu call_module\n",
            "[stage-4] Executing Module: layer1.1.relu with inputs ['layer1_1_bn1']\n",
            "layer1_1_conv2 call_module\n",
            "[stage-4] Executing Module: layer1.1.conv2 with inputs ['layer1_1_relu']\n",
            "\n",
            "--- Executing stage-5 ---\n",
            "layer1_1_bn2 call_module\n",
            "[stage-5] Executing Module: layer1.1.bn2 with inputs ['layer1_1_conv2']\n",
            "add_1 call_function\n",
            "[stage-5] Executing Function: add with inputs layer1_1_bn2, layer1_0_relu_1\n",
            "layer1_1_relu_1 call_module\n",
            "[stage-5] Executing Module: layer1.1.relu with inputs ['add_1']\n",
            "layer2_0_conv1 call_module\n",
            "[stage-5] Executing Module: layer2.0.conv1 with inputs ['layer1_1_relu_1']\n",
            "\n",
            "--- Executing stage-6 ---\n",
            "layer2_0_bn1 call_module\n",
            "[stage-6] Executing Module: layer2.0.bn1 with inputs ['layer2_0_conv1']\n",
            "layer2_0_relu call_module\n",
            "[stage-6] Executing Module: layer2.0.relu with inputs ['layer2_0_bn1']\n",
            "layer2_0_conv2 call_module\n",
            "[stage-6] Executing Module: layer2.0.conv2 with inputs ['layer2_0_relu']\n",
            "layer2_0_bn2 call_module\n",
            "[stage-6] Executing Module: layer2.0.bn2 with inputs ['layer2_0_conv2']\n",
            "\n",
            "--- Executing stage-7 ---\n",
            "layer2_0_downsample_0 call_module\n",
            "[stage-7] Executing Module: layer2.0.downsample.0 with inputs ['layer1_1_relu_1']\n",
            "layer2_0_downsample_1 call_module\n",
            "[stage-7] Executing Module: layer2.0.downsample.1 with inputs ['layer2_0_downsample_0']\n",
            "add_2 call_function\n",
            "[stage-7] Executing Function: add with inputs layer2_0_bn2, layer2_0_downsample_1\n",
            "layer2_0_relu_1 call_module\n",
            "[stage-7] Executing Module: layer2.0.relu with inputs ['add_2']\n",
            "\n",
            "--- Executing stage-8 ---\n",
            "layer2_1_conv1 call_module\n",
            "[stage-8] Executing Module: layer2.1.conv1 with inputs ['layer2_0_relu_1']\n",
            "layer2_1_bn1 call_module\n",
            "[stage-8] Executing Module: layer2.1.bn1 with inputs ['layer2_1_conv1']\n",
            "layer2_1_relu call_module\n",
            "[stage-8] Executing Module: layer2.1.relu with inputs ['layer2_1_bn1']\n",
            "layer2_1_conv2 call_module\n",
            "[stage-8] Executing Module: layer2.1.conv2 with inputs ['layer2_1_relu']\n",
            "\n",
            "--- Executing stage-9 ---\n",
            "layer2_1_bn2 call_module\n",
            "[stage-9] Executing Module: layer2.1.bn2 with inputs ['layer2_1_conv2']\n",
            "add_3 call_function\n",
            "[stage-9] Executing Function: add with inputs layer2_1_bn2, layer2_0_relu_1\n",
            "layer2_1_relu_1 call_module\n",
            "[stage-9] Executing Module: layer2.1.relu with inputs ['add_3']\n",
            "layer3_0_conv1 call_module\n",
            "[stage-9] Executing Module: layer3.0.conv1 with inputs ['layer2_1_relu_1']\n",
            "\n",
            "--- Executing stage-10 ---\n",
            "layer3_0_bn1 call_module\n",
            "[stage-10] Executing Module: layer3.0.bn1 with inputs ['layer3_0_conv1']\n",
            "layer3_0_relu call_module\n",
            "[stage-10] Executing Module: layer3.0.relu with inputs ['layer3_0_bn1']\n",
            "layer3_0_conv2 call_module\n",
            "[stage-10] Executing Module: layer3.0.conv2 with inputs ['layer3_0_relu']\n",
            "layer3_0_bn2 call_module\n",
            "[stage-10] Executing Module: layer3.0.bn2 with inputs ['layer3_0_conv2']\n",
            "\n",
            "--- Executing stage-11 ---\n",
            "layer3_0_downsample_0 call_module\n",
            "[stage-11] Executing Module: layer3.0.downsample.0 with inputs ['layer2_1_relu_1']\n",
            "layer3_0_downsample_1 call_module\n",
            "[stage-11] Executing Module: layer3.0.downsample.1 with inputs ['layer3_0_downsample_0']\n",
            "add_4 call_function\n",
            "[stage-11] Executing Function: add with inputs layer3_0_bn2, layer3_0_downsample_1\n",
            "layer3_0_relu_1 call_module\n",
            "[stage-11] Executing Module: layer3.0.relu with inputs ['add_4']\n",
            "\n",
            "--- Executing stage-12 ---\n",
            "layer3_1_conv1 call_module\n",
            "[stage-12] Executing Module: layer3.1.conv1 with inputs ['layer3_0_relu_1']\n",
            "layer3_1_bn1 call_module\n",
            "[stage-12] Executing Module: layer3.1.bn1 with inputs ['layer3_1_conv1']\n",
            "layer3_1_relu call_module\n",
            "[stage-12] Executing Module: layer3.1.relu with inputs ['layer3_1_bn1']\n",
            "layer3_1_conv2 call_module\n",
            "[stage-12] Executing Module: layer3.1.conv2 with inputs ['layer3_1_relu']\n",
            "\n",
            "--- Executing stage-13 ---\n",
            "layer3_1_bn2 call_module\n",
            "[stage-13] Executing Module: layer3.1.bn2 with inputs ['layer3_1_conv2']\n",
            "add_5 call_function\n",
            "[stage-13] Executing Function: add with inputs layer3_1_bn2, layer3_0_relu_1\n",
            "layer3_1_relu_1 call_module\n",
            "[stage-13] Executing Module: layer3.1.relu with inputs ['add_5']\n",
            "layer4_0_conv1 call_module\n",
            "[stage-13] Executing Module: layer4.0.conv1 with inputs ['layer3_1_relu_1']\n",
            "\n",
            "--- Executing stage-14 ---\n",
            "layer4_0_bn1 call_module\n",
            "[stage-14] Executing Module: layer4.0.bn1 with inputs ['layer4_0_conv1']\n",
            "layer4_0_relu call_module\n",
            "[stage-14] Executing Module: layer4.0.relu with inputs ['layer4_0_bn1']\n",
            "layer4_0_conv2 call_module\n",
            "[stage-14] Executing Module: layer4.0.conv2 with inputs ['layer4_0_relu']\n",
            "layer4_0_bn2 call_module\n",
            "[stage-14] Executing Module: layer4.0.bn2 with inputs ['layer4_0_conv2']\n",
            "\n",
            "--- Executing stage-15 ---\n",
            "layer4_0_downsample_0 call_module\n",
            "[stage-15] Executing Module: layer4.0.downsample.0 with inputs ['layer3_1_relu_1']\n",
            "layer4_0_downsample_1 call_module\n",
            "[stage-15] Executing Module: layer4.0.downsample.1 with inputs ['layer4_0_downsample_0']\n",
            "add_6 call_function\n",
            "[stage-15] Executing Function: add with inputs layer4_0_bn2, layer4_0_downsample_1\n",
            "layer4_0_relu_1 call_module\n",
            "[stage-15] Executing Module: layer4.0.relu with inputs ['add_6']\n",
            "\n",
            "--- Executing stage-16 ---\n",
            "layer4_1_conv1 call_module\n",
            "[stage-16] Executing Module: layer4.1.conv1 with inputs ['layer4_0_relu_1']\n",
            "layer4_1_bn1 call_module\n",
            "[stage-16] Executing Module: layer4.1.bn1 with inputs ['layer4_1_conv1']\n",
            "layer4_1_relu call_module\n",
            "[stage-16] Executing Module: layer4.1.relu with inputs ['layer4_1_bn1']\n",
            "layer4_1_conv2 call_module\n",
            "[stage-16] Executing Module: layer4.1.conv2 with inputs ['layer4_1_relu']\n",
            "\n",
            "--- Executing stage-17 ---\n",
            "layer4_1_bn2 call_module\n",
            "[stage-17] Executing Module: layer4.1.bn2 with inputs ['layer4_1_conv2']\n",
            "add_7 call_function\n",
            "[stage-17] Executing Function: add with inputs layer4_1_bn2, layer4_0_relu_1\n",
            "layer4_1_relu_1 call_module\n",
            "[stage-17] Executing Module: layer4.1.relu with inputs ['add_7']\n",
            "avgpool call_module\n",
            "[stage-17] Executing Module: avgpool with inputs ['layer4_1_relu_1']\n",
            "\n",
            "--- Executing stage-18 ---\n",
            "flatten call_function\n",
            "[stage-18] Executing Function: flatten with inputs avgpool\n",
            "fc call_module\n",
            "[stage-18] Executing Module: fc with inputs ['flatten']\n",
            "output output\n",
            "[stage-18] Final Output: output\n",
            "\n",
            "Comparing Reconstructed Output with Actual Output for ResNet18:\n",
            "Success: The reconstructed output matches the actual output.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "KwkwCyXLZq8m"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}