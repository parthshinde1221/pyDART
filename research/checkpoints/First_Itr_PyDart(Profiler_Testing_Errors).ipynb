{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# PyDart Library â€“ First Iteration\n",
        "\n",
        "## Overview\n",
        "\n",
        "This notebook represents the initial iteration of the complete PyDart Library, developed following the completion of the Profiler. In this version, the library includes an explicit `Scheduler` class inspired by the DART research paper.\n",
        "\n",
        "## Main Contributions\n",
        "\n",
        "1. **Forward Pass Decomposition**  \n",
        "   - The decomposition was performed on the forward pass of the deep neural network rather than on its DAG (Directed Acyclic Graph).\n",
        "\n",
        "2. **Event Aggregation**  \n",
        "   - The decomposition process aggregated individual events captured by the Profiler.\n",
        "\n",
        "3. **Dynamic Programming (DP) Algorithm**  \n",
        "   - A simplified version of the DP algorithm, as described in the research paper, was implemented.  \n",
        "   - Note: This implementation was not fully complete.\n",
        "\n",
        "## Error Analysis and Iterative Improvements\n",
        "\n",
        "During profiling, several issues were identified:\n",
        "\n",
        "- **Profiling Errors:**  \n",
        "  - Most errors were related to improper or missing data retrieval.\n",
        "\n",
        "- **Decomposition Errors:**  \n",
        "  - There were issues with the incorrect decomposition of the forward pass.  \n",
        "  - These errors have been addressed in subsequent iterations (detailed in the next notebook).\n",
        "\n",
        "Since these errors were not directly related to the DP algorithm, the focus was placed on correcting the earlier stages of the process before refining the algorithm further.\n",
        "\n",
        "## Development and Testing Phases\n",
        "\n",
        "Throughout the development process, multiple Jupyter notebooks were used for testing and developing individual classes. The workflow is divided into three main phases:\n",
        "\n",
        "1. **Initialization Phase**  \n",
        "   - Configuration and setup of initial components\n",
        "     - Profiler Class - Profile and store the results into a persistent DB.\n",
        "     - Stage Class - The main building block or the fundamental unit that actually runs on the node.\n",
        "     - Task Class - The actual DNN inference task for the model and encapusulates the Stages and the dependencies/dependent stages in it\n",
        "     gives allocation on the Nodes.\n",
        "     - Scheduler Class - Allocates the stages offiilne using the DP based approach and then dispatches during the runtime stage.\n",
        "      - Taskset Class - Comprises of all the Tasks and the Scheduler objects in it and is used as the focal point for running the library from the user prespective.\n",
        "\n",
        "2. **Runtime Phase**  \n",
        "   - The phase intended for end-user interaction with the library.\n",
        "   - This runs by running the taskset excute\n",
        "\n",
        "3. **Evaluation Phase**  \n",
        "   - The phase where the library and framework are evaluated for performance and correctness.\n",
        "   - This is the phase , where we evaluate the library on the particular system and task configurations , to test its speedup and throughput.\n",
        "\n",
        "---\n",
        "\n",
        "**Note** - There were multiple iterations of this , while developing the said classes.I have included , what I felt like were the main checkpoints.\n",
        "Even the following iterations after this follow almost a similar approach.\n"
      ],
      "metadata": {
        "id": "eQzfqngc9EUm"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-craS1JtRDdz"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import torch\n",
        "import threading\n",
        "import queue\n",
        "from typing import Callable, Any, List\n",
        "\n",
        "class Node:\n",
        "    \"\"\"\n",
        "    Represents a computational resource: either a CPU-only node (1 CPU core)\n",
        "    or a GPU+CPU pair. Each Node has:\n",
        "      - node_id (e.g., 'CPU-0', 'GPU-0-CPU-1')\n",
        "      - A worker thread + queue to run tasks\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, node_id: str, cpus=None, gpu=None):\n",
        "        self._node_id = node_id\n",
        "        self._cpus = tuple(cpus or [])\n",
        "        self._gpu = gpu\n",
        "\n",
        "        self._original_affinity = os.sched_getaffinity(0)\n",
        "        self._task_queue = queue.Queue()\n",
        "        self._stop_signal = False\n",
        "\n",
        "        self._worker_thread = threading.Thread(target=self._worker_loop, daemon=True)\n",
        "        self._worker_thread.start()\n",
        "\n",
        "    @property\n",
        "    def node_id(self):\n",
        "        return self._node_id\n",
        "\n",
        "    @property\n",
        "    def cpus(self):\n",
        "        return self._cpus\n",
        "\n",
        "    @property\n",
        "    def gpu(self):\n",
        "        return self._gpu\n",
        "\n",
        "    def assign_task(self, func: Callable, *args, **kwargs) -> queue.Queue:\n",
        "        \"\"\"\n",
        "        Enqueue a function to this node. Returns a queue from which\n",
        "        the caller can retrieve the result (blocking).\n",
        "        \"\"\"\n",
        "        result_queue = queue.Queue(maxsize=1)\n",
        "        self._task_queue.put((func, args, kwargs, result_queue))\n",
        "        return result_queue\n",
        "\n",
        "    def stop(self):\n",
        "        \"\"\"\n",
        "        Signal the node to stop after processing queued tasks.\n",
        "        \"\"\"\n",
        "        self._stop_signal = True\n",
        "        self._task_queue.put(None)\n",
        "        self._worker_thread.join()\n",
        "\n",
        "    def _worker_loop(self):\n",
        "        while not self._stop_signal:\n",
        "            item = self._task_queue.get()\n",
        "            if item is None:\n",
        "                break\n",
        "            func, args, kwargs, result_queue = item\n",
        "            try:\n",
        "                self._set_context()\n",
        "                result = func(*args, **kwargs)\n",
        "            except Exception as e:\n",
        "                result = e\n",
        "            finally:\n",
        "                self._reset_context()\n",
        "\n",
        "            result_queue.put(result)\n",
        "\n",
        "    def _set_context(self):\n",
        "        if self._cpus:\n",
        "            os.sched_setaffinity(0, self._cpus)\n",
        "        if self._gpu is not None and torch.cuda.is_available():\n",
        "            torch.cuda.set_device(self._gpu)\n",
        "\n",
        "    def _reset_context(self):\n",
        "        os.sched_setaffinity(0, self._original_affinity)\n",
        "        # optionally reset GPU device if needed\n",
        "\n",
        "    @staticmethod\n",
        "    def discover_nodes() -> List['Node']:\n",
        "        \"\"\"\n",
        "        Create a Node for each CPU core, and for each GPU+CPU pair.\n",
        "        \"\"\"\n",
        "        nodes = []\n",
        "        num_cpus = os.cpu_count() or 1\n",
        "        ngpus = torch.cuda.device_count()\n",
        "\n",
        "        # CPU-only nodes\n",
        "        for core_id in range(num_cpus):\n",
        "            node = Node(node_id=f\"CPU-{core_id}\", cpus=[core_id])\n",
        "            nodes.append(node)\n",
        "\n",
        "        # GPU+CPU nodes\n",
        "        for g in range(ngpus):\n",
        "            for core_id in range(num_cpus):\n",
        "                node = Node(node_id=f\"GPU-{g}-CPU-{core_id}\", cpus=[core_id], gpu=g)\n",
        "                nodes.append(node)\n",
        "\n",
        "        return nodes\n",
        "\n",
        "    def __repr__(self):\n",
        "        return f\"Node({self._node_id}, cpus={self._cpus}, gpu={self._gpu})\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import os\n",
        "import re\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.profiler\n",
        "\n",
        "class Profiler:\n",
        "    \"\"\"\n",
        "    In 'init' mode: Gather detailed profiling info for each leaf layer on each Node,\n",
        "    storing results in a CSV-based ProfileDB.\n",
        "    In 'runtime' mode: Potentially gather minimal logs (optional).\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, mode: str, profile_db_path='profiling_results.csv', log_dir='logs'):\n",
        "        assert mode in ['init', 'runtime']\n",
        "        self.mode = mode\n",
        "        self.profile_db_path = profile_db_path\n",
        "        self.log_dir = log_dir\n",
        "        os.makedirs(self.log_dir, exist_ok=True)\n",
        "\n",
        "        columns = [\n",
        "            'Model', 'Layer', 'Compute',\n",
        "            'Self CPU (us)', 'CPU Total (us)', 'CUDA Total (us)',\n",
        "            'Self CPU Mem (bytes)', 'Self CUDA Mem (bytes)',\n",
        "            'Total Execution Time (us)', 'Total Memory Used (bytes)'\n",
        "        ]\n",
        "        if os.path.exists(self.profile_db_path):\n",
        "            self.profile_db = pd.read_csv(self.profile_db_path)\n",
        "        else:\n",
        "            self.profile_db = pd.DataFrame(columns=columns)\n",
        "\n",
        "        self.runtime_csv = os.path.join(self.log_dir, 'runtime_results.csv')\n",
        "        if not os.path.exists(self.runtime_csv):\n",
        "            rt_cols = ['Model', 'Layer', 'Compute', 'Execution Time (us)']\n",
        "            pd.DataFrame(columns=rt_cols).to_csv(self.runtime_csv, index=False)\n",
        "\n",
        "    def _register_hooks(self, model: nn.Module):\n",
        "        def hook_wrapper(layer_name):\n",
        "            def hook(mod, inp, out):\n",
        "                with torch.profiler.record_function(layer_name):\n",
        "                    pass\n",
        "            return hook\n",
        "\n",
        "        for idx, (name, layer) in enumerate(model.named_modules()):\n",
        "            if not isinstance(layer, nn.Sequential) and not isinstance(layer, nn.ModuleList) and layer != model:\n",
        "                layer.register_forward_hook(hook_wrapper(f\"{name}_{idx}\"))\n",
        "\n",
        "    def profile_model(self, model: nn.Module, dataloader, node, model_name: str, warmup_iters=3):\n",
        "        \"\"\"\n",
        "        Schedule a profiling task on 'node'. In 'init' mode, we gather\n",
        "        full per-layer times.\n",
        "        \"\"\"\n",
        "\n",
        "        def profiling_task():\n",
        "            device = torch.device(f\"cuda:{node.gpu}\" if node.gpu is not None and torch.cuda.is_available() else \"cpu\")\n",
        "            model.to(device)\n",
        "\n",
        "            if self.mode == 'init':\n",
        "                # warmup\n",
        "                with torch.no_grad():\n",
        "                    cnt = 0\n",
        "                    for inputs, targets in dataloader:\n",
        "                        inputs = inputs.to(device)\n",
        "                        targets = targets.to(device)\n",
        "                        model(inputs)\n",
        "                        cnt += 1\n",
        "                        if cnt >= warmup_iters:\n",
        "                            break\n",
        "                self._profile_init(model, dataloader, node, model_name, device)\n",
        "            else:\n",
        "                self._profile_runtime(model, dataloader, node, model_name, device)\n",
        "\n",
        "        rq = node.assign_task(profiling_task)\n",
        "        rq.get()  # block\n",
        "\n",
        "    def _profile_init(self, model, dataloader, node, model_name, device):\n",
        "        self._register_hooks(model)\n",
        "        with torch.profiler.profile(\n",
        "            activities=[torch.profiler.ProfilerActivity.CPU, torch.profiler.ProfilerActivity.CUDA],\n",
        "            profile_memory=True\n",
        "        ) as prof:\n",
        "            with torch.no_grad():\n",
        "                for step, (inp, tgt) in enumerate(dataloader):\n",
        "                    if step >= 1:\n",
        "                        break\n",
        "                    inp = inp.to(device)\n",
        "                    tgt = tgt.to(device)\n",
        "                    model(inp)\n",
        "                    prof.step()\n",
        "\n",
        "        stats = self._process_events(prof, model, node, runtime=False)\n",
        "        self._update_profile_db(stats, model_name, node, runtime=False)\n",
        "\n",
        "    def _profile_runtime(self, model, dataloader, node, model_name, device):\n",
        "        self._register_hooks(model)\n",
        "        with torch.no_grad():\n",
        "            for step, (inp, tgt) in enumerate(dataloader):\n",
        "                if step >= 1:\n",
        "                    break\n",
        "                inp = inp.to(device)\n",
        "                tgt = tgt.to(device)\n",
        "                with torch.profiler.profile(\n",
        "                    activities=[torch.profiler.ProfilerActivity.CPU, torch.profiler.ProfilerActivity.CUDA]\n",
        "                ) as prof:\n",
        "                    model(inp)\n",
        "                    prof.step()\n",
        "                stats = self._process_events(prof, model, node, runtime=True)\n",
        "                self._append_runtime_csv(stats, model_name, node)\n",
        "\n",
        "    def _process_events(self, profiler, model, node, runtime=False):\n",
        "        recognized = set()\n",
        "        for n, m in model.named_modules():\n",
        "            if n:\n",
        "                recognized.add(n)\n",
        "\n",
        "        aggregated = {\n",
        "            'forward_pass': dict(self_cpu_time_total=0, cpu_time_total=0, cuda_time_total=0,\n",
        "                                 self_cpu_memory_usage=0, self_cuda_memory_usage=0, compute=node.node_id),\n",
        "            'misc': dict(self_cpu_time_total=0, cpu_time_total=0, cuda_time_total=0,\n",
        "                         self_cpu_memory_usage=0, self_cuda_memory_usage=0, compute=node.node_id)\n",
        "        }\n",
        "\n",
        "        events = list(profiler.events())\n",
        "        found_root = False\n",
        "\n",
        "        def strip_suffix(s):\n",
        "            return re.sub(r'(\\.|_)\\d+$', '', s)\n",
        "\n",
        "        for e in events:\n",
        "            if e.name == \"\":\n",
        "                found_root = True\n",
        "                aggregated['forward_pass']['self_cpu_time_total'] += e.self_cpu_time_total\n",
        "                aggregated['forward_pass']['cpu_time_total'] += e.cpu_time_total\n",
        "                aggregated['forward_pass']['cuda_time_total'] += e.device_time_total\n",
        "                if not runtime:\n",
        "                    aggregated['forward_pass']['self_cpu_memory_usage'] += e.self_cpu_memory_usage\n",
        "                    aggregated['forward_pass']['self_cuda_memory_usage'] += e.self_device_memory_usage\n",
        "            else:\n",
        "                base = strip_suffix(e.name)\n",
        "                if base in recognized:\n",
        "                    if base not in aggregated:\n",
        "                        aggregated[base] = dict(\n",
        "                            self_cpu_time_total=0, cpu_time_total=0, cuda_time_total=0,\n",
        "                            self_cpu_memory_usage=0, self_cuda_memory_usage=0,\n",
        "                            compute=node.node_id\n",
        "                        )\n",
        "                    aggregated[base]['self_cpu_time_total'] += e.self_cpu_time_total\n",
        "                    aggregated[base]['cpu_time_total'] += e.cpu_time_total\n",
        "                    aggregated[base]['cuda_time_total'] += e.device_time_total\n",
        "                    if not runtime:\n",
        "                        aggregated[base]['self_cpu_memory_usage'] += e.self_cpu_memory_usage\n",
        "                        aggregated[base]['self_cuda_memory_usage'] += e.self_device_memory_usage\n",
        "                else:\n",
        "                    aggregated['misc']['self_cpu_time_total'] += e.self_cpu_time_total\n",
        "                    aggregated['misc']['cpu_time_total'] += e.cpu_time_total\n",
        "                    aggregated['misc']['cuda_time_total'] += e.device_time_total\n",
        "                    if not runtime:\n",
        "                        aggregated['misc']['self_cpu_memory_usage'] += e.self_cpu_memory_usage\n",
        "                        aggregated['misc']['self_cuda_memory_usage'] += e.self_device_memory_usage\n",
        "\n",
        "        # If no root event found, sum all into forward_pass\n",
        "        if not found_root:\n",
        "            for k in list(aggregated.keys()):\n",
        "                if k not in ('forward_pass', 'misc'):\n",
        "                    aggregated['forward_pass']['self_cpu_time_total'] += aggregated[k]['self_cpu_time_total']\n",
        "                    aggregated['forward_pass']['cpu_time_total'] += aggregated[k]['cpu_time_total']\n",
        "                    aggregated['forward_pass']['cuda_time_total'] += aggregated[k]['cuda_time_total']\n",
        "                    if not runtime:\n",
        "                        aggregated['forward_pass']['self_cpu_memory_usage'] += aggregated[k]['self_cpu_memory_usage']\n",
        "                        aggregated['forward_pass']['self_cuda_memory_usage'] += aggregated[k]['self_cuda_memory_usage']\n",
        "\n",
        "            aggregated['forward_pass']['self_cpu_time_total'] += aggregated['misc']['self_cpu_time_total']\n",
        "            aggregated['forward_pass']['cpu_time_total'] += aggregated['misc']['cpu_time_total']\n",
        "            aggregated['forward_pass']['cuda_time_total'] += aggregated['misc']['cuda_time_total']\n",
        "            if not runtime:\n",
        "                aggregated['forward_pass']['self_cpu_memory_usage'] += aggregated['misc']['self_cpu_memory_usage']\n",
        "                aggregated['forward_pass']['self_cuda_memory_usage'] += aggregated['misc']['self_cuda_memory_usage']\n",
        "\n",
        "        return aggregated\n",
        "\n",
        "    def _update_profile_db(self, stats, model_name, node, runtime=False):\n",
        "        if runtime:\n",
        "            return\n",
        "        for layer_name, data in stats.items():\n",
        "            total_t = data['cpu_time_total'] + data['cuda_time_total']\n",
        "            total_m = data['self_cpu_memory_usage'] + data['self_cuda_memory_usage']\n",
        "            row = {\n",
        "                'Model': model_name,\n",
        "                'Layer': layer_name,\n",
        "                'Compute': data['compute'],\n",
        "                'Self CPU (us)': data['self_cpu_time_total'],\n",
        "                'CPU Total (us)': data['cpu_time_total'],\n",
        "                'CUDA Total (us)': data['cuda_time_total'],\n",
        "                'Self CPU Mem (bytes)': data['self_cpu_memory_usage'],\n",
        "                'Self CUDA Mem (bytes)': data['self_cuda_memory_usage'],\n",
        "                'Total Execution Time (us)': total_t,\n",
        "                'Total Memory Used (bytes)': total_m\n",
        "            }\n",
        "            self.profile_db = self._upsert(self.profile_db, row)\n",
        "        self.profile_db.to_csv(self.profile_db_path, index=False)\n",
        "\n",
        "    def _upsert(self, df, row):\n",
        "        mask = (\n",
        "            (df['Model'] == row['Model']) &\n",
        "            (df['Layer'] == row['Layer']) &\n",
        "            (df['Compute'] == row['Compute'])\n",
        "        )\n",
        "        if not df[mask].empty:\n",
        "            existing_time = df.loc[mask, 'Total Execution Time (us)'].max()\n",
        "            if row['Total Execution Time (us)'] > existing_time:\n",
        "                for k, v in row.items():\n",
        "                    df.loc[mask, k] = v\n",
        "        else:\n",
        "            df = pd.concat([df, pd.DataFrame([row])], ignore_index=True)\n",
        "        return df\n",
        "\n",
        "    def _append_runtime_csv(self, stats, model_name, node):\n",
        "        rows = []\n",
        "        for layer_name, data in stats.items():\n",
        "            exec_time = data['cpu_time_total'] + data['cuda_time_total']\n",
        "            rows.append({\n",
        "                'Model': model_name,\n",
        "                'Layer': layer_name,\n",
        "                'Compute': data['compute'],\n",
        "                'Execution Time (us)': exec_time\n",
        "            })\n",
        "        if rows:\n",
        "            rdf = pd.read_csv(self.runtime_csv)\n",
        "            rdf = pd.concat([rdf, pd.DataFrame(rows)], ignore_index=True)\n",
        "            rdf.to_csv(self.runtime_csv, index=False)\n",
        "\n",
        "    def get_profile_db(self):\n",
        "        return self.profile_db\n",
        "\n",
        "    def print_profile_db(self):\n",
        "        if self.profile_db.empty:\n",
        "            print(\"ProfileDB is empty.\")\n",
        "        else:\n",
        "            print(\"ProfileDB:\\n\", self.profile_db.to_string(index=False))"
      ],
      "metadata": {
        "id": "7ee2Y4qbRIIe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "class Stage:\n",
        "    \"\"\"\n",
        "    A contiguous block of layers assigned to a single Node.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, stage_id: str, layers: torch.nn.ModuleList, assigned_node: 'Node'):\n",
        "        self.stage_id = stage_id\n",
        "        self.layers = layers\n",
        "        self.assigned_node = assigned_node\n",
        "\n",
        "        self.input_data = None\n",
        "        self.output_data = None\n",
        "        self.execution_time = 0.0\n",
        "\n",
        "        self.dependencies = set()\n",
        "        self.dependents = set()\n",
        "\n",
        "    def run_stage(self):\n",
        "        import time\n",
        "        start = time.time()\n",
        "        device = torch.device(\n",
        "            f\"cuda:{self.assigned_node.gpu}\" if self.assigned_node.gpu is not None and torch.cuda.is_available()\n",
        "            else \"cpu\"\n",
        "        )\n",
        "        inp = self.input_data.to(device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            out = inp\n",
        "            for layer in self.layers:\n",
        "                out = layer(out)\n",
        "\n",
        "        if device.type == 'cuda':\n",
        "            out = out.cpu()\n",
        "\n",
        "        self.output_data = out\n",
        "        end = time.time()\n",
        "        self.execution_time = end - start\n",
        "        return self.output_data"
      ],
      "metadata": {
        "id": "I2pTtaa3Rae3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import threading\n",
        "import numpy as np\n",
        "from typing import List\n",
        "\n",
        "class Scheduler:\n",
        "    \"\"\"\n",
        "    A DP-based scheduler that partitions the entire set of layers into\n",
        "    contiguous stages, assigns each stage to one node, and then\n",
        "    can run them in a pipeline-parallel manner.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, profiler: 'Profiler', nodes: List['Node'], observation_window: float):\n",
        "        self.profiler = profiler\n",
        "        self.nodes = nodes\n",
        "        self.observation_window = observation_window\n",
        "\n",
        "        # We store a global stage graph: stage_id -> {node_idx, layer_names, deps, dependents}\n",
        "        self.global_stage_graph = {}\n",
        "\n",
        "    def decompose_and_allocate(self, model: torch.nn.Module, model_name: str, k_stages: int = None):\n",
        "        \"\"\"\n",
        "        1) Identify leaf layers in a sequential order.\n",
        "        2) Use DP to form at most k_stages contiguous blocks, each assigned to a single node.\n",
        "        3) Minimizes the maximum load across all nodes.\n",
        "        4) Build the global_stage_graph with adjacency.\n",
        "        \"\"\"\n",
        "        # gather leaf layers in order\n",
        "        leaf_layers = []\n",
        "        for name, module in model.named_modules():\n",
        "            if len(list(module.children())) == 0 and name:\n",
        "                leaf_layers.append(name)\n",
        "\n",
        "        n_layers = len(leaf_layers)\n",
        "        if n_layers == 0:\n",
        "            print(\"No leaf layers found in model!\")\n",
        "            return\n",
        "\n",
        "        if k_stages is None:\n",
        "            k_stages = min(len(self.nodes), n_layers)\n",
        "\n",
        "        profile_db = self.profiler.get_profile_db()\n",
        "        model_df = profile_db[profile_db['Model'] == model_name]\n",
        "\n",
        "        # times[i][j] = time (seconds) for layer i on node j\n",
        "        times = [[0.0]*len(self.nodes) for _ in range(n_layers)]\n",
        "        for i, lname in enumerate(leaf_layers):\n",
        "            for j, node in enumerate(self.nodes):\n",
        "                row = model_df[(model_df['Layer'] == lname) & (model_df['Compute'] == node.node_id)]\n",
        "                if not row.empty:\n",
        "                    t_us = row['Total Execution Time (us)'].values[0]\n",
        "                    times[i][j] = t_us / 1e6  # convert microseconds to sec\n",
        "                else:\n",
        "                    times[i][j] = 1.0  # fallback\n",
        "\n",
        "        # build prefix sums\n",
        "        prefix_times = [[0.0]*(n_layers+1) for _ in range(len(self.nodes))]\n",
        "        for j in range(len(self.nodes)):\n",
        "            for i in range(1, n_layers+1):\n",
        "                prefix_times[j][i] = prefix_times[j][i-1] + times[i-1][j]\n",
        "\n",
        "        def block_cost(x, i, node_j):\n",
        "            return prefix_times[node_j][i] - prefix_times[node_j][x]\n",
        "\n",
        "        INF = float('inf')\n",
        "        DP = [[INF]*(k_stages+1) for _ in range(n_layers+1)]\n",
        "        back_node = [[-1]*(k_stages+1) for _ in range(n_layers+1)]\n",
        "        back_split = [[-1]*(k_stages+1) for _ in range(n_layers+1)]\n",
        "\n",
        "        DP[0][0] = 0.0\n",
        "\n",
        "        for i in range(1, n_layers+1):\n",
        "            for s in range(1, k_stages+1):\n",
        "                for j in range(len(self.nodes)):\n",
        "                    for x in range(i):\n",
        "                        c = block_cost(x, i, j)\n",
        "                        cand = max(DP[x][s-1], c)\n",
        "                        if cand < DP[i][s]:\n",
        "                            DP[i][s] = cand\n",
        "                            back_node[i][s] = j\n",
        "                            back_split[i][s] = x\n",
        "\n",
        "        min_max_load = DP[n_layers][k_stages]\n",
        "        print(f\"[Scheduler] DP => min possible max load = {min_max_load:.4f} sec\")\n",
        "\n",
        "        # reconstruct\n",
        "        i = n_layers\n",
        "        s = k_stages\n",
        "        partitions = []\n",
        "        while i > 0 and s > 0:\n",
        "            nj = back_node[i][s]\n",
        "            xx = back_split[i][s]\n",
        "            partitions.append((xx, i, nj))\n",
        "            i = xx\n",
        "            s -= 1\n",
        "\n",
        "        partitions.reverse()\n",
        "\n",
        "        # build global_stage_graph\n",
        "        self.global_stage_graph.clear()\n",
        "        idx = 0\n",
        "        for (start_i, end_i, node_j) in partitions:\n",
        "            stage_id = f\"stage-{idx}\"\n",
        "            assigned_node = self.nodes[node_j]\n",
        "            layer_subset = leaf_layers[start_i:end_i]\n",
        "\n",
        "            self.global_stage_graph[stage_id] = {\n",
        "                'node_idx': node_j,\n",
        "                'layer_names': layer_subset,\n",
        "                'dependencies': [],\n",
        "                'dependents': []\n",
        "            }\n",
        "            idx += 1\n",
        "\n",
        "        # link them linearly (stage i depends on stage i-1)\n",
        "        keys = list(self.global_stage_graph.keys())\n",
        "        for k in range(1, len(keys)):\n",
        "            prev_stg = keys[k-1]\n",
        "            curr_stg = keys[k]\n",
        "            self.global_stage_graph[curr_stg]['dependencies'].append(prev_stg)\n",
        "            self.global_stage_graph[prev_stg]['dependents'].append(curr_stg)\n",
        "\n",
        "    def create_stages_for_task(self, task: 'Task'):\n",
        "        \"\"\"\n",
        "        Convert global_stage_graph into actual Stage objects for the given task.\n",
        "        \"\"\"\n",
        "        import torch.nn as nn\n",
        "        for stage_id, info in self.global_stage_graph.items():\n",
        "            node_idx = info['node_idx']\n",
        "            node = self.nodes[node_idx]\n",
        "\n",
        "            layer_list = []\n",
        "            for lname in info['layer_names']:\n",
        "                layer_list.append(task.model.get_submodule(lname))\n",
        "\n",
        "            stg = Stage(stage_id, nn.ModuleList(layer_list), node)\n",
        "            stg.dependencies = set(info['dependencies'])\n",
        "            stg.dependents = set(info['dependents'])\n",
        "            task.stages[stage_id] = stg\n",
        "\n",
        "    def run_pipeline_parallel(self, task: 'Task'):\n",
        "        \"\"\"\n",
        "        Pipeline parallel approach:\n",
        "         - track how many dependencies remain\n",
        "         - each stage that has 0 dependencies is enqueued\n",
        "         - on completion, reduce the dependent's dep count\n",
        "        \"\"\"\n",
        "        lock = threading.Lock()\n",
        "        dep_count = {}\n",
        "        for sid, stg in task.stages.items():\n",
        "            dep_count[sid] = len(stg.dependencies)\n",
        "\n",
        "        total_stages = len(task.stages)\n",
        "        completed = 0\n",
        "        done_event = threading.Event()\n",
        "\n",
        "        def stage_done_callback(sid: str):\n",
        "            nonlocal completed\n",
        "            with lock:\n",
        "                for dep_id in task.stages[sid].dependents:\n",
        "                    dep_count[dep_id] -= 1\n",
        "                    if dep_count[dep_id] == 0:\n",
        "                        enqueue_stage(dep_id)\n",
        "                completed += 1\n",
        "                if completed >= total_stages:\n",
        "                    done_event.set()\n",
        "\n",
        "        def enqueue_stage(sid: str):\n",
        "            stg = task.stages[sid]\n",
        "\n",
        "            def run_fn():\n",
        "                return stg.run_stage()\n",
        "\n",
        "            def worker():\n",
        "                rq = stg.assigned_node.assign_task(run_fn)\n",
        "                _ = rq.get()\n",
        "                stage_done_callback(sid)\n",
        "\n",
        "            threading.Thread(target=worker, daemon=True).start()\n",
        "\n",
        "        # initial\n",
        "        for sid, c in dep_count.items():\n",
        "            if c == 0:\n",
        "                enqueue_stage(sid)\n",
        "\n",
        "        done_event.wait()"
      ],
      "metadata": {
        "id": "dU1ezvA3RTl2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import Dict, Optional\n",
        "\n",
        "class Task:\n",
        "    \"\"\"\n",
        "    Represents a single inference pass for a model + input.\n",
        "    Stages are built by the Scheduler's create_stages_for_task call.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, task_id: str, model: torch.nn.Module, input_data: torch.Tensor, model_name: str):\n",
        "        self.task_id = task_id\n",
        "        self.model = model\n",
        "        self.input_data = input_data\n",
        "        self.model_name = model_name\n",
        "\n",
        "        self.stages: Dict[str, Stage] = {}\n",
        "\n",
        "        self.start_time: Optional[float] = None\n",
        "        self.finish_time: Optional[float] = None\n",
        "\n",
        "    def get_total_execution_time(self):\n",
        "        if self.start_time and self.finish_time:\n",
        "            return self.finish_time - self.start_time\n",
        "        return 0.0\n",
        "\n",
        "\n",
        "############################################################\n",
        "# 6) Taskset Class\n",
        "############################################################\n",
        "\n",
        "import time\n",
        "\n",
        "class Taskset:\n",
        "    \"\"\"\n",
        "    A collection of tasks to run with the DP-based pipeline approach.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, tasks: List[Task], scheduler: Scheduler):\n",
        "        self.tasks = tasks\n",
        "        self.scheduler = scheduler\n",
        "\n",
        "    def schedule_all_tasks(self):\n",
        "        # Build stage objects for each Task from the global stage graph\n",
        "        for t in self.tasks:\n",
        "            self.scheduler.create_stages_for_task(t)\n",
        "\n",
        "    def execute_all(self):\n",
        "        # We run each task in parallel\n",
        "        import threading\n",
        "        start_global = time.time()\n",
        "        threads = []\n",
        "\n",
        "        def run_task(task: Task):\n",
        "            task.start_time = time.time()\n",
        "            self.scheduler.run_pipeline_parallel(task)\n",
        "            task.finish_time = time.time()\n",
        "\n",
        "        for t in self.tasks:\n",
        "            th = threading.Thread(target=run_task, args=(t,), daemon=True)\n",
        "            th.start()\n",
        "            threads.append(th)\n",
        "\n",
        "        for th in threads:\n",
        "            th.join()\n",
        "\n",
        "        end_global = time.time()\n",
        "        print(f\"[Taskset] All tasks completed in {end_global - start_global:.2f}s\")\n",
        "\n",
        "    def collect_metrics(self):\n",
        "        # For demonstration, we do not track node usage in detail here.\n",
        "        return {}"
      ],
      "metadata": {
        "id": "WCpoeZQPaoyu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Evaluator:\n",
        "    \"\"\"\n",
        "    Compares naive single-device run vs. pipeline approach.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, device='cpu'):\n",
        "        self.device = device\n",
        "        self.naive_run_time = None\n",
        "        self.pipeline_run_time = None\n",
        "\n",
        "    def run_naive_pytorch(self, tasks: List[Task]):\n",
        "        \"\"\"\n",
        "        Runs each Task sequentially on self.device (layer-by-layer in order).\n",
        "        Measures total time across all tasks.\n",
        "        \"\"\"\n",
        "        import time\n",
        "        dev = torch.device(self.device)\n",
        "        start = time.time()\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for task in tasks:\n",
        "                task.model.to(dev)\n",
        "                out = task.model(task.input_data.to(dev))\n",
        "                _ = out.cpu() if dev.type == 'cuda' else out\n",
        "\n",
        "        end = time.time()\n",
        "        self.naive_run_time = end - start\n",
        "        print(f\"[Evaluator] Naive run on device={self.device} for {len(tasks)} tasks => {self.naive_run_time:.2f}s\")\n",
        "\n",
        "    def run_pipeline(self, taskset: Taskset):\n",
        "        import time\n",
        "        start = time.time()\n",
        "        taskset.schedule_all_tasks()\n",
        "        taskset.execute_all()\n",
        "        end = time.time()\n",
        "\n",
        "        self.pipeline_run_time = end - start\n",
        "        print(f\"[Evaluator] Pipeline approach => {self.pipeline_run_time:.2f}s\")\n",
        "\n",
        "    def compare_results(self, node_usage_info):\n",
        "        print(\"\\n=== Comparison ===\")\n",
        "        if self.naive_run_time is None or self.pipeline_run_time is None:\n",
        "            print(\"Cannot compare, missing times.\")\n",
        "            return\n",
        "        print(f\"Naive total : {self.naive_run_time:.2f}s\")\n",
        "        print(f\"Pipeline total : {self.pipeline_run_time:.2f}s\")\n",
        "        if self.pipeline_run_time > 0:\n",
        "            speedup = self.naive_run_time / self.pipeline_run_time\n",
        "        else:\n",
        "            speedup = 1.0\n",
        "        print(f\"Speedup = {speedup:.2f}x\")\n",
        "\n",
        "        if node_usage_info:\n",
        "            print(\"Node Usage Info:\")\n",
        "            for k, v in node_usage_info.items():\n",
        "                print(f\"  {k}: {v}\")\n"
      ],
      "metadata": {
        "id": "5FibX3D8auYW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# def main_demo():\n",
        "#     import torch\n",
        "#     import torch.nn as nn\n",
        "#     import torch.utils.data as data\n",
        "#     import gc  # for Python garbage collection\n",
        "\n",
        "#     # 1) Build a sample model\n",
        "#     class BiggerFFN(nn.Module):\n",
        "#         def __init__(self):\n",
        "#             super().__init__()\n",
        "#             self.net = nn.Sequential(\n",
        "#                 nn.Linear(256, 512),\n",
        "#                 nn.ReLU(),\n",
        "#                 nn.Linear(512, 256),\n",
        "#                 nn.ReLU(),\n",
        "#                 nn.Linear(256, 128),\n",
        "#                 nn.ReLU(),\n",
        "#                 nn.Linear(128, 10)\n",
        "#             )\n",
        "#         def forward(self, x):\n",
        "#             return self.net(x)\n",
        "\n",
        "#     model = BiggerFFN()\n",
        "#     input_tensor = torch.randn(1, 256)\n",
        "\n",
        "#     # 2) A dummy dataset for profiling\n",
        "#     class DummyDataset(data.Dataset):\n",
        "#         def __init__(self, size=5):\n",
        "#             self.size = size\n",
        "#         def __len__(self):\n",
        "#             return self.size\n",
        "#         def __getitem__(self, idx):\n",
        "#             return torch.randn(1, 256), torch.tensor(0)\n",
        "\n",
        "#     dloader = data.DataLoader(DummyDataset(), batch_size=1)\n",
        "\n",
        "#     # 3) Profiler in 'init' mode\n",
        "#     profiler_init = Profiler(mode='init', profile_db_path='dp_partition_profiling.csv')\n",
        "\n",
        "#     # 4) Discover Nodes\n",
        "#     nodes = Node.discover_nodes()\n",
        "#     if not nodes:\n",
        "#         print(\"No nodes discovered!\")\n",
        "#         return\n",
        "\n",
        "#     print(\"Discovered Nodes (up to first 4):\")\n",
        "#     for nd in nodes[:4]:\n",
        "#         print(\" \", nd)\n",
        "#     if len(nodes) > 4:\n",
        "#         print(\"  ...\")\n",
        "\n",
        "#     # 5) Profile the model on each node\n",
        "#     model_name = \"Partitioned_BiggerFFN\"\n",
        "#     for node in nodes:\n",
        "#         print(f\"\\nProfiling model on {node.node_id} ...\")\n",
        "#         profiler_init.profile_model(model, dloader, node, model_name)\n",
        "\n",
        "#     # 6) Build DP-based Scheduler\n",
        "#     df = profiler_init.get_profile_db()\n",
        "#     total_us = df[df['Model'] == model_name]['Total Execution Time (us)'].sum()\n",
        "#     total_s = total_us / 1e6\n",
        "#     observation_window = total_s*1.1  # arbitrary slack\n",
        "\n",
        "#     scheduler = Scheduler(profiler_init, nodes, observation_window)\n",
        "#     scheduler.decompose_and_allocate(model, model_name, k_stages=None)  # let it choose up to len(nodes) or n_layers\n",
        "\n",
        "#     # 7) Create multiple tasks\n",
        "#     task1 = Task(\"Task1\", model, input_tensor.clone(), model_name)\n",
        "#     task2 = Task(\"Task2\", model, input_tensor.clone(), model_name)\n",
        "#     task3 = Task(\"Task3\", model, input_tensor.clone(), model_name)\n",
        "#     tasks = [task1, task2, task3]\n",
        "\n",
        "#     # 8) Create a Taskset\n",
        "#     taskset = Taskset(tasks, scheduler)\n",
        "\n",
        "#     # 9) Evaluator\n",
        "#     evaluator = Evaluator(device='gpu')\n",
        "\n",
        "#     # 10) Naive run\n",
        "#     evaluator.run_naive_pytorch(tasks)\n",
        "\n",
        "#     # **CLEAR CACHES** after naive run, before pipeline run\n",
        "#     # (If running on GPU, torch.cuda.empty_cache() helps free memory\n",
        "#     #  also we can do Python-level gc to ensure minimal leftover references)\n",
        "#     if torch.cuda.is_available():\n",
        "#         torch.cuda.empty_cache()\n",
        "#     gc.collect()\n",
        "\n",
        "#     # 11) Pipeline run\n",
        "#     print(\"\\n=== Running pipeline approach (DP-based grouping) ===\")\n",
        "#     evaluator.run_pipeline(taskset)\n",
        "\n",
        "#     # 12) Compare results\n",
        "#     usage_info = taskset.collect_metrics()\n",
        "#     evaluator.compare_results(usage_info)\n",
        "\n",
        "#     # 13) Stop all nodes\n",
        "#     for nd in nodes:\n",
        "#         nd.stop()\n",
        "\n",
        "#     # Print final ProfileDB\n",
        "#     print(\"\\nFinal ProfileDB:\")\n",
        "#     profiler_init.print_profile_db()\n"
      ],
      "metadata": {
        "id": "WlaNwF7ra1gI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.utils.data as data\n",
        "import gc  # for garbage collection if needed\n",
        "import torchvision.models as models  # for ResNet\n",
        "\n",
        "def main_demo():\n",
        "\n",
        "    #####################################################\n",
        "    # 1) Example Models Definition\n",
        "    #####################################################\n",
        "\n",
        "    # a) Our original BiggerFFN model\n",
        "    class BiggerFFN(nn.Module):\n",
        "        def __init__(self):\n",
        "            super().__init__()\n",
        "            self.net = nn.Sequential(\n",
        "                nn.Linear(256, 512),\n",
        "                nn.ReLU(),\n",
        "                nn.Linear(512, 256),\n",
        "                nn.ReLU(),\n",
        "                nn.Linear(256, 128),\n",
        "                nn.ReLU(),\n",
        "                nn.Linear(128, 10)\n",
        "            )\n",
        "        def forward(self, x):\n",
        "            return self.net(x)\n",
        "\n",
        "    # b) A ResNet18 (untrained) from torchvision\n",
        "    resnet_model = models.resnet18(pretrained=False)\n",
        "    # By default, ResNet18 expects input shape [N, 3, H, W], e.g. [1, 3, 224, 224]\n",
        "\n",
        "    # We'll use these model names to store separate profiles\n",
        "    ffn_model_name = \"Partitioned_BiggerFFN\"\n",
        "    resnet_model_name = \"Partitioned_ResNet18\"\n",
        "\n",
        "    #####################################################\n",
        "    # 2) Dummy Datasets + Dataloaders for Profiling\n",
        "    #####################################################\n",
        "\n",
        "    # a) For BiggerFFN, we want an input of shape (1,256)\n",
        "    class FFNDataset(data.Dataset):\n",
        "        def __init__(self, size=5):\n",
        "            self.size = size\n",
        "        def __len__(self):\n",
        "            return self.size\n",
        "        def __getitem__(self, idx):\n",
        "            # Return (input_tensor, target), matching shape [1,256]\n",
        "            return torch.randn(1, 256), torch.tensor(0)\n",
        "\n",
        "    # b) For ResNet18, we want an input of shape [1,3,224,224]\n",
        "    class ResNetDataset(data.Dataset):\n",
        "        def __init__(self, size=5):\n",
        "            self.size = size\n",
        "        def __len__(self):\n",
        "            return self.size\n",
        "        def __getitem__(self, idx):\n",
        "            # Typically ResNet18 expects 3-channel images\n",
        "            return torch.randn(3, 224, 224), torch.tensor(0)\n",
        "\n",
        "    # Instantiate data loaders for each model\n",
        "    ffn_loader = data.DataLoader(FFNDataset(), batch_size=1)\n",
        "    resnet_loader = data.DataLoader(ResNetDataset(), batch_size=1)\n",
        "\n",
        "    #####################################################\n",
        "    #    INIT PHASE\n",
        "    #####################################################\n",
        "    def init_phase():\n",
        "        print(\"=== INIT PHASE ===\\n\")\n",
        "\n",
        "        # 1) Create a Profiler in 'init' mode\n",
        "        profiler_init = Profiler(mode='init', profile_db_path='dp_partition_profiling.csv')\n",
        "\n",
        "        # 2) Discover nodes\n",
        "        nodes = Node.discover_nodes()\n",
        "        if not nodes:\n",
        "            print(\"No nodes discovered! Exiting init phase.\")\n",
        "            return None, None, None\n",
        "\n",
        "        print(\"Discovered Nodes (up to first 4):\")\n",
        "        for nd in nodes[:4]:\n",
        "            print(\" \", nd)\n",
        "        if len(nodes) > 4:\n",
        "            print(\"  ...\")\n",
        "\n",
        "        # 3) Profile BOTH models on each node\n",
        "\n",
        "        # a) Profile the BiggerFFN\n",
        "        print(\"\\n[INIT] Profiling BiggerFFN on each node ...\")\n",
        "        for node in nodes:\n",
        "            profiler_init.profile_model(BiggerFFN(), ffn_loader, node, ffn_model_name)\n",
        "\n",
        "        # b) Profile the ResNet18\n",
        "        print(\"\\n[INIT] Profiling ResNet18 on each node ...\")\n",
        "        for node in nodes:\n",
        "            profiler_init.profile_model(resnet_model, resnet_loader, node, resnet_model_name)\n",
        "\n",
        "        # 4) Build TWO DP-based schedulers (one for each model),\n",
        "        #    because each model has different layer sets.\n",
        "\n",
        "        # ---- Scheduler for BiggerFFN ----\n",
        "        df = profiler_init.get_profile_db()\n",
        "        total_us_ffn = df[df['Model'] == ffn_model_name]['Total Execution Time (us)'].sum()\n",
        "        total_s_ffn = total_us_ffn / 1e6\n",
        "        obs_window_ffn = total_s_ffn * 1.1  # 10% slack\n",
        "\n",
        "        scheduler_ffn = Scheduler(profiler_init, nodes, obs_window_ffn)\n",
        "        scheduler_ffn.decompose_and_allocate(BiggerFFN(), ffn_model_name, k_stages=None)\n",
        "\n",
        "        # ---- Scheduler for ResNet18 ----\n",
        "        total_us_res = df[df['Model'] == resnet_model_name]['Total Execution Time (us)'].sum()\n",
        "        total_s_res = total_us_res / 1e6\n",
        "        obs_window_res = total_s_res * 1.1\n",
        "\n",
        "        scheduler_res = Scheduler(profiler_init, nodes, obs_window_res)\n",
        "        scheduler_res.decompose_and_allocate(resnet_model, resnet_model_name, k_stages=None)\n",
        "\n",
        "        return profiler_init, nodes, (scheduler_ffn, scheduler_res)\n",
        "\n",
        "    #####################################################\n",
        "    #    RUNTIME PHASE\n",
        "    #####################################################\n",
        "    def runtime_phase(profiler_init, nodes, schedulers):\n",
        "        print(\"\\n=== RUNTIME PHASE ===\\n\")\n",
        "\n",
        "        scheduler_ffn, scheduler_res = schedulers\n",
        "\n",
        "        # a) Create tasks for each model\n",
        "        # - For the FFN, shape [1,256]\n",
        "        input_ffn = torch.randn(1, 256)\n",
        "        task_ffn1 = Task(\"TaskFFN1\", BiggerFFN(), input_ffn.clone(), ffn_model_name)\n",
        "        task_ffn2 = Task(\"TaskFFN2\", BiggerFFN(), input_ffn.clone(), ffn_model_name)\n",
        "\n",
        "        # - For the ResNet, shape [1,3,224,224]\n",
        "        input_res = torch.randn(1, 3, 224, 224)\n",
        "        task_res1 = Task(\"TaskRes1\", resnet_model, input_res.clone(), resnet_model_name)\n",
        "        task_res2 = Task(\"TaskRes2\", resnet_model, input_res.clone(), resnet_model_name)\n",
        "\n",
        "        # We'll do a separate pipeline run for FFN tasks vs. ResNet tasks\n",
        "        # because each set uses a different scheduler/stage decomposition.\n",
        "\n",
        "        ############################################\n",
        "        # b) Evaluate FFN tasks\n",
        "        ############################################\n",
        "        tasks_ffn = [task_ffn1, task_ffn2]\n",
        "        taskset_ffn = Taskset(tasks_ffn, scheduler_ffn)\n",
        "        evaluator_ffn = Evaluator(device='cuda')\n",
        "\n",
        "        print(\"\\n--- Evaluating BiggerFFN Tasks ---\")\n",
        "\n",
        "        # (1) Naive\n",
        "        evaluator_ffn.run_naive_pytorch(tasks_ffn)\n",
        "\n",
        "        # Clear caches if desired\n",
        "        if torch.cuda.is_available():\n",
        "            torch.cuda.empty_cache()\n",
        "        gc.collect()\n",
        "\n",
        "        # (2) Pipeline\n",
        "        print(\"\\n=== Running pipeline approach for BiggerFFN ===\")\n",
        "        evaluator_ffn.run_pipeline(taskset_ffn)\n",
        "\n",
        "        usage_info_ffn = taskset_ffn.collect_metrics()\n",
        "        evaluator_ffn.compare_results(usage_info_ffn)\n",
        "\n",
        "        # c) Evaluate ResNet tasks\n",
        "        tasks_res = [task_res1, task_res2]\n",
        "        taskset_res = Taskset(tasks_res, scheduler_res)\n",
        "        evaluator_res = Evaluator(device='cuda')\n",
        "\n",
        "        print(\"\\n--- Evaluating ResNet18 Tasks ---\")\n",
        "\n",
        "        # (1) Naive\n",
        "        evaluator_res.run_naive_pytorch(tasks_res)\n",
        "\n",
        "        # Clear caches if desired\n",
        "        if torch.cuda.is_available():\n",
        "            torch.cuda.empty_cache()\n",
        "        gc.collect()\n",
        "\n",
        "        # (2) Pipeline\n",
        "        print(\"\\n=== Running pipeline approach for ResNet18 ===\")\n",
        "        evaluator_res.run_pipeline(taskset_res)\n",
        "\n",
        "        usage_info_res = taskset_res.collect_metrics()\n",
        "        evaluator_res.compare_results(usage_info_res)\n",
        "\n",
        "        tasks_ffn_resnet = [task_ffn1, task_ffn2,task_res1,task_res2]\n",
        "        taskset_ffn = Taskset(tasks_ffn_resnet, scheduler_ffn)\n",
        "        evaluator_ffn = Evaluator(device='cuda')\n",
        "        # d) Stop nodes\n",
        "        for nd in nodes:\n",
        "            nd.stop()\n",
        "\n",
        "        # e) Print final ProfileDB\n",
        "        print(\"\\nFinal ProfileDB:\")\n",
        "        profiler_init.print_profile_db()\n",
        "\n",
        "    #####################################################\n",
        "    # main_demo flow\n",
        "    #####################################################\n",
        "\n",
        "    profiler_init, nodes, schedulers = init_phase()\n",
        "    if profiler_init is not None and nodes is not None and schedulers is not None:\n",
        "        runtime_phase(profiler_init, nodes, schedulers)\n",
        "    else:\n",
        "        print(\"Init phase failed. No runtime execution.\")\n"
      ],
      "metadata": {
        "id": "vImtaM7zcoIr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # Suppose all your classes are in a module named pipeline_lib\n",
        "# # For example:\n",
        "# # from pipeline_lib import Node, Profiler, Stage, Scheduler, Task, Taskset, Evaluator\n",
        "\n",
        "# import torch\n",
        "# import torch.nn as nn\n",
        "# import torch.utils.data as data\n",
        "# import gc\n",
        "# import torchvision.models as models\n",
        "\n",
        "# def demo_single_scheduler():\n",
        "#     \"\"\"\n",
        "#     Demonstrates a SINGLE unified scheduler that can handle multiple models\n",
        "#     by storing multiple stage graphs internally. The user only sees a single 'Scheduler'.\n",
        "#     \"\"\"\n",
        "\n",
        "#     print(\"\\n=== Demo: Single Unified Scheduler ===\")\n",
        "\n",
        "#     # 1) Define two models\n",
        "#     class BiggerFFN(nn.Module):\n",
        "#         def __init__(self):\n",
        "#             super().__init__()\n",
        "#             self.net = nn.Sequential(\n",
        "#                 nn.Linear(256, 512),\n",
        "#                 nn.ReLU(),\n",
        "#                 nn.Linear(512, 256),\n",
        "#                 nn.ReLU(),\n",
        "#                 nn.Linear(256, 128),\n",
        "#                 nn.ReLU(),\n",
        "#                 nn.Linear(128, 10)\n",
        "#             )\n",
        "#         def forward(self, x):\n",
        "#             return self.net(x)\n",
        "\n",
        "#     resnet_model = models.resnet18(pretrained=False)\n",
        "#     ffn_model_name = \"Partitioned_BiggerFFN\"\n",
        "#     resnet_model_name = \"Partitioned_ResNet18\"\n",
        "\n",
        "#     # 2) Profile both models\n",
        "#     profiler_init = Profiler(mode='init', profile_db_path='profiling_results_single_sched.csv')\n",
        "#     nodes = Node.discover_nodes()\n",
        "#     if not nodes:\n",
        "#         print(\"No nodes discovered, exiting.\")\n",
        "#         return\n",
        "\n",
        "#     # trivial data\n",
        "#     class FFNDataset(data.Dataset):\n",
        "#         def __len__(self):\n",
        "#             return 5\n",
        "#         def __getitem__(self, idx):\n",
        "#             return torch.randn(1,256), torch.tensor(0)\n",
        "\n",
        "#     class ResNetDataset(data.Dataset):\n",
        "#         def __len__(self):\n",
        "#             return 5\n",
        "#         def __getitem__(self, idx):\n",
        "#             return torch.randn(3,224,224), torch.tensor(0)\n",
        "\n",
        "#     ffn_loader = data.DataLoader(FFNDataset(), batch_size=1)\n",
        "#     resnet_loader = data.DataLoader(ResNetDataset(), batch_size=1)\n",
        "\n",
        "#     # Profile each model on each node\n",
        "#     for node in nodes:\n",
        "#         profiler_init.profile_model(BiggerFFN(), ffn_loader, node, ffn_model_name)\n",
        "#         profiler_init.profile_model(resnet_model, resnet_loader, node, resnet_model_name)\n",
        "\n",
        "#     df = profiler_init.get_profile_db()\n",
        "\n",
        "#     # 3) Single \"MultiModelScheduler\"\n",
        "#     class MultiModelScheduler(Scheduler):\n",
        "#         def __init__(self, profiler, nodes):\n",
        "#             # We won't call parent init with observation_window because each model might differ\n",
        "#             # We'll store a dict of model_name -> (observation_window, stage_graph, etc.)\n",
        "#             self.profiler = profiler\n",
        "#             self.nodes = nodes\n",
        "#             self.model_plans = {}  # e.g. { model_name: { 'obs_window': X, 'stage_graph': {...} } }\n",
        "\n",
        "#         def decompose_and_allocate_multi(self, model_obj, model_name):\n",
        "#             \"\"\"\n",
        "#             Similar to decompose_and_allocate, but we store the result in self.model_plans[model_name].\n",
        "#             \"\"\"\n",
        "#             # Build times\n",
        "#             df_local = self.profiler.get_profile_db()\n",
        "#             sub = df_local[df_local['Model'] == model_name]\n",
        "#             total_us = sub['Total Execution Time (us)'].sum()\n",
        "#             total_s = total_us / 1e6\n",
        "#             obs_window = total_s * 1.2  # some slack\n",
        "\n",
        "#             # We'll do a partial approach: call parent's logic but store in a dictionary\n",
        "#             # For that, we might define parent's DP method as a function we can call\n",
        "#             # or replicate the code. For brevity, let's replicate a shorter version:\n",
        "\n",
        "#             # Build list of leaf layers, compute times, do DP, store stage graph\n",
        "#             # We'll store stage_graph in self.model_plans[model_name]\n",
        "#             # ... (omitting the full DP code for brevity)...\n",
        "\n",
        "#             super().__init__(self.profiler, self.nodes, obs_window)  # we hacky call the parent init\n",
        "\n",
        "#             # Actually call parent's \"decompose_and_allocate\"\n",
        "#             super().decompose_and_allocate(model_obj, model_name, k_stages=None)\n",
        "\n",
        "#             # Now store the parent's result\n",
        "#             plan = self.global_stage_graph\n",
        "#             self.model_plans[model_name] = {\n",
        "#                 'obs_window': obs_window,\n",
        "#                 'stage_graph': plan\n",
        "#             }\n",
        "#             # Clear parent's global_stage_graph so we can do the next model\n",
        "#             self.global_stage_graph = {}\n",
        "\n",
        "#         def create_stages_for_task(self, task):\n",
        "#             \"\"\"\n",
        "#             For a task referencing 'task.model_name', we retrieve the stored stage_graph.\n",
        "#             Then we build Stage objects accordingly.\n",
        "#             \"\"\"\n",
        "#             if task.model_name not in self.model_plans:\n",
        "#                 raise ValueError(f\"No stage graph found for model {task.model_name}\")\n",
        "#             stage_graph = self.model_plans[task.model_name]['stage_graph']\n",
        "\n",
        "#             import torch.nn as nn\n",
        "#             for stage_id, info in stage_graph.items():\n",
        "#                 node_idx = info['node_idx']\n",
        "#                 node = self.nodes[node_idx]\n",
        "\n",
        "#                 layer_list = []\n",
        "#                 for lname in info['layer_names']:\n",
        "#                     layer_list.append(task.model.get_submodule(lname))\n",
        "\n",
        "#                 stg = Stage(stage_id, nn.ModuleList(layer_list), node)\n",
        "#                 stg.dependencies = set(info['dependencies'])\n",
        "#                 stg.dependents = set(info['dependents'])\n",
        "#                 task.stages[stage_id] = stg\n",
        "\n",
        "#         def run_pipeline_parallel(self, task):\n",
        "#             \"\"\"\n",
        "#             Same pipeline approach, but we do it for the task's stage objects.\n",
        "#             \"\"\"\n",
        "#             # We can reuse the parent's method\n",
        "#             super().run_pipeline_parallel(task)\n",
        "\n",
        "#     # Instantiate the multi-model scheduler\n",
        "#     multi_sched = MultiModelScheduler(profiler_init, nodes)\n",
        "\n",
        "#     # Decompose both models\n",
        "#     multi_sched.decompose_and_allocate_multi(BiggerFFN(), ffn_model_name)\n",
        "#     multi_sched.decompose_and_allocate_multi(resnet_model, resnet_model_name)\n",
        "\n",
        "#     # 4) Create tasks\n",
        "#     input_ffn = torch.randn(1, 256)\n",
        "#     task_ffn1 = Task(\"TaskFFN1\", BiggerFFN(), input_ffn.clone(), ffn_model_name)\n",
        "#     task_ffn2 = Task(\"TaskFFN2\", BiggerFFN(), input_ffn.clone(), ffn_model_name)\n",
        "\n",
        "#     input_res = torch.randn(1, 3, 224, 224)\n",
        "#     task_res1 = Task(\"TaskRes1\", resnet_model, input_res.clone(), resnet_model_name)\n",
        "#     task_res2 = Task(\"TaskRes2\", resnet_model, input_res.clone(), resnet_model_name)\n",
        "\n",
        "#     tasks_all = [task_ffn1, task_ffn2, task_res1, task_res2]\n",
        "\n",
        "#     # 5) Single Taskset referencing multi_sched\n",
        "#     class MultiModelTaskset:\n",
        "#         def __init__(self, tasks, scheduler):\n",
        "#             self.tasks = tasks\n",
        "#             self.scheduler = scheduler\n",
        "#         def schedule_all_tasks(self):\n",
        "#             for t in self.tasks:\n",
        "#                 self.scheduler.create_stages_for_task(t)\n",
        "#         def execute_all(self):\n",
        "#             import time, threading\n",
        "#             start = time.time()\n",
        "#             threads = []\n",
        "#             def worker(task):\n",
        "#                 self.scheduler.run_pipeline_parallel(task)\n",
        "#             for t in self.tasks:\n",
        "#                 th = threading.Thread(target=worker, args=(t,), daemon=True)\n",
        "#                 th.start()\n",
        "#                 threads.append(th)\n",
        "#             for th in threads:\n",
        "#                 th.join()\n",
        "#             end = time.time()\n",
        "#             print(f\"[SingleScheduler] All tasks done in {end-start:.4f}s\")\n",
        "#         def collect_metrics(self):\n",
        "#             return {}\n",
        "\n",
        "#     mm_taskset = MultiModelTaskset(tasks_all, multi_sched)\n",
        "\n",
        "#     # 6) Evaluate\n",
        "#     evaluator = Evaluator(device='cuda')\n",
        "#     evaluator.run_naive_pytorch(tasks_all)\n",
        "\n",
        "#     if torch.cuda.is_available():\n",
        "#         torch.cuda.empty_cache()\n",
        "#     gc.collect()\n",
        "\n",
        "#     print(\"\\n=== Pipeline for All Tasks (Single Unified Scheduler) ===\")\n",
        "#     evaluator.run_pipeline(mm_taskset)\n",
        "#     usage_info = mm_taskset.collect_metrics()\n",
        "#     evaluator.compare_results(usage_info)\n",
        "\n",
        "#     # Stop nodes\n",
        "#     for nd in nodes:\n",
        "#         nd.stop()\n",
        "\n",
        "#     print(\"\\n=== Done: Single Unified Scheduler Demo ===\")\n",
        "#     profiler_init.print_profile_db()"
      ],
      "metadata": {
        "id": "k0DiDjgMig0-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "demo_single_scheduler()"
      ],
      "metadata": {
        "id": "JkKsY-yrirFa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# main_demo()"
      ],
      "metadata": {
        "id": "n8qqAmVDa-Ym"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # -*- coding: utf-8 -*-\n",
        "# \"\"\"Poc_final_dart.ipynb\n",
        "\n",
        "# Automatically generated by Colab.\n",
        "# \"\"\"\n",
        "\n",
        "# import os\n",
        "# import torch\n",
        "# import threading\n",
        "# import queue\n",
        "# from typing import Callable, Any, List\n",
        "\n",
        "# ############################################\n",
        "# # 1) Node Class (Unchanged)\n",
        "# ############################################\n",
        "\n",
        "# class Node:\n",
        "#     def __init__(self, node_id: str, cpus=None, gpu=None):\n",
        "#         self._node_id = node_id\n",
        "#         self._cpus = tuple(cpus or [])\n",
        "#         self._gpu = gpu\n",
        "\n",
        "#         self._original_affinity = os.sched_getaffinity(0)\n",
        "#         self._task_queue = queue.Queue()\n",
        "#         self._stop_signal = False\n",
        "\n",
        "#         self._worker_thread = threading.Thread(target=self._worker_loop, daemon=True)\n",
        "#         self._worker_thread.start()\n",
        "\n",
        "#     @property\n",
        "#     def node_id(self):\n",
        "#         return self._node_id\n",
        "\n",
        "#     @property\n",
        "#     def cpus(self):\n",
        "#         return self._cpus\n",
        "\n",
        "#     @property\n",
        "#     def gpu(self):\n",
        "#         return self._gpu\n",
        "\n",
        "#     def assign_task(self, func: Callable, *args, **kwargs) -> queue.Queue:\n",
        "#         result_queue = queue.Queue(maxsize=1)\n",
        "#         self._task_queue.put((func, args, kwargs, result_queue))\n",
        "#         return result_queue\n",
        "\n",
        "#     def stop(self):\n",
        "#         self._stop_signal = True\n",
        "#         self._task_queue.put(None)\n",
        "#         self._worker_thread.join()\n",
        "\n",
        "#     def _worker_loop(self):\n",
        "#         while not self._stop_signal:\n",
        "#             item = self._task_queue.get()\n",
        "#             if item is None:\n",
        "#                 break\n",
        "#             func, args, kwargs, result_queue = item\n",
        "#             try:\n",
        "#                 self._set_context()\n",
        "#                 result = func(*args, **kwargs)\n",
        "#             except Exception as e:\n",
        "#                 result = e\n",
        "#             finally:\n",
        "#                 self._reset_context()\n",
        "\n",
        "#             result_queue.put(result)\n",
        "\n",
        "#     def _set_context(self):\n",
        "#         if self._cpus:\n",
        "#             os.sched_setaffinity(0, self._cpus)\n",
        "#         if self._gpu is not None and torch.cuda.is_available():\n",
        "#             torch.cuda.set_device(self._gpu)\n",
        "\n",
        "#     def _reset_context(self):\n",
        "#         os.sched_setaffinity(0, self._original_affinity)\n",
        "\n",
        "#     @staticmethod\n",
        "#     def discover_nodes() -> List['Node']:\n",
        "#         nodes = []\n",
        "#         num_cpus = os.cpu_count() or 1\n",
        "#         ngpus = torch.cuda.device_count()\n",
        "#         for core_id in range(num_cpus):\n",
        "#             node = Node(node_id=f\"CPU-{core_id}\", cpus=[core_id])\n",
        "#             nodes.append(node)\n",
        "#         for g in range(ngpus):\n",
        "#             for core_id in range(num_cpus):\n",
        "#                 node = Node(node_id=f\"GPU-{g}-CPU-{core_id}\", cpus=[core_id], gpu=g)\n",
        "#                 nodes.append(node)\n",
        "#         return nodes\n",
        "\n",
        "#     def __repr__(self):\n",
        "#         return f\"Node({self._node_id}, cpus={self._cpus}, gpu={self._gpu})\"\n",
        "\n",
        "\n",
        "# ############################################\n",
        "# # 2) Profiler Class (Unchanged)\n",
        "# ############################################\n",
        "\n",
        "# import time\n",
        "# import os\n",
        "# import re\n",
        "# import pandas as pd\n",
        "# import torch\n",
        "# import torch.nn as nn\n",
        "# import torch.profiler\n",
        "\n",
        "# class Profiler:\n",
        "#     def __init__(self, mode: str, profile_db_path='profiling_results.csv', log_dir='logs'):\n",
        "#         assert mode in ['init', 'runtime']\n",
        "#         self.mode = mode\n",
        "#         self.profile_db_path = profile_db_path\n",
        "#         self.log_dir = log_dir\n",
        "#         os.makedirs(self.log_dir, exist_ok=True)\n",
        "\n",
        "#         columns = [\n",
        "#             'Model', 'Layer', 'Compute',\n",
        "#             'Self CPU (us)', 'CPU Total (us)', 'CUDA Total (us)',\n",
        "#             'Self CPU Mem (bytes)', 'Self CUDA Mem (bytes)',\n",
        "#             'Total Execution Time (us)', 'Total Memory Used (bytes)'\n",
        "#         ]\n",
        "#         if os.path.exists(self.profile_db_path):\n",
        "#             self.profile_db = pd.read_csv(self.profile_db_path)\n",
        "#         else:\n",
        "#             self.profile_db = pd.DataFrame(columns=columns)\n",
        "\n",
        "#         self.runtime_csv = os.path.join(self.log_dir, 'runtime_results.csv')\n",
        "#         if not os.path.exists(self.runtime_csv):\n",
        "#             rt_cols = ['Model', 'Layer', 'Compute', 'Execution Time (us)']\n",
        "#             pd.DataFrame(columns=rt_cols).to_csv(self.runtime_csv, index=False)\n",
        "\n",
        "#     def _register_hooks(self, model: nn.Module):\n",
        "#         def hook_wrapper(layer_name):\n",
        "#             def hook(mod, inp, out):\n",
        "#                 with torch.profiler.record_function(layer_name):\n",
        "#                     pass\n",
        "#             return hook\n",
        "\n",
        "#         for idx, (name, layer) in enumerate(model.named_modules()):\n",
        "#             if not isinstance(layer, nn.Sequential) and not isinstance(layer, nn.ModuleList) and layer != model:\n",
        "#                 layer.register_forward_hook(hook_wrapper(f\"{name}_{idx}\"))\n",
        "\n",
        "#     def profile_model(self, model: nn.Module, dataloader, node, model_name: str, warmup_iters=3):\n",
        "#         def profiling_task():\n",
        "#             device = torch.device(f\"cuda:{node.gpu}\" if node.gpu is not None and torch.cuda.is_available() else \"cpu\")\n",
        "#             model.to(device)\n",
        "\n",
        "#             if self.mode == 'init':\n",
        "#                 with torch.no_grad():\n",
        "#                     cnt = 0\n",
        "#                     for inputs, targets in dataloader:\n",
        "#                         inputs = inputs.to(device)\n",
        "#                         targets = targets.to(device)\n",
        "#                         model(inputs)\n",
        "#                         cnt += 1\n",
        "#                         if cnt >= warmup_iters:\n",
        "#                             break\n",
        "#                 self._profile_init(model, dataloader, node, model_name, device)\n",
        "#             else:\n",
        "#                 self._profile_runtime(model, dataloader, node, model_name, device)\n",
        "\n",
        "#         rq = node.assign_task(profiling_task)\n",
        "#         rq.get()\n",
        "\n",
        "#     def _profile_init(self, model, dataloader, node, model_name, device):\n",
        "#         self._register_hooks(model)\n",
        "#         with torch.profiler.profile(\n",
        "#             activities=[torch.profiler.ProfilerActivity.CPU, torch.profiler.ProfilerActivity.CUDA],\n",
        "#             profile_memory=True\n",
        "#         ) as prof:\n",
        "#             with torch.no_grad():\n",
        "#                 for step, (inp, tgt) in enumerate(dataloader):\n",
        "#                     if step >= 1:\n",
        "#                         break\n",
        "#                     inp = inp.to(device)\n",
        "#                     tgt = tgt.to(device)\n",
        "#                     model(inp)\n",
        "#                     prof.step()\n",
        "\n",
        "#         stats = self._process_events(prof, model, node, runtime=False)\n",
        "#         self._update_profile_db(stats, model_name, node, runtime=False)\n",
        "\n",
        "#     def _profile_runtime(self, model, dataloader, node, model_name, device):\n",
        "#         self._register_hooks(model)\n",
        "#         with torch.no_grad():\n",
        "#             for step, (inp, tgt) in enumerate(dataloader):\n",
        "#                 if step >= 1:\n",
        "#                     break\n",
        "#                 inp = inp.to(device)\n",
        "#                 tgt = tgt.to(device)\n",
        "#                 with torch.profiler.profile(\n",
        "#                     activities=[torch.profiler.ProfilerActivity.CPU, torch.profiler.ProfilerActivity.CUDA]\n",
        "#                 ) as prof:\n",
        "#                     model(inp)\n",
        "#                     prof.step()\n",
        "#                 stats = self._process_events(prof, model, node, runtime=True)\n",
        "#                 self._append_runtime_csv(stats, model_name, node)\n",
        "\n",
        "#     def _process_events(self, profiler, model, node, runtime=False):\n",
        "#         recognized = set()\n",
        "#         for n, m in model.named_modules():\n",
        "#             if n:\n",
        "#                 recognized.add(n)\n",
        "\n",
        "#         aggregated = {\n",
        "#             'forward_pass': dict(self_cpu_time_total=0, cpu_time_total=0, cuda_time_total=0,\n",
        "#                                  self_cpu_memory_usage=0, self_cuda_memory_usage=0, compute=node.node_id),\n",
        "#             'misc': dict(self_cpu_time_total=0, cpu_time_total=0, cuda_time_total=0,\n",
        "#                          self_cpu_memory_usage=0, self_cuda_memory_usage=0, compute=node.node_id)\n",
        "#         }\n",
        "\n",
        "#         events = list(profiler.events())\n",
        "#         found_root = False\n",
        "\n",
        "#         def strip_suffix(s):\n",
        "#             return re.sub(r'(\\.|_)\\d+$', '', s)\n",
        "\n",
        "#         for e in events:\n",
        "#             if e.name == \"\":\n",
        "#                 found_root = True\n",
        "#                 aggregated['forward_pass']['self_cpu_time_total'] += e.self_cpu_time_total\n",
        "#                 aggregated['forward_pass']['cpu_time_total'] += e.cpu_time_total\n",
        "#                 aggregated['forward_pass']['cuda_time_total'] += e.device_time_total\n",
        "#                 if not runtime:\n",
        "#                     aggregated['forward_pass']['self_cpu_memory_usage'] += e.self_cpu_memory_usage\n",
        "#                     aggregated['forward_pass']['self_cuda_memory_usage'] += e.self_device_memory_usage\n",
        "#             else:\n",
        "#                 base = strip_suffix(e.name)\n",
        "#                 if base in recognized:\n",
        "#                     if base not in aggregated:\n",
        "#                         aggregated[base] = dict(\n",
        "#                             self_cpu_time_total=0, cpu_time_total=0, cuda_time_total=0,\n",
        "#                             self_cpu_memory_usage=0, self_cuda_memory_usage=0,\n",
        "#                             compute=node.node_id\n",
        "#                         )\n",
        "#                     aggregated[base]['self_cpu_time_total'] += e.self_cpu_time_total\n",
        "#                     aggregated[base]['cpu_time_total'] += e.cpu_time_total\n",
        "#                     aggregated[base]['cuda_time_total'] += e.device_time_total\n",
        "#                     if not runtime:\n",
        "#                         aggregated[base]['self_cpu_memory_usage'] += e.self_cpu_memory_usage\n",
        "#                         aggregated[base]['self_cuda_memory_usage'] += e.self_device_memory_usage\n",
        "#                 else:\n",
        "#                     aggregated['misc']['self_cpu_time_total'] += e.self_cpu_time_total\n",
        "#                     aggregated['misc']['cpu_time_total'] += e.cpu_time_total\n",
        "#                     aggregated['misc']['cuda_time_total'] += e.device_time_total\n",
        "#                     if not runtime:\n",
        "#                         aggregated['misc']['self_cpu_memory_usage'] += e.self_cpu_memory_usage\n",
        "#                         aggregated['misc']['self_cuda_memory_usage'] += e.self_device_memory_usage\n",
        "\n",
        "#         if not found_root:\n",
        "#             for k in list(aggregated.keys()):\n",
        "#                 if k not in ('forward_pass', 'misc'):\n",
        "#                     aggregated['forward_pass']['self_cpu_time_total'] += aggregated[k]['self_cpu_time_total']\n",
        "#                     aggregated['forward_pass']['cpu_time_total'] += aggregated[k]['cpu_time_total']\n",
        "#                     aggregated['forward_pass']['cuda_time_total'] += aggregated[k]['cuda_time_total']\n",
        "#                     if not runtime:\n",
        "#                         aggregated['forward_pass']['self_cpu_memory_usage'] += aggregated[k]['self_cpu_memory_usage']\n",
        "#                         aggregated['forward_pass']['self_cuda_memory_usage'] += aggregated[k]['self_cuda_memory_usage']\n",
        "\n",
        "#             aggregated['forward_pass']['self_cpu_time_total'] += aggregated['misc']['self_cpu_time_total']\n",
        "#             aggregated['forward_pass']['cpu_time_total'] += aggregated['misc']['cpu_time_total']\n",
        "#             aggregated['forward_pass']['cuda_time_total'] += aggregated['misc']['cuda_time_total']\n",
        "#             if not runtime:\n",
        "#                 aggregated['forward_pass']['self_cpu_memory_usage'] += aggregated['misc']['self_cpu_memory_usage']\n",
        "#                 aggregated['forward_pass']['self_cuda_memory_usage'] += aggregated['misc']['self_cuda_memory_usage']\n",
        "\n",
        "#         return aggregated\n",
        "\n",
        "#     def _update_profile_db(self, stats, model_name, node, runtime=False):\n",
        "#         if runtime:\n",
        "#             return\n",
        "#         for layer_name, data in stats.items():\n",
        "#             total_t = data['cpu_time_total'] + data['cuda_time_total']\n",
        "#             total_m = data['self_cpu_memory_usage'] + data['self_cuda_memory_usage']\n",
        "#             row = {\n",
        "#                 'Model': model_name,\n",
        "#                 'Layer': layer_name,\n",
        "#                 'Compute': data['compute'],\n",
        "#                 'Self CPU (us)': data['self_cpu_time_total'],\n",
        "#                 'CPU Total (us)': data['cpu_time_total'],\n",
        "#                 'CUDA Total (us)': data['cuda_time_total'],\n",
        "#                 'Self CPU Mem (bytes)': data['self_cpu_memory_usage'],\n",
        "#                 'Self CUDA Mem (bytes)': data['self_cuda_memory_usage'],\n",
        "#                 'Total Execution Time (us)': total_t,\n",
        "#                 'Total Memory Used (bytes)': total_m\n",
        "#             }\n",
        "#             self.profile_db = self._upsert(self.profile_db, row)\n",
        "#         self.profile_db.to_csv(self.profile_db_path, index=False)\n",
        "\n",
        "#     def _upsert(self, df, row):\n",
        "#         mask = (\n",
        "#             (df['Model'] == row['Model']) &\n",
        "#             (df['Layer'] == row['Layer']) &\n",
        "#             (df['Compute'] == row['Compute'])\n",
        "#         )\n",
        "#         if not df[mask].empty:\n",
        "#             existing_time = df.loc[mask, 'Total Execution Time (us)'].max()\n",
        "#             if row['Total Execution Time (us)'] > existing_time:\n",
        "#                 for k, v in row.items():\n",
        "#                     df.loc[mask, k] = v\n",
        "#         else:\n",
        "#             df = pd.concat([df, pd.DataFrame([row])], ignore_index=True)\n",
        "#         return df\n",
        "\n",
        "#     def _append_runtime_csv(self, stats, model_name, node):\n",
        "#         rows = []\n",
        "#         for layer_name, data in stats.items():\n",
        "#             exec_time = data['cpu_time_total'] + data['cuda_time_total']\n",
        "#             rows.append({\n",
        "#                 'Model': model_name,\n",
        "#                 'Layer': layer_name,\n",
        "#                 'Compute': data['compute'],\n",
        "#                 'Execution Time (us)': exec_time\n",
        "#             })\n",
        "#         if rows:\n",
        "#             rdf = pd.read_csv(self.runtime_csv)\n",
        "#             rdf = pd.concat([rdf, pd.DataFrame(rows)], ignore_index=True)\n",
        "#             rdf.to_csv(self.runtime_csv, index=False)\n",
        "\n",
        "#     def get_profile_db(self):\n",
        "#         return self.profile_db\n",
        "\n",
        "#     def print_profile_db(self):\n",
        "#         if self.profile_db.empty:\n",
        "#             print(\"ProfileDB is empty.\")\n",
        "#         else:\n",
        "#             print(\"ProfileDB:\\n\", self.profile_db.to_string(index=False))\n",
        "\n",
        "\n",
        "# ############################################\n",
        "# # 3) Stage Class (Unchanged)\n",
        "# ############################################\n",
        "\n",
        "# class Stage:\n",
        "#     def __init__(self, stage_id: str, layers: nn.ModuleList, assigned_node: 'Node'):\n",
        "#         self.stage_id = stage_id\n",
        "#         self.layers = layers\n",
        "#         self.assigned_node = assigned_node\n",
        "\n",
        "#         self.input_data = None\n",
        "#         self.output_data = None\n",
        "#         self.execution_time = 0.0\n",
        "\n",
        "#         self.dependencies = set()\n",
        "#         self.dependents = set()\n",
        "\n",
        "#     def run_stage(self):\n",
        "#         import time\n",
        "#         start = time.time()\n",
        "#         device = torch.device(\n",
        "#             f\"cuda:{self.assigned_node.gpu}\" if self.assigned_node.gpu is not None and torch.cuda.is_available()\n",
        "#             else \"cpu\"\n",
        "#         )\n",
        "#         inp = self.input_data.to(device)\n",
        "\n",
        "#         with torch.no_grad():\n",
        "#             out = inp\n",
        "#             for layer in self.layers:\n",
        "#                 out = layer(out)\n",
        "\n",
        "#         if device.type == 'cuda':\n",
        "#             out = out.cpu()\n",
        "\n",
        "#         self.output_data = out\n",
        "#         end = time.time()\n",
        "#         self.execution_time = end - start\n",
        "#         return self.output_data\n",
        "\n",
        "\n",
        "# ############################################\n",
        "# # 4) Scheduler Class (Updated to Use Observation Window)\n",
        "# ############################################\n",
        "\n",
        "# class Scheduler:\n",
        "#     \"\"\"\n",
        "#     A DP-based scheduler that:\n",
        "#       - Uses an observation_window to interpret times as a fraction => utilization\n",
        "#       - w[k] is the existing utilization on node k (0 <= w[k] <= 1, if we interpret it strictly).\n",
        "#       - For each new Task, runs a DP up to k_stages partitions, picks best s,\n",
        "#         and updates w[k] accordingly.\n",
        "#     \"\"\"\n",
        "\n",
        "#     def __init__(self, profiler: 'Profiler', nodes: List['Node'], observation_window: float, slack_percent: float = 0.1):\n",
        "#         \"\"\"\n",
        "#         :param profiler: A Profiler with layer->node times in a ProfileDB.\n",
        "#         :param nodes: A list of Node objects\n",
        "#         :param observation_window: The base time window for converting raw times -> utilization fraction\n",
        "#         :param slack_percent: an optional multiplier to add slack: e.g. final window = observation_window*(1+slack_percent)\n",
        "#         \"\"\"\n",
        "#         self.profiler = profiler\n",
        "#         self.nodes = nodes\n",
        "\n",
        "#         # incorporate slack into the final used window\n",
        "#         self.observation_window = observation_window * (1.0 + slack_percent)\n",
        "\n",
        "#         # node_usage w[k] store fraction of the observation window\n",
        "#         self.node_usage = [0.0] * len(nodes)\n",
        "\n",
        "#         # store per-task stage graph\n",
        "#         self.task_stage_graphs: Dict[str, Dict[str, dict]] = {}\n",
        "\n",
        "#     def _get_block_fraction(self, block_time: float) -> float:\n",
        "#         \"\"\"\n",
        "#         Convert raw block_time in seconds -> fraction of the observation window.\n",
        "#         \"\"\"\n",
        "#         frac = block_time / self.observation_window\n",
        "#         return frac\n",
        "\n",
        "#     def _get_block_time(self, prefix_times, x, i, node_j) -> float:\n",
        "#         return prefix_times[node_j][i] - prefix_times[node_j][x]\n",
        "\n",
        "#     def decompose_and_allocate_task(self, task: 'Task', k_stages_max: int = None):\n",
        "#         \"\"\"\n",
        "#         DP with w[k] usage and observation_window.\n",
        "#         We sum up block_time/observation_window to w[k].\n",
        "#         \"\"\"\n",
        "#         model = task.model\n",
        "#         model_name = task.model_name\n",
        "#         task_id = task.task_id\n",
        "\n",
        "#         # Gather leaf layers\n",
        "#         leaf_layers = []\n",
        "#         for name, module in model.named_modules():\n",
        "#             if len(list(module.children())) == 0 and name:\n",
        "#                 leaf_layers.append(name)\n",
        "#         n_layers = len(leaf_layers)\n",
        "#         if n_layers == 0:\n",
        "#             print(f\"[Scheduler] No leaf layers found for model={model_name}\")\n",
        "#             self.task_stage_graphs[task_id] = {}\n",
        "#             return\n",
        "\n",
        "#         if k_stages_max is None:\n",
        "#             k_stages_max = min(len(self.nodes), n_layers)\n",
        "\n",
        "#         profile_db = self.profiler.get_profile_db()\n",
        "#         model_df = profile_db[profile_db['Model'] == model_name]\n",
        "\n",
        "#         times = [[0.0]*len(self.nodes) for _ in range(n_layers)]\n",
        "#         for i2, lname in enumerate(leaf_layers):\n",
        "#             for j, node in enumerate(self.nodes):\n",
        "#                 row = model_df[(model_df['Layer'] == lname) & (model_df['Compute'] == node.node_id)]\n",
        "#                 if not row.empty:\n",
        "#                     t_us = row['Total Execution Time (us)'].values[0]\n",
        "#                     times[i2][j] = t_us / 1e6  # raw seconds\n",
        "#                 else:\n",
        "#                     times[i2][j] = 1.0\n",
        "\n",
        "#         # prefix sums\n",
        "#         prefix_times = [[0.0]*(n_layers+1) for _ in range(len(self.nodes))]\n",
        "#         for j in range(len(self.nodes)):\n",
        "#             for i2 in range(1, n_layers+1):\n",
        "#                 prefix_times[j][i2] = prefix_times[j][i2-1] + times[i2-1][j]\n",
        "\n",
        "#         INF = float('inf')\n",
        "#         DP = [[INF]*(k_stages_max+1) for _ in range(n_layers+1)]\n",
        "#         back_node = [[-1]*(k_stages_max+1) for _ in range(n_layers+1)]\n",
        "#         back_split = [[-1]*(k_stages_max+1) for _ in range(n_layers+1)]\n",
        "\n",
        "#         DP[0][0] = 0.0\n",
        "\n",
        "#         # fill DP\n",
        "#         for i2 in range(1, n_layers+1):\n",
        "#             for s in range(1, k_stages_max+1):\n",
        "#                 for j in range(len(self.nodes)):\n",
        "#                     wj = self.node_usage[j]  # existing usage fraction\n",
        "#                     for x in range(i2):\n",
        "#                         block_t = self._get_block_time(prefix_times, x, i2, j) # raw time\n",
        "#                         block_frac = self._get_block_fraction(block_t)         # fraction\n",
        "#                         cand = max(DP[x][s-1], wj + block_frac)\n",
        "#                         if cand < DP[i2][s]:\n",
        "#                             DP[i2][s] = cand\n",
        "#                             back_node[i2][s] = j\n",
        "#                             back_split[i2][s] = x\n",
        "\n",
        "#         best_val = INF\n",
        "#         best_s = None\n",
        "#         for s in range(1, k_stages_max+1):\n",
        "#             if DP[n_layers][s] < best_val:\n",
        "#                 best_val = DP[n_layers][s]\n",
        "#                 best_s = s\n",
        "#         print(f\"[Scheduler] Task={task_id}, Model={model_name}, best s in [1..{k_stages_max}] => {best_s}, final utilization={best_val:.4f}\")\n",
        "\n",
        "#         i2 = n_layers\n",
        "#         s = best_s\n",
        "#         partitions = []\n",
        "#         while i2 > 0 and s > 0:\n",
        "#             j = back_node[i2][s]\n",
        "#             x = back_split[i2][s]\n",
        "#             partitions.append((x, i2, j))\n",
        "#             i2 = x\n",
        "#             s -= 1\n",
        "#         partitions.reverse()\n",
        "\n",
        "#         stage_graph = {}\n",
        "#         idx = 0\n",
        "#         for (start_i, end_i, node_j) in partitions:\n",
        "#             stage_id = f\"{task_id}-stage-{idx}\"\n",
        "#             layer_subset = leaf_layers[start_i:end_i]\n",
        "#             stage_graph[stage_id] = {\n",
        "#                 'node_idx': node_j,\n",
        "#                 'layer_names': layer_subset,\n",
        "#                 'dependencies': [],\n",
        "#                 'dependents': []\n",
        "#             }\n",
        "#             idx += 1\n",
        "\n",
        "#         keys = list(stage_graph.keys())\n",
        "#         for k2 in range(1, len(keys)):\n",
        "#             prev_stg = keys[k2-1]\n",
        "#             curr_stg = keys[k2]\n",
        "#             stage_graph[curr_stg]['dependencies'].append(prev_stg)\n",
        "#             stage_graph[prev_stg]['dependents'].append(curr_stg)\n",
        "\n",
        "#         self.task_stage_graphs[task_id] = stage_graph\n",
        "\n",
        "#         # update w[k]\n",
        "#         for (start_i, end_i, node_j) in partitions:\n",
        "#             raw_block_time = self._get_block_time(prefix_times, start_i, end_i, node_j)\n",
        "#             block_frac = self._get_block_fraction(raw_block_time)\n",
        "#             self.node_usage[node_j] += block_frac\n",
        "\n",
        "#         self.print_task_stages(task_id)\n",
        "\n",
        "#     def print_task_stages(self, task_id: str):\n",
        "#         if task_id not in self.task_stage_graphs:\n",
        "#             print(f\"[Scheduler] No stage graph for task={task_id}\")\n",
        "#             return\n",
        "#         stg_graph = self.task_stage_graphs[task_id]\n",
        "#         print(f\"[Scheduler] Stage Decomposition for Task={task_id}:\")\n",
        "#         for sid, info in stg_graph.items():\n",
        "#             node_j = info['node_idx']\n",
        "#             node_id = self.nodes[node_j].node_id\n",
        "#             layers = info['layer_names']\n",
        "#             deps = info['dependencies']\n",
        "#             print(f\"  Stage={sid}, Node={node_id}, Layers={layers}, DependsOn={list(deps)}\")\n",
        "\n",
        "#     def create_stages_for_task(self, task: 'Task'):\n",
        "#         import torch.nn as nn\n",
        "#         t_id = task.task_id\n",
        "#         if t_id not in self.task_stage_graphs:\n",
        "#             print(f\"[Scheduler] No stage graph found for Task={t_id}.\")\n",
        "#             return\n",
        "#         stg_graph = self.task_stage_graphs[t_id]\n",
        "#         for stage_id, info in stg_graph.items():\n",
        "#             node_idx = info['node_idx']\n",
        "#             node = self.nodes[node_idx]\n",
        "\n",
        "#             layer_list = []\n",
        "#             for lname in info['layer_names']:\n",
        "#                 layer_list.append(task.model.get_submodule(lname))\n",
        "\n",
        "#             stg = Stage(stage_id, nn.ModuleList(layer_list), node)\n",
        "#             stg.dependencies = set(info['dependencies'])\n",
        "#             stg.dependents = set(info['dependents'])\n",
        "#             task.stages[stage_id] = stg\n",
        "\n",
        "#     def run_pipeline_parallel(self, task: 'Task'):\n",
        "#         lock = threading.Lock()\n",
        "#         dep_count = {}\n",
        "#         for sid, stg in task.stages.items():\n",
        "#             dep_count[sid] = len(stg.dependencies)\n",
        "\n",
        "#         total_stages = len(task.stages)\n",
        "#         completed = 0\n",
        "#         done_event = threading.Event()\n",
        "\n",
        "#         def stage_done_callback(sid: str):\n",
        "#             nonlocal completed\n",
        "#             with lock:\n",
        "#                 for dep_id in task.stages[sid].dependents:\n",
        "#                     dep_count[dep_id] -= 1\n",
        "#                     if dep_count[dep_id] == 0:\n",
        "#                         enqueue_stage(dep_id)\n",
        "#                 completed += 1\n",
        "#                 if completed >= total_stages:\n",
        "#                     done_event.set()\n",
        "\n",
        "#         def enqueue_stage(sid: str):\n",
        "#             stg = task.stages[sid]\n",
        "#             def run_fn():\n",
        "#                 return stg.run_stage()\n",
        "#             def worker():\n",
        "#                 rq = stg.assigned_node.assign_task(run_fn)\n",
        "#                 _ = rq.get()\n",
        "#                 stage_done_callback(sid)\n",
        "#             threading.Thread(target=worker, daemon=True).start()\n",
        "\n",
        "#         for sid, c in dep_count.items():\n",
        "#             if c == 0:\n",
        "#                 enqueue_stage(sid)\n",
        "\n",
        "#         done_event.wait()\n",
        "\n",
        "\n",
        "# ############################################\n",
        "# # 5) Task Class (Unchanged)\n",
        "# ############################################\n",
        "\n",
        "# from typing import Dict, Optional\n",
        "\n",
        "# class Task:\n",
        "#     def __init__(self, task_id: str, model: nn.Module, input_data: torch.Tensor, model_name: str):\n",
        "#         self.task_id = task_id\n",
        "#         self.model = model\n",
        "#         self.input_data = input_data\n",
        "#         self.model_name = model_name\n",
        "\n",
        "#         self.stages: Dict[str, Stage] = {}\n",
        "\n",
        "#         self.start_time: Optional[float] = None\n",
        "#         self.finish_time: Optional[float] = None\n",
        "\n",
        "#     def get_total_execution_time(self):\n",
        "#         if self.start_time and self.finish_time:\n",
        "#             return self.finish_time - self.start_time\n",
        "#         return 0.0\n",
        "\n",
        "\n",
        "# ############################################\n",
        "# # 6) Taskset Class (Unchanged)\n",
        "# ############################################\n",
        "\n",
        "# import time\n",
        "\n",
        "# class Taskset:\n",
        "#     def __init__(self, tasks: List[Task], scheduler: Scheduler):\n",
        "#         self.tasks = tasks\n",
        "#         self.scheduler = scheduler\n",
        "\n",
        "#     def schedule_all_tasks(self):\n",
        "#         for t in self.tasks:\n",
        "#             self.scheduler.create_stages_for_task(t)\n",
        "\n",
        "#     def execute_all(self):\n",
        "#         import threading\n",
        "#         start_global = time.time()\n",
        "#         threads = []\n",
        "#         def run_task(task: Task):\n",
        "#             task.start_time = time.time()\n",
        "#             self.scheduler.run_pipeline_parallel(task)\n",
        "#             task.finish_time = time.time()\n",
        "\n",
        "#         for t in self.tasks:\n",
        "#             th = threading.Thread(target=run_task, args=(t,), daemon=True)\n",
        "#             th.start()\n",
        "#             threads.append(th)\n",
        "\n",
        "#         for th in threads:\n",
        "#             th.join()\n",
        "\n",
        "#         end_global = time.time()\n",
        "#         print(f\"[Taskset] All tasks completed in {end_global - start_global:.2f}s\")\n",
        "\n",
        "#     def collect_metrics(self):\n",
        "#         return {}\n",
        "\n",
        "\n",
        "# ############################################\n",
        "# # 7) Evaluator Class (Unchanged)\n",
        "# ############################################\n",
        "\n",
        "# class Evaluator:\n",
        "#     def __init__(self, device='cpu', print_precision=2, rtol=1e-03, atol=1e-05):\n",
        "#         self.device = device\n",
        "#         self.print_precision = print_precision\n",
        "#         self.rtol = rtol\n",
        "#         self.atol = atol\n",
        "\n",
        "#         self.naive_run_time = None\n",
        "#         self.pipeline_run_time = None\n",
        "\n",
        "#         self.naive_outputs = {}\n",
        "#         self.pipeline_outputs = {}\n",
        "\n",
        "#     def run_naive_pytorch(self, tasks: List['Task']):\n",
        "#         import time\n",
        "#         dev = torch.device(self.device)\n",
        "#         start = time.time()\n",
        "\n",
        "#         with torch.no_grad():\n",
        "#             for task in tasks:\n",
        "#                 task.model.to(dev)\n",
        "#                 out = task.model(task.input_data.to(dev))\n",
        "#                 out_cpu = out.cpu() if dev.type == 'cuda' else out\n",
        "#                 self.naive_outputs[task.task_id] = out_cpu\n",
        "\n",
        "#         end = time.time()\n",
        "#         self.naive_run_time = end - start\n",
        "#         print(f\"[Evaluator] Naive run on device='{self.device}' for {len(tasks)} tasks => \"\n",
        "#               f\"{self.naive_run_time:.{self.print_precision}f}s\")\n",
        "\n",
        "#     def run_pipeline(self, taskset: 'Taskset'):\n",
        "#         import time\n",
        "#         start = time.time()\n",
        "#         taskset.schedule_all_tasks()\n",
        "#         taskset.execute_all()\n",
        "#         end = time.time()\n",
        "#         self.pipeline_run_time = end - start\n",
        "#         print(f\"[Evaluator] Pipeline approach => {self.pipeline_run_time:.{self.print_precision}f}s\")\n",
        "\n",
        "#         for task in taskset.tasks:\n",
        "#             final_out = self._get_task_final_output(task)\n",
        "#             self.pipeline_outputs[task.task_id] = final_out\n",
        "\n",
        "#     def _get_task_final_output(self, task: 'Task'):\n",
        "#         if not task.stages:\n",
        "#             return None\n",
        "#         final_stage = None\n",
        "#         for sid, stg in task.stages.items():\n",
        "#             if not stg.dependents:\n",
        "#                 final_stage = stg\n",
        "#                 break\n",
        "#         if final_stage is None:\n",
        "#             sorted_ids = sorted(task.stages.keys())\n",
        "#             final_stage = task.stages[sorted_ids[-1]]\n",
        "#         return final_stage.output_data\n",
        "\n",
        "#     def compare_results(self, node_usage_info: dict = None):\n",
        "#         print(\"\\n=== Comparison ===\")\n",
        "#         if self.naive_run_time is None or self.pipeline_run_time is None:\n",
        "#             print(\"Cannot compare times; missing run times.\")\n",
        "#         else:\n",
        "#             print(f\"Naive total   : {self.naive_run_time:.{self.print_precision}f}s\")\n",
        "#             print(f\"Pipeline total: {self.pipeline_run_time:.{self.print_precision}f}s\")\n",
        "#             if self.pipeline_run_time == 0.0:\n",
        "#                 speedup = float('inf')\n",
        "#             else:\n",
        "#                 speedup = self.naive_run_time / self.pipeline_run_time\n",
        "#             print(f\"Speedup = {speedup:.{self.print_precision}f}x\")\n",
        "\n",
        "#         print(\"\\n--- Output Similarity Checks ---\")\n",
        "#         for task_id, naive_out in self.naive_outputs.items():\n",
        "#             pipe_out = self.pipeline_outputs.get(task_id, None)\n",
        "#             if pipe_out is None:\n",
        "#                 print(f\"Task={task_id}: No pipeline output found.\")\n",
        "#                 continue\n",
        "#             if naive_out.shape != pipe_out.shape:\n",
        "#                 print(f\"Task={task_id}: Output shape mismatch.\")\n",
        "#                 continue\n",
        "#             close = torch.allclose(naive_out, pipe_out, rtol=self.rtol, atol=self.atol)\n",
        "#             if close:\n",
        "#                 print(f\"Task={task_id}: Outputs match within tolerance.\")\n",
        "#             else:\n",
        "#                 print(f\"Task={task_id}: Outputs differ beyond tolerance.\")\n",
        "\n",
        "#         if node_usage_info:\n",
        "#             print(\"\\nNode Usage Info:\")\n",
        "#             for k, v in node_usage_info.items():\n",
        "#                 print(f\"  Node={k}, usage={v}\")\n",
        "\n",
        "#         print(\"\\n=== End Comparison ===\")\n",
        "\n",
        "\n",
        "# ############################################\n",
        "# # 8) main_demo test script (Updated)\n",
        "# ############################################\n",
        "\n",
        "# import torch\n",
        "# import torch.nn as nn\n",
        "# import torch.utils.data as data\n",
        "# import gc\n",
        "# import torchvision.models as models\n",
        "\n",
        "# def main_demo():\n",
        "#     print(\"=== main_demo with Observation Window Logic ===\")\n",
        "\n",
        "#     # 1) Setup init phase\n",
        "#     def init_phase():\n",
        "#         print(\"=== INIT PHASE ===\")\n",
        "#         profiler = Profiler(mode='init', profile_db_path='dp_partition_profiling.csv')\n",
        "#         nodes = Node.discover_nodes()\n",
        "#         if not nodes:\n",
        "#             print(\"No nodes discovered. Exiting.\")\n",
        "#             return None, None\n",
        "\n",
        "#         # For demonstration, we profile 2 models\n",
        "#         print(\"\\n-- Profiling models --\")\n",
        "#         # a) Dummy dataset for a simple FFN\n",
        "#         class FFNDataset(data.Dataset):\n",
        "#             def __init__(self, size=5):\n",
        "#                 self.size = size\n",
        "#             def __len__(self):\n",
        "#                 return self.size\n",
        "#             def __getitem__(self, idx):\n",
        "#                 return torch.randn(1,256), torch.tensor(0)\n",
        "\n",
        "#         # b) Dummy dataset for ResNet\n",
        "#         class ResNetDataset(data.Dataset):\n",
        "#             def __init__(self, size=5):\n",
        "#                 self.size = size\n",
        "#             def __len__(self):\n",
        "#                 return self.size\n",
        "#             def __getitem__(self, idx):\n",
        "#                 return torch.randn(3,224,224), torch.tensor(0)\n",
        "\n",
        "#         ffn_loader = data.DataLoader(FFNDataset(), batch_size=1)\n",
        "#         res_loader = data.DataLoader(ResNetDataset(), batch_size=1)\n",
        "\n",
        "#         # Profile both\n",
        "#         ffn_model = nn.Sequential(nn.Linear(256, 512), nn.ReLU(), nn.Linear(512,128))\n",
        "#         # or the bigger ffn if you prefer\n",
        "\n",
        "#         resnet_model = models.resnet18(pretrained=False)\n",
        "\n",
        "#         ffn_model_name = \"InitPhaseFFN\"\n",
        "#         res_model_name = \"InitPhaseResNet\"\n",
        "\n",
        "#         for node in nodes:\n",
        "#             profiler.profile_model(ffn_model, ffn_loader, node, ffn_model_name)\n",
        "#             profiler.profile_model(resnet_model, res_loader, node, res_model_name)\n",
        "\n",
        "#         return profiler, nodes\n",
        "\n",
        "\n",
        "#     # 2) Runtime phase\n",
        "#     def runtime_phase(profiler, nodes):\n",
        "#         print(\"\\n=== RUNTIME PHASE ===\")\n",
        "\n",
        "#         # Choose an observation window (in seconds). Suppose we pick 2.0 seconds\n",
        "#         # Then a slack of 20% => final window=2.4\n",
        "#         # The HPC or real-time system might pick this logic differently\n",
        "#         observation_window = 2.0\n",
        "#         slack_percent = 0.2\n",
        "\n",
        "#         # Create our DP-based Scheduler with the observation window\n",
        "#         scheduler = Scheduler(profiler, nodes, observation_window=observation_window, slack_percent=slack_percent)\n",
        "\n",
        "#         # Let's define tasks with actual models\n",
        "#         # a) Our bigger FFN model\n",
        "#         class BiggerFFN(nn.Module):\n",
        "#             def __init__(self):\n",
        "#                 super().__init__()\n",
        "#                 self.net = nn.Sequential(\n",
        "#                     nn.Linear(256, 512),\n",
        "#                     nn.ReLU(),\n",
        "#                     nn.Linear(512, 256),\n",
        "#                     nn.ReLU(),\n",
        "#                     nn.Linear(256, 128),\n",
        "#                     nn.ReLU(),\n",
        "#                     nn.Linear(128, 10)\n",
        "#                 )\n",
        "#             def forward(self, x):\n",
        "#                 return self.net(x)\n",
        "\n",
        "#         bigger_ffn_model_name = \"BiggerFFN_ObsWin\"\n",
        "\n",
        "#         # b) Our ResNet\n",
        "#         resnet_model_name = \"ResNet18_ObsWin\"\n",
        "#         resnet_model = models.resnet18(pretrained=False)\n",
        "\n",
        "#         # We won't re-profile them now, we assume the ProfileDB has times for these layers or fallback=1.0\n",
        "#         # Let's create tasks:\n",
        "#         input_ffn = torch.randn(1, 256)\n",
        "#         task_ffn1 = Task(\"TaskFFN1\", BiggerFFN(), input_ffn.clone(), bigger_ffn_model_name)\n",
        "#         task_ffn2 = Task(\"TaskFFN2\", BiggerFFN(), input_ffn.clone(), bigger_ffn_model_name)\n",
        "\n",
        "#         input_res = torch.randn(1, 3, 224, 224)\n",
        "#         task_res1 = Task(\"TaskRes1\", resnet_model, input_res.clone(), resnet_model_name)\n",
        "#         task_res2 = Task(\"TaskRes2\", resnet_model, input_res.clone(), resnet_model_name)\n",
        "#         taks_res3 = Task(\"TaskRes3\", resnet_model, input_res.clone(), resnet_model_name)\n",
        "\n",
        "#         all_tasks = [task_ffn1, task_ffn2, task_res1, task_res2,task_res3]\n",
        "\n",
        "#         # Decompose & allocate each\n",
        "#         for t in all_tasks:\n",
        "#             scheduler.decompose_and_allocate_task(t, k_stages_max=None)\n",
        "\n",
        "#         # Build Taskset\n",
        "#         taskset = Taskset(all_tasks, scheduler)\n",
        "#         evaluator = Evaluator(device='cuda', print_precision=3)\n",
        "\n",
        "#         # 1) naive\n",
        "#         evaluator.run_naive_pytorch(all_tasks)\n",
        "#         if torch.cuda.is_available():\n",
        "#             torch.cuda.empty_cache()\n",
        "#         gc.collect()\n",
        "\n",
        "#         # 2) pipeline\n",
        "#         print(\"\\n-- Pipeline run with observation window approach --\")\n",
        "#         evaluator.run_pipeline(taskset)\n",
        "\n",
        "#         usage_info = {}\n",
        "#         # usage_info = {scheduler.nodes[i].node_id: scheduler.node_usage[i] for i in range(len(scheduler.nodes))}\n",
        "\n",
        "#         evaluator.compare_results(usage_info)\n",
        "\n",
        "#         # stop nodes\n",
        "#         for nd in nodes:\n",
        "#             nd.stop()\n",
        "\n",
        "#         # print final profile\n",
        "#         print(\"\\nFinal ProfileDB:\")\n",
        "#         profiler.print_profile_db()\n",
        "\n",
        "\n",
        "#     # actually run\n",
        "#     profiler, nodes = init_phase()\n",
        "#     if profiler is not None and nodes is not None:\n",
        "#         runtime_phase(profiler, nodes)\n",
        "#     else:\n",
        "#         print(\"Init phase failed; skipping runtime.\")\n"
      ],
      "metadata": {
        "id": "-SaimDortzzi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# main_demo()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 515
        },
        "id": "WSPTkrJrw_9l",
        "outputId": "28282892-0ab6-4357-e3bb-000cc1850a22"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== main_demo with Observation Window Logic ===\n",
            "=== INIT PHASE ===\n",
            "\n",
            "-- Profiling models --\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
            "  warnings.warn(msg)\n",
            "<ipython-input-1-3e3b8743823b>:301: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
            "  df = pd.concat([df, pd.DataFrame([row])], ignore_index=True)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== RUNTIME PHASE ===\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'task_res3' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-65ac186c420a>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmain_demo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-1-3e3b8743823b>\u001b[0m in \u001b[0;36mmain_demo\u001b[0;34m()\u001b[0m\n\u001b[1;32m    896\u001b[0m     \u001b[0mprofiler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnodes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minit_phase\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mprofiler\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mnodes\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 898\u001b[0;31m         \u001b[0mruntime_phase\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprofiler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnodes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    899\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    900\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Init phase failed; skipping runtime.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-1-3e3b8743823b>\u001b[0m in \u001b[0;36mruntime_phase\u001b[0;34m(profiler, nodes)\u001b[0m\n\u001b[1;32m    859\u001b[0m         \u001b[0mtaks_res3\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTask\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"TaskRes3\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresnet_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_res\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresnet_model_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    860\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 861\u001b[0;31m         \u001b[0mall_tasks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtask_ffn1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtask_ffn2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtask_res1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtask_res2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtask_res3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    862\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    863\u001b[0m         \u001b[0;31m# Decompose & allocate each\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'task_res3' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"Poc_final_dart.ipynb\n",
        "\n",
        "Automatically generated by Colab.\n",
        "\"\"\"\n",
        "\n",
        "import os\n",
        "import torch\n",
        "import threading\n",
        "import queue\n",
        "from typing import Callable, Any, List\n",
        "\n",
        "############################################\n",
        "# 1) Node Class (Unchanged)\n",
        "############################################\n",
        "\n",
        "class Node:\n",
        "    def __init__(self, node_id: str, cpus=None, gpu=None):\n",
        "        self._node_id = node_id\n",
        "        self._cpus = tuple(cpus or [])\n",
        "        self._gpu = gpu\n",
        "\n",
        "        self._original_affinity = os.sched_getaffinity(0)\n",
        "        self._task_queue = queue.Queue()\n",
        "        self._stop_signal = False\n",
        "\n",
        "        self._worker_thread = threading.Thread(target=self._worker_loop, daemon=True)\n",
        "        self._worker_thread.start()\n",
        "\n",
        "    @property\n",
        "    def node_id(self):\n",
        "        return self._node_id\n",
        "\n",
        "    @property\n",
        "    def cpus(self):\n",
        "        return self._cpus\n",
        "\n",
        "    @property\n",
        "    def gpu(self):\n",
        "        return self._gpu\n",
        "\n",
        "    def assign_task(self, func: Callable, *args, **kwargs) -> queue.Queue:\n",
        "        result_queue = queue.Queue(maxsize=1)\n",
        "        self._task_queue.put((func, args, kwargs, result_queue))\n",
        "        return result_queue\n",
        "\n",
        "    def stop(self):\n",
        "        self._stop_signal = True\n",
        "        self._task_queue.put(None)\n",
        "        self._worker_thread.join()\n",
        "\n",
        "    def _worker_loop(self):\n",
        "        while not self._stop_signal:\n",
        "            item = self._task_queue.get()\n",
        "            if item is None:\n",
        "                break\n",
        "            func, args, kwargs, result_queue = item\n",
        "            try:\n",
        "                self._set_context()\n",
        "                result = func(*args, **kwargs)\n",
        "            except Exception as e:\n",
        "                result = e\n",
        "            finally:\n",
        "                self._reset_context()\n",
        "\n",
        "            result_queue.put(result)\n",
        "\n",
        "    def _set_context(self):\n",
        "        if self._cpus:\n",
        "            os.sched_setaffinity(0, self._cpus)\n",
        "        if self._gpu is not None and torch.cuda.is_available():\n",
        "            torch.cuda.set_device(self._gpu)\n",
        "\n",
        "    def _reset_context(self):\n",
        "        os.sched_setaffinity(0, self._original_affinity)\n",
        "\n",
        "    @staticmethod\n",
        "    def discover_nodes() -> List['Node']:\n",
        "        nodes = []\n",
        "        num_cpus = os.cpu_count() or 1\n",
        "        ngpus = torch.cuda.device_count()\n",
        "        for core_id in range(num_cpus):\n",
        "            node = Node(node_id=f\"CPU-{core_id}\", cpus=[core_id])\n",
        "            nodes.append(node)\n",
        "        for g in range(ngpus):\n",
        "            for core_id in range(num_cpus):\n",
        "                node = Node(node_id=f\"GPU-{g}-CPU-{core_id}\", cpus=[core_id], gpu=g)\n",
        "                nodes.append(node)\n",
        "        return nodes\n",
        "\n",
        "    def __repr__(self):\n",
        "        return f\"Node({self._node_id}, cpus={self._cpus}, gpu={self._gpu})\"\n",
        "\n",
        "\n",
        "############################################\n",
        "# 2) Profiler Class (Unchanged)\n",
        "############################################\n",
        "\n",
        "import time\n",
        "import os\n",
        "import re\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.profiler\n",
        "\n",
        "class Profiler:\n",
        "    def __init__(self, mode: str, profile_db_path='profiling_results.csv', log_dir='logs'):\n",
        "        assert mode in ['init', 'runtime']\n",
        "        self.mode = mode\n",
        "        self.profile_db_path = profile_db_path\n",
        "        self.log_dir = log_dir\n",
        "        os.makedirs(self.log_dir, exist_ok=True)\n",
        "\n",
        "        columns = [\n",
        "            'Model', 'Layer', 'Compute',\n",
        "            'Self CPU (us)', 'CPU Total (us)', 'CUDA Total (us)',\n",
        "            'Self CPU Mem (bytes)', 'Self CUDA Mem (bytes)',\n",
        "            'Total Execution Time (us)', 'Total Memory Used (bytes)'\n",
        "        ]\n",
        "        if os.path.exists(self.profile_db_path):\n",
        "            self.profile_db = pd.read_csv(self.profile_db_path)\n",
        "        else:\n",
        "            self.profile_db = pd.DataFrame(columns=columns)\n",
        "\n",
        "        self.runtime_csv = os.path.join(self.log_dir, 'runtime_results.csv')\n",
        "        if not os.path.exists(self.runtime_csv):\n",
        "            rt_cols = ['Model', 'Layer', 'Compute', 'Execution Time (us)']\n",
        "            pd.DataFrame(columns=rt_cols).to_csv(self.runtime_csv, index=False)\n",
        "\n",
        "    def _register_hooks(self, model: nn.Module):\n",
        "        def hook_wrapper(layer_name):\n",
        "            def hook(mod, inp, out):\n",
        "                with torch.profiler.record_function(layer_name):\n",
        "                    pass\n",
        "            return hook\n",
        "\n",
        "        for idx, (name, layer) in enumerate(model.named_modules()):\n",
        "            if not isinstance(layer, nn.Sequential) and not isinstance(layer, nn.ModuleList) and layer != model:\n",
        "                layer.register_forward_hook(hook_wrapper(f\"{name}_{idx}\"))\n",
        "\n",
        "    def profile_model(self, model: nn.Module, dataloader, node, model_name: str, warmup_iters=3):\n",
        "        def profiling_task():\n",
        "            device = torch.device(f\"cuda:{node.gpu}\" if node.gpu is not None and torch.cuda.is_available() else \"cpu\")\n",
        "            model.to(device)\n",
        "\n",
        "            if self.mode == 'init':\n",
        "                with torch.no_grad():\n",
        "                    cnt = 0\n",
        "                    for inputs, targets in dataloader:\n",
        "                        inputs = inputs.to(device)\n",
        "                        targets = targets.to(device)\n",
        "                        model(inputs)\n",
        "                        cnt += 1\n",
        "                        if cnt >= warmup_iters:\n",
        "                            break\n",
        "                self._profile_init(model, dataloader, node, model_name, device)\n",
        "            else:\n",
        "                self._profile_runtime(model, dataloader, node, model_name, device)\n",
        "\n",
        "        rq = node.assign_task(profiling_task)\n",
        "        rq.get()\n",
        "\n",
        "    def _profile_init(self, model, dataloader, node, model_name, device):\n",
        "        self._register_hooks(model)\n",
        "        with torch.profiler.profile(\n",
        "            activities=[torch.profiler.ProfilerActivity.CPU, torch.profiler.ProfilerActivity.CUDA],\n",
        "            profile_memory=True\n",
        "        ) as prof:\n",
        "            with torch.no_grad():\n",
        "                for step, (inp, tgt) in enumerate(dataloader):\n",
        "                    if step >= 1:\n",
        "                        break\n",
        "                    inp = inp.to(device)\n",
        "                    tgt = tgt.to(device)\n",
        "                    model(inp)\n",
        "                    prof.step()\n",
        "\n",
        "        stats = self._process_events(prof, model, node, runtime=False)\n",
        "        self._update_profile_db(stats, model_name, node, runtime=False)\n",
        "\n",
        "    def _profile_runtime(self, model, dataloader, node, model_name, device):\n",
        "        self._register_hooks(model)\n",
        "        with torch.no_grad():\n",
        "            for step, (inp, tgt) in enumerate(dataloader):\n",
        "                if step >= 1:\n",
        "                    break\n",
        "                inp = inp.to(device)\n",
        "                tgt = tgt.to(device)\n",
        "                with torch.profiler.profile(\n",
        "                    activities=[torch.profiler.ProfilerActivity.CPU, torch.profiler.ProfilerActivity.CUDA]\n",
        "                ) as prof:\n",
        "                    model(inp)\n",
        "                    prof.step()\n",
        "                stats = self._process_events(prof, model, node, runtime=True)\n",
        "                self._append_runtime_csv(stats, model_name, node)\n",
        "\n",
        "    def _process_events(self, profiler, model, node, runtime=False):\n",
        "        recognized = set()\n",
        "        for n, m in model.named_modules():\n",
        "            if n:\n",
        "                recognized.add(n)\n",
        "\n",
        "        aggregated = {\n",
        "            'forward_pass': dict(self_cpu_time_total=0, cpu_time_total=0, cuda_time_total=0,\n",
        "                                 self_cpu_memory_usage=0, self_cuda_memory_usage=0, compute=node.node_id),\n",
        "            'misc': dict(self_cpu_time_total=0, cpu_time_total=0, cuda_time_total=0,\n",
        "                         self_cpu_memory_usage=0, self_cuda_memory_usage=0, compute=node.node_id)\n",
        "        }\n",
        "\n",
        "        events = list(profiler.events())\n",
        "        found_root = False\n",
        "\n",
        "        def strip_suffix(s):\n",
        "            return re.sub(r'(\\.|_)\\d+$', '', s)\n",
        "\n",
        "        for e in events:\n",
        "            if e.name == \"\":\n",
        "                found_root = True\n",
        "                aggregated['forward_pass']['self_cpu_time_total'] += e.self_cpu_time_total\n",
        "                aggregated['forward_pass']['cpu_time_total'] += e.cpu_time_total\n",
        "                aggregated['forward_pass']['cuda_time_total'] += e.device_time_total\n",
        "                if not runtime:\n",
        "                    aggregated['forward_pass']['self_cpu_memory_usage'] += e.self_cpu_memory_usage\n",
        "                    aggregated['forward_pass']['self_cuda_memory_usage'] += e.self_device_memory_usage\n",
        "            else:\n",
        "                base = strip_suffix(e.name)\n",
        "                if base in recognized:\n",
        "                    if base not in aggregated:\n",
        "                        aggregated[base] = dict(\n",
        "                            self_cpu_time_total=0, cpu_time_total=0, cuda_time_total=0,\n",
        "                            self_cpu_memory_usage=0, self_cuda_memory_usage=0,\n",
        "                            compute=node.node_id\n",
        "                        )\n",
        "                    aggregated[base]['self_cpu_time_total'] += e.self_cpu_time_total\n",
        "                    aggregated[base]['cpu_time_total'] += e.cpu_time_total\n",
        "                    aggregated[base]['cuda_time_total'] += e.device_time_total\n",
        "                    if not runtime:\n",
        "                        aggregated[base]['self_cpu_memory_usage'] += e.self_cpu_memory_usage\n",
        "                        aggregated[base]['self_cuda_memory_usage'] += e.self_device_memory_usage\n",
        "                else:\n",
        "                    aggregated['misc']['self_cpu_time_total'] += e.self_cpu_time_total\n",
        "                    aggregated['misc']['cpu_time_total'] += e.cpu_time_total\n",
        "                    aggregated['misc']['cuda_time_total'] += e.device_time_total\n",
        "                    if not runtime:\n",
        "                        aggregated['misc']['self_cpu_memory_usage'] += e.self_cpu_memory_usage\n",
        "                        aggregated['misc']['self_cuda_memory_usage'] += e.self_device_memory_usage\n",
        "\n",
        "        if not found_root:\n",
        "            for k in list(aggregated.keys()):\n",
        "                if k not in ('forward_pass', 'misc'):\n",
        "                    aggregated['forward_pass']['self_cpu_time_total'] += aggregated[k]['self_cpu_time_total']\n",
        "                    aggregated['forward_pass']['cpu_time_total'] += aggregated[k]['cpu_time_total']\n",
        "                    aggregated['forward_pass']['cuda_time_total'] += aggregated[k]['cuda_time_total']\n",
        "                    if not runtime:\n",
        "                        aggregated['forward_pass']['self_cpu_memory_usage'] += aggregated[k]['self_cpu_memory_usage']\n",
        "                        aggregated['forward_pass']['self_cuda_memory_usage'] += aggregated[k]['self_cuda_memory_usage']\n",
        "\n",
        "            aggregated['forward_pass']['self_cpu_time_total'] += aggregated['misc']['self_cpu_time_total']\n",
        "            aggregated['forward_pass']['cpu_time_total'] += aggregated['misc']['cpu_time_total']\n",
        "            aggregated['forward_pass']['cuda_time_total'] += aggregated['misc']['cuda_time_total']\n",
        "            if not runtime:\n",
        "                aggregated['forward_pass']['self_cpu_memory_usage'] += aggregated['misc']['self_cpu_memory_usage']\n",
        "                aggregated['forward_pass']['self_cuda_memory_usage'] += aggregated['misc']['self_cuda_memory_usage']\n",
        "\n",
        "        return aggregated\n",
        "\n",
        "    def _update_profile_db(self, stats, model_name, node, runtime=False):\n",
        "        if runtime:\n",
        "            return\n",
        "        for layer_name, data in stats.items():\n",
        "            total_t = data['cpu_time_total'] + data['cuda_time_total']\n",
        "            total_m = data['self_cpu_memory_usage'] + data['self_cuda_memory_usage']\n",
        "            row = {\n",
        "                'Model': model_name,\n",
        "                'Layer': layer_name,\n",
        "                'Compute': data['compute'],\n",
        "                'Self CPU (us)': data['self_cpu_time_total'],\n",
        "                'CPU Total (us)': data['cpu_time_total'],\n",
        "                'CUDA Total (us)': data['cuda_time_total'],\n",
        "                'Self CPU Mem (bytes)': data['self_cpu_memory_usage'],\n",
        "                'Self CUDA Mem (bytes)': data['self_cuda_memory_usage'],\n",
        "                'Total Execution Time (us)': total_t,\n",
        "                'Total Memory Used (bytes)': total_m\n",
        "            }\n",
        "            self.profile_db = self._upsert(self.profile_db, row)\n",
        "        self.profile_db.to_csv(self.profile_db_path, index=False)\n",
        "\n",
        "    def _upsert(self, df, row):\n",
        "        mask = (\n",
        "            (df['Model'] == row['Model']) &\n",
        "            (df['Layer'] == row['Layer']) &\n",
        "            (df['Compute'] == row['Compute'])\n",
        "        )\n",
        "        if not df[mask].empty:\n",
        "            existing_time = df.loc[mask, 'Total Execution Time (us)'].max()\n",
        "            if row['Total Execution Time (us)'] > existing_time:\n",
        "                for k, v in row.items():\n",
        "                    df.loc[mask, k] = v\n",
        "        else:\n",
        "            # Ensure row is not empty before concatenation\n",
        "            new_row = pd.DataFrame([row])\n",
        "            df = pd.concat([df, new_row], ignore_index=True)\n",
        "        return df\n",
        "\n",
        "    def _append_runtime_csv(self, stats, model_name, node):\n",
        "        rows = []\n",
        "        for layer_name, data in stats.items():\n",
        "            exec_time = data['cpu_time_total'] + data['cuda_time_total']\n",
        "            rows.append({\n",
        "                'Model': model_name,\n",
        "                'Layer': layer_name,\n",
        "                'Compute': data['compute'],\n",
        "                'Execution Time (us)': exec_time\n",
        "            })\n",
        "        if rows:\n",
        "            # Avoid reading empty runtime_csv by checking if it exists\n",
        "            if os.path.exists(self.runtime_csv):\n",
        "                rdf = pd.read_csv(self.runtime_csv)\n",
        "                rdf = pd.concat([rdf, pd.DataFrame(rows)], ignore_index=True)\n",
        "            else:\n",
        "                rdf = pd.DataFrame(rows)\n",
        "            rdf.to_csv(self.runtime_csv, index=False)\n",
        "\n",
        "    def get_profile_db(self):\n",
        "        return self.profile_db\n",
        "\n",
        "    def print_profile_db(self):\n",
        "        if self.profile_db.empty:\n",
        "            print(\"ProfileDB is empty.\")\n",
        "        else:\n",
        "            print(\"ProfileDB:\\n\", self.profile_db.to_string(index=False))\n",
        "\n",
        "\n",
        "############################################\n",
        "# 3) Stage Class (Unchanged)\n",
        "############################################\n",
        "\n",
        "class Stage:\n",
        "    def __init__(self, stage_id: str, layers: nn.ModuleList, assigned_node: 'Node'):\n",
        "        self.stage_id = stage_id\n",
        "        self.layers = layers\n",
        "        self.assigned_node = assigned_node\n",
        "\n",
        "        self.input_data = None\n",
        "        self.output_data = None\n",
        "        self.execution_time = 0.0\n",
        "\n",
        "        self.dependencies = set()\n",
        "        self.dependents = set()\n",
        "\n",
        "    def run_stage(self):\n",
        "        import time\n",
        "        start = time.time()\n",
        "        device = torch.device(\n",
        "            f\"cuda:{self.assigned_node.gpu}\" if self.assigned_node.gpu is not None and torch.cuda.is_available()\n",
        "            else \"cpu\"\n",
        "        )\n",
        "        if self.input_data is None:\n",
        "            print(f\"[Stage] {self.stage_id}: No input data provided.\")\n",
        "            out = torch.tensor([])\n",
        "        else:\n",
        "            inp = self.input_data.to(device)\n",
        "\n",
        "            with torch.no_grad():\n",
        "                out = inp\n",
        "                for layer in self.layers:\n",
        "                    out = layer(out)\n",
        "\n",
        "            if device.type == 'cuda':\n",
        "                out = out.cpu()\n",
        "\n",
        "        self.output_data = out\n",
        "        end = time.time()\n",
        "        self.execution_time = end - start\n",
        "        return self.output_data\n",
        "\n",
        "\n",
        "############################################\n",
        "# 4) Scheduler Class (Updated to Use Observation Window and Data Propagation)\n",
        "############################################\n",
        "\n",
        "class Scheduler:\n",
        "    \"\"\"\n",
        "    A DP-based scheduler that:\n",
        "      - Uses an observation_window to interpret times as a fraction => utilization\n",
        "      - w[k] is the existing utilization on node k (0 <= w[k] <= 1, considering observation window)\n",
        "      - For each new Task, runs a DP up to k_stages_max partitions, picks best s,\n",
        "        and updates w[k] accordingly.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, profiler: 'Profiler', nodes: List['Node'], observation_window: float, slack_percent: float = 0.1):\n",
        "        \"\"\"\n",
        "        :param profiler: A Profiler with layer->node times in a ProfileDB.\n",
        "        :param nodes: A list of Node objects\n",
        "        :param observation_window: The base time window for converting raw times -> utilization fraction\n",
        "        :param slack_percent: an optional multiplier to add slack: e.g. final window = observation_window*(1+slack_percent)\n",
        "        \"\"\"\n",
        "        self.profiler = profiler\n",
        "        self.nodes = nodes\n",
        "\n",
        "        # Incorporate slack into the final used window\n",
        "        self.observation_window = observation_window * (1.0 + slack_percent)\n",
        "\n",
        "        # node_usage w[k] store fraction of the observation window\n",
        "        self.node_usage = [0.0] * len(nodes)\n",
        "\n",
        "        # store per-task stage graph\n",
        "        self.task_stage_graphs: Dict[str, Dict[str, dict]] = {}\n",
        "\n",
        "    def _get_block_fraction(self, block_time: float) -> float:\n",
        "        \"\"\"\n",
        "        Convert raw block_time in seconds -> fraction of the observation window.\n",
        "        \"\"\"\n",
        "        frac = block_time / self.observation_window\n",
        "        return frac\n",
        "\n",
        "    def _get_block_time(self, prefix_times, x, i, node_j) -> float:\n",
        "        return prefix_times[node_j][i] - prefix_times[node_j][x]\n",
        "\n",
        "    def decompose_and_allocate_task(self, task: 'Task', k_stages_max: int = None):\n",
        "        \"\"\"\n",
        "        DP with w[k] usage and observation_window.\n",
        "        We sum up block_time/observation_window to w[k].\n",
        "        \"\"\"\n",
        "        model = task.model\n",
        "        model_name = task.model_name\n",
        "        task_id = task.task_id\n",
        "\n",
        "        # Gather leaf layers\n",
        "        leaf_layers = []\n",
        "        for name, module in model.named_modules():\n",
        "            if len(list(module.children())) == 0 and name:\n",
        "                leaf_layers.append(name)\n",
        "        n_layers = len(leaf_layers)\n",
        "        if n_layers == 0:\n",
        "            print(f\"[Scheduler] No leaf layers found for model={model_name}\")\n",
        "            self.task_stage_graphs[task_id] = {}\n",
        "            return\n",
        "\n",
        "        if k_stages_max is None:\n",
        "            k_stages_max = min(len(self.nodes), n_layers)\n",
        "\n",
        "        profile_db = self.profiler.get_profile_db()\n",
        "        model_df = profile_db[profile_db['Model'] == model_name]\n",
        "\n",
        "        times = [[0.0]*len(self.nodes) for _ in range(n_layers)]\n",
        "        for i2, lname in enumerate(leaf_layers):\n",
        "            for j, node in enumerate(self.nodes):\n",
        "                row = model_df[(model_df['Layer'] == lname) & (model_df['Compute'] == node.node_id)]\n",
        "                if not row.empty:\n",
        "                    t_us = row['Total Execution Time (us)'].values[0]\n",
        "                    times[i2][j] = t_us / 1e6  # raw seconds\n",
        "                else:\n",
        "                    times[i2][j] = 1.0  # fallback\n",
        "\n",
        "        # prefix sums\n",
        "        prefix_times = [[0.0]*(n_layers+1) for _ in range(len(self.nodes))]\n",
        "        for j in range(len(self.nodes)):\n",
        "            for i2 in range(1, n_layers+1):\n",
        "                prefix_times[j][i2] = prefix_times[j][i2-1] + times[i2-1][j]\n",
        "\n",
        "        INF = float('inf')\n",
        "        DP = [[INF]*(k_stages_max+1) for _ in range(n_layers+1)]\n",
        "        back_node = [[-1]*(k_stages_max+1) for _ in range(n_layers+1)]\n",
        "        back_split = [[-1]*(k_stages_max+1) for _ in range(n_layers+1)]\n",
        "\n",
        "        DP[0][0] = 0.0\n",
        "\n",
        "        # Fill DP\n",
        "        for i2 in range(1, n_layers+1):\n",
        "            for s in range(1, k_stages_max+1):\n",
        "                for j in range(len(self.nodes)):\n",
        "                    wj = self.node_usage[j]  # existing usage fraction\n",
        "                    for x in range(i2):\n",
        "                        block_t = self._get_block_time(prefix_times, x, i2, j)  # raw time\n",
        "                        block_frac = self._get_block_fraction(block_t)         # fraction\n",
        "                        cand = max(DP[x][s-1], wj + block_frac)\n",
        "                        if cand < DP[i2][s]:\n",
        "                            DP[i2][s] = cand\n",
        "                            back_node[i2][s] = j\n",
        "                            back_split[i2][s] = x\n",
        "\n",
        "        # Find best s\n",
        "        best_val = INF\n",
        "        best_s = None\n",
        "        for s in range(1, k_stages_max+1):\n",
        "            if DP[n_layers][s] < best_val:\n",
        "                best_val = DP[n_layers][s]\n",
        "                best_s = s\n",
        "        print(f\"[Scheduler] Task={task_id}, Model={model_name}, best s in [1..{k_stages_max}] => {best_s}, final utilization={best_val:.4f}\")\n",
        "\n",
        "        # Reconstruct partitions\n",
        "        i2 = n_layers\n",
        "        s = best_s\n",
        "        partitions = []\n",
        "        while i2 > 0 and s > 0:\n",
        "            j = back_node[i2][s]\n",
        "            x = back_split[i2][s]\n",
        "            partitions.append((x, i2, j))\n",
        "            i2 = x\n",
        "            s -= 1\n",
        "        partitions.reverse()\n",
        "\n",
        "        # Build a stage graph for this task\n",
        "        stage_graph = {}\n",
        "        idx = 0\n",
        "        for (start_i, end_i, node_j) in partitions:\n",
        "            stage_id = f\"{task_id}-stage-{idx}\"\n",
        "            layer_subset = leaf_layers[start_i:end_i]\n",
        "            stage_graph[stage_id] = {\n",
        "                'node_idx': node_j,\n",
        "                'layer_names': layer_subset,\n",
        "                'dependencies': [],\n",
        "                'dependents': []\n",
        "            }\n",
        "            idx += 1\n",
        "\n",
        "        # Link stages linearly\n",
        "        keys = list(stage_graph.keys())\n",
        "        for k2 in range(1, len(keys)):\n",
        "            prev_stg = keys[k2-1]\n",
        "            curr_stg = keys[k2]\n",
        "            stage_graph[curr_stg]['dependencies'].append(prev_stg)\n",
        "            stage_graph[prev_stg]['dependents'].append(curr_stg)\n",
        "\n",
        "        # Store the stage graph\n",
        "        self.task_stage_graphs[task_id] = stage_graph\n",
        "\n",
        "        # Update w[k] with assigned block fractions\n",
        "        for (start_i, end_i, node_j) in partitions:\n",
        "            raw_block_time = self._get_block_time(prefix_times, start_i, end_i, node_j)\n",
        "            block_frac = self._get_block_fraction(raw_block_time)\n",
        "            self.node_usage[node_j] += block_frac\n",
        "\n",
        "        # Print final stage decomposition\n",
        "        self.print_task_stages(task_id)\n",
        "\n",
        "    def print_task_stages(self, task_id: str):\n",
        "        \"\"\"\n",
        "        Print: which stage (plus consecutive layers) is on which node, for the given task.\n",
        "        \"\"\"\n",
        "        if task_id not in self.task_stage_graphs:\n",
        "            print(f\"[Scheduler] No stage graph for task={task_id}\")\n",
        "            return\n",
        "\n",
        "        stg_graph = self.task_stage_graphs[task_id]\n",
        "        print(f\"[Scheduler] Stage Decomposition for Task={task_id}:\")\n",
        "        for sid, info in stg_graph.items():\n",
        "            node_j = info['node_idx']\n",
        "            node_id = self.nodes[node_j].node_id\n",
        "            layers = info['layer_names']\n",
        "            deps = info['dependencies']\n",
        "            print(f\"  Stage={sid}, Node={node_id}, Layers={layers}, DependsOn={list(deps)}\")\n",
        "\n",
        "    def create_stages_for_task(self, task: 'Task'):\n",
        "        \"\"\"\n",
        "        Convert the stored stage graph for this task into Stage objects for pipeline execution.\n",
        "        \"\"\"\n",
        "        import torch.nn as nn\n",
        "        t_id = task.task_id\n",
        "        if t_id not in self.task_stage_graphs:\n",
        "            print(f\"[Scheduler] No stage graph found for Task={t_id}.\")\n",
        "            return\n",
        "\n",
        "        stg_graph = self.task_stage_graphs[t_id]\n",
        "        for stage_id, info in stg_graph.items():\n",
        "            node_idx = info['node_idx']\n",
        "            node = self.nodes[node_idx]\n",
        "\n",
        "            layer_list = []\n",
        "            for lname in info['layer_names']:\n",
        "                layer = task.model.get_submodule(lname)\n",
        "                if layer is not None:\n",
        "                    layer_list.append(layer)\n",
        "                else:\n",
        "                    print(f\"[Scheduler] Warning: Layer '{lname}' not found in model '{task.model_name}'.\")\n",
        "\n",
        "            stg = Stage(stage_id, nn.ModuleList(layer_list), node)\n",
        "            stg.dependencies = set(info['dependencies'])\n",
        "            stg.dependents = set(info['dependents'])\n",
        "            task.stages[stage_id] = stg\n",
        "\n",
        "    def run_pipeline_parallel(self, task: 'Task'):\n",
        "        \"\"\"\n",
        "        The pipeline logic for one task:\n",
        "         - Start all stages with zero deps\n",
        "         - Each stage on finishing decrements the dependents\n",
        "         - Launch a worker thread for each stage\n",
        "         - Propagate output data to dependent stages\n",
        "        \"\"\"\n",
        "        lock = threading.Lock()\n",
        "        dep_count = {}\n",
        "        for sid, stg in task.stages.items():\n",
        "            dep_count[sid] = len(stg.dependencies)\n",
        "\n",
        "        total_stages = len(task.stages)\n",
        "        completed = 0\n",
        "        done_event = threading.Event()\n",
        "\n",
        "        def stage_done_callback(sid: str):\n",
        "            nonlocal completed\n",
        "            with lock:\n",
        "                completed_stage = task.stages[sid]\n",
        "                for dep_id in completed_stage.dependents:\n",
        "                    dependent_stage = task.stages[dep_id]\n",
        "                    # Propagate output data to dependent stage's input\n",
        "                    dependent_stage.input_data = completed_stage.output_data\n",
        "                    dep_count[dep_id] -= 1\n",
        "                    if dep_count[dep_id] == 0:\n",
        "                        enqueue_stage(dep_id)\n",
        "                completed += 1\n",
        "                if completed >= total_stages:\n",
        "                    done_event.set()\n",
        "\n",
        "        def enqueue_stage(sid: str):\n",
        "            stg = task.stages[sid]\n",
        "            def run_fn():\n",
        "                return stg.run_stage()\n",
        "            def worker():\n",
        "                rq = stg.assigned_node.assign_task(run_fn)\n",
        "                result = rq.get()\n",
        "                if isinstance(result, Exception):\n",
        "                    print(f\"[Pipeline] Stage {sid} encountered an exception: {result}\")\n",
        "                stage_done_callback(sid)\n",
        "            threading.Thread(target=worker, daemon=True).start()\n",
        "\n",
        "        # Enqueue initial stages with zero dependencies\n",
        "        for sid, c in dep_count.items():\n",
        "            if c == 0:\n",
        "                enqueue_stage(sid)\n",
        "\n",
        "        done_event.wait()\n",
        "\n",
        "\n",
        "############################################\n",
        "# 5) Task Class (Unchanged)\n",
        "############################################\n",
        "\n",
        "from typing import Dict, Optional\n",
        "\n",
        "class Task:\n",
        "    \"\"\"\n",
        "    Represents a single inference pass for a model + input.\n",
        "    Stages are built by the Scheduler's create_stages_for_task call.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, task_id: str, model: nn.Module, input_data: torch.Tensor, model_name: str):\n",
        "        self.task_id = task_id\n",
        "        self.model = model\n",
        "        self.input_data = input_data\n",
        "        self.model_name = model_name\n",
        "\n",
        "        self.stages: Dict[str, Stage] = {}\n",
        "\n",
        "        self.start_time: Optional[float] = None\n",
        "        self.finish_time: Optional[float] = None\n",
        "\n",
        "    def get_total_execution_time(self):\n",
        "        if self.start_time and self.finish_time:\n",
        "            return self.finish_time - self.start_time\n",
        "        return 0.0\n",
        "\n",
        "\n",
        "############################################\n",
        "# 6) Taskset Class (Unchanged)\n",
        "############################################\n",
        "\n",
        "import time\n",
        "\n",
        "class Taskset:\n",
        "    \"\"\"\n",
        "    A collection of tasks to run with the DP-based pipeline approach.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, tasks: List[Task], scheduler: Scheduler):\n",
        "        self.tasks = tasks\n",
        "        self.scheduler = scheduler\n",
        "\n",
        "    def schedule_all_tasks(self):\n",
        "        # Build stage objects for each Task from the global stage graph\n",
        "        for t in self.tasks:\n",
        "            self.scheduler.create_stages_for_task(t)\n",
        "\n",
        "    def execute_all(self):\n",
        "        # We run each task in parallel\n",
        "        import threading\n",
        "        start_global = time.time()\n",
        "        threads = []\n",
        "\n",
        "        def run_task(task: Task):\n",
        "            task.start_time = time.time()\n",
        "            self.scheduler.run_pipeline_parallel(task)\n",
        "            task.finish_time = time.time()\n",
        "\n",
        "        for t in self.tasks:\n",
        "            th = threading.Thread(target=run_task, args=(t,), daemon=True)\n",
        "            th.start()\n",
        "            threads.append(th)\n",
        "\n",
        "        for th in threads:\n",
        "            th.join()\n",
        "\n",
        "        end_global = time.time()\n",
        "        print(f\"[Taskset] All tasks completed in {end_global - start_global:.2f}s\")\n",
        "\n",
        "    def collect_metrics(self):\n",
        "        # For demonstration, we do not track node usage in detail here.\n",
        "        return {}\n",
        "\n",
        "\n",
        "############################################\n",
        "# 7) Evaluator Class (Unchanged)\n",
        "############################################\n",
        "\n",
        "class Evaluator:\n",
        "    \"\"\"\n",
        "    Compares:\n",
        "      1) Naive single-device run\n",
        "      2) Pipeline approach (DP-based)\n",
        "    and checks whether the final outputs match.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, device='cpu', print_precision=2, rtol=1e-03, atol=1e-05):\n",
        "        self.device = device\n",
        "        self.print_precision = print_precision\n",
        "        self.rtol = rtol\n",
        "        self.atol = atol\n",
        "\n",
        "        self.naive_run_time = None\n",
        "        self.pipeline_run_time = None\n",
        "\n",
        "        # Store final outputs\n",
        "        #   self.naive_outputs[task_id] = tensor\n",
        "        #   self.pipeline_outputs[task_id] = tensor\n",
        "        self.naive_outputs = {}\n",
        "        self.pipeline_outputs = {}\n",
        "\n",
        "    def run_naive_pytorch(self, tasks: List['Task']):\n",
        "        \"\"\"\n",
        "        Runs each Task sequentially on self.device (layer-by-layer).\n",
        "        Also stores each Task's final output in self.naive_outputs[task.task_id].\n",
        "        \"\"\"\n",
        "        import time\n",
        "        dev = torch.device(self.device)\n",
        "        start = time.time()\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for task in tasks:\n",
        "                task.model.to(dev)\n",
        "                out = task.model(task.input_data.to(dev))\n",
        "                out_cpu = out.cpu() if dev.type == 'cuda' else out\n",
        "                self.naive_outputs[task.task_id] = out_cpu  # store the final naive output\n",
        "\n",
        "        end = time.time()\n",
        "        self.naive_run_time = end - start\n",
        "        print(f\"[Evaluator] Naive run on device='{self.device}' for {len(tasks)} tasks => \"\n",
        "              f\"{self.naive_run_time:.{self.print_precision}f}s\")\n",
        "\n",
        "    def run_pipeline(self, taskset: 'Taskset'):\n",
        "        \"\"\"\n",
        "        1) schedule_all_tasks -> builds Stage objects for each Task\n",
        "        2) execute_all -> runs pipeline parallel\n",
        "        3) gather final outputs from each Task\n",
        "        \"\"\"\n",
        "        import time\n",
        "        start = time.time()\n",
        "\n",
        "        taskset.schedule_all_tasks()   # build stage objects\n",
        "        taskset.execute_all()          # pipeline parallel run\n",
        "\n",
        "        end = time.time()\n",
        "        self.pipeline_run_time = end - start\n",
        "        print(f\"[Evaluator] Pipeline approach => {self.pipeline_run_time:.{self.print_precision}f}s\")\n",
        "\n",
        "        # Collect final outputs from tasks\n",
        "        for task in taskset.tasks:\n",
        "            # we assume the last stage's output_data is the final output\n",
        "            # let's gather that\n",
        "            final_out = self._get_task_final_output(task)\n",
        "            self.pipeline_outputs[task.task_id] = final_out\n",
        "\n",
        "    def _get_task_final_output(self, task: 'Task'):\n",
        "        \"\"\"\n",
        "        Retrieve the final stage's output_data (the stage that has no dependents).\n",
        "        Or just take the largest stage index if we name them in ascending order.\n",
        "        \"\"\"\n",
        "        if not task.stages:\n",
        "            return None\n",
        "        # find stage with no dependents (or the max stage ID)\n",
        "        final_stage = None\n",
        "        for sid, stg in task.stages.items():\n",
        "            if not stg.dependents:  # no dependents => final stage\n",
        "                final_stage = stg\n",
        "                break\n",
        "        if final_stage is None:\n",
        "            # fallback: just pick the highest stage ID by sorting\n",
        "            sorted_ids = sorted(task.stages.keys())\n",
        "            final_stage = task.stages[sorted_ids[-1]]\n",
        "        return final_stage.output_data\n",
        "\n",
        "    def compare_results(self, node_usage_info: dict = None):\n",
        "        \"\"\"\n",
        "        Compares:\n",
        "          - naive_run_time vs pipeline_run_time\n",
        "          - naive_outputs vs pipeline_outputs\n",
        "        \"\"\"\n",
        "        print(\"\\n=== Comparison ===\")\n",
        "        if self.naive_run_time is None or self.pipeline_run_time is None:\n",
        "            print(\"Cannot compare times; missing run times.\")\n",
        "        else:\n",
        "            print(f\"Naive total   : {self.naive_run_time:.{self.print_precision}f}s\")\n",
        "            print(f\"Pipeline total: {self.pipeline_run_time:.{self.print_precision}f}s\")\n",
        "            if self.pipeline_run_time == 0.0:\n",
        "                speedup = float('inf')\n",
        "            else:\n",
        "                speedup = self.naive_run_time / self.pipeline_run_time\n",
        "            print(f\"Speedup = {speedup:.{self.print_precision}f}x\")\n",
        "\n",
        "        # Compare final outputs\n",
        "        print(\"\\n--- Output Similarity Checks ---\")\n",
        "        for task_id, naive_out in self.naive_outputs.items():\n",
        "            pipe_out = self.pipeline_outputs.get(task_id, None)\n",
        "            if pipe_out is None:\n",
        "                print(f\"Task={task_id}: No pipeline output found.\")\n",
        "                continue\n",
        "            # Check if shape matches\n",
        "            if naive_out.shape != pipe_out.shape:\n",
        "                print(f\"Task={task_id}: Output shape mismatch.\")\n",
        "                continue\n",
        "            # Compare with torch.allclose\n",
        "            close = torch.allclose(naive_out, pipe_out, rtol=self.rtol, atol=self.atol)\n",
        "            if close:\n",
        "                print(f\"Task={task_id}: Outputs match within tolerance.\")\n",
        "            else:\n",
        "                print(f\"Task={task_id}: Outputs differ beyond tolerance.\")\n",
        "\n",
        "        # Optionally show node usage info\n",
        "        if node_usage_info:\n",
        "            print(\"\\nNode Usage Info:\")\n",
        "            for k, v in node_usage_info.items():\n",
        "                print(f\"  Node={k}, usage={v}\")\n",
        "\n",
        "        print(\"\\n=== End Comparison ===\")\n",
        "\n",
        "\n",
        "############################################\n",
        "# 8) main_demo Test Script (Updated)\n",
        "############################################\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.utils.data as data\n",
        "import gc\n",
        "import torchvision.models as models\n",
        "\n",
        "def main_demo():\n",
        "    print(\"=== main_demo with Observation Window Logic ===\")\n",
        "\n",
        "    # 1) Setup init phase\n",
        "    def init_phase():\n",
        "        print(\"=== INIT PHASE ===\")\n",
        "        profiler = Profiler(mode='init', profile_db_path='dp_partition_profiling.csv')\n",
        "        nodes = Node.discover_nodes()\n",
        "        if not nodes:\n",
        "            print(\"No nodes discovered. Exiting.\")\n",
        "            return None, None, None, None\n",
        "\n",
        "        # For demonstration, we profile 2 models\n",
        "        print(\"\\n-- Profiling models --\")\n",
        "        # a) Dummy dataset for a simple FFN\n",
        "        class FFNDataset(data.Dataset):\n",
        "            def __init__(self, size=5):\n",
        "                self.size = size\n",
        "            def __len__(self):\n",
        "                return self.size\n",
        "            def __getitem__(self, idx):\n",
        "                return torch.randn(1,256), torch.tensor(0)\n",
        "\n",
        "        # b) Dummy dataset for ResNet\n",
        "        class ResNetDataset(data.Dataset):\n",
        "            def __init__(self, size=5):\n",
        "                self.size = size\n",
        "            def __len__(self):\n",
        "                return self.size\n",
        "            def __getitem__(self, idx):\n",
        "                return torch.randn(3,224,224), torch.tensor(0)\n",
        "\n",
        "        ffn_loader = data.DataLoader(FFNDataset(), batch_size=1)\n",
        "        res_loader = data.DataLoader(ResNetDataset(), batch_size=1)\n",
        "\n",
        "        # Profile both\n",
        "        ffn_model = nn.Sequential(nn.Linear(256, 512), nn.ReLU(), nn.Linear(512,128))\n",
        "        # Updated ResNet initialization to use 'weights' instead of 'pretrained'\n",
        "        resnet_model = models.resnet18(weights=None)\n",
        "\n",
        "        ffn_model_name = \"InitPhaseFFN\"\n",
        "        res_model_name = \"InitPhaseResNet\"\n",
        "\n",
        "        for node in nodes:\n",
        "            profiler.profile_model(ffn_model, ffn_loader, node, ffn_model_name)\n",
        "            profiler.profile_model(resnet_model, res_loader, node, res_model_name)\n",
        "\n",
        "        # Define tasks to calculate observation window\n",
        "        # For simplicity, assume tasks are similar to those in runtime_phase\n",
        "        class BiggerFFN(nn.Module):\n",
        "            def __init__(self):\n",
        "                super().__init__()\n",
        "                self.net = nn.Sequential(\n",
        "                    nn.Linear(256, 512),\n",
        "                    nn.ReLU(),\n",
        "                    nn.Linear(512, 256),\n",
        "                    nn.ReLU(),\n",
        "                    nn.Linear(256, 128),\n",
        "                    nn.ReLU(),\n",
        "                    nn.Linear(128, 10)\n",
        "                )\n",
        "            def forward(self, x):\n",
        "                return self.net(x)\n",
        "\n",
        "        bigger_ffn_model_name = \"BiggerFFN_ObsWin\"\n",
        "        resnet_model_name = \"ResNet18_ObsWin\"\n",
        "        resnet_model_updated = models.resnet18(weights=None)  # Updated ResNet initialization\n",
        "\n",
        "        # Create dummy tasks to compute observation window\n",
        "        input_ffn = torch.randn(1, 256)\n",
        "        task_ffn1 = Task(\"TaskFFN1\", BiggerFFN(), input_ffn.clone(), bigger_ffn_model_name)\n",
        "        task_ffn2 = Task(\"TaskFFN2\", BiggerFFN(), input_ffn.clone(), bigger_ffn_model_name)\n",
        "\n",
        "        input_res = torch.randn(1, 3, 224, 224)\n",
        "        task_res1 = Task(\"TaskRes1\", resnet_model_updated, input_res.clone(), resnet_model_name)\n",
        "        task_res2 = Task(\"TaskRes2\", resnet_model_updated, input_res.clone(), resnet_model_name)\n",
        "        task_res3 = Task(\"TaskRes3\", resnet_model_updated, input_res.clone(), resnet_model_name)\n",
        "\n",
        "        all_tasks = [task_ffn1, task_ffn2, task_res1, task_res2,task_res3]\n",
        "\n",
        "        # Compute expected execution time for each task (sum of minimal layer times)\n",
        "        profile_db = profiler.get_profile_db()\n",
        "\n",
        "        total_expected_time = 0.0\n",
        "        for task in all_tasks:\n",
        "            model_df = profile_db[profile_db['Model'] == task.model_name]\n",
        "            leaf_layers = [name for name, m in task.model.named_modules() if len(list(m.children())) == 0 and name]\n",
        "            task_time = 0.0\n",
        "            for lname in leaf_layers:\n",
        "                # Find the minimal execution time across all nodes for this layer\n",
        "                layer_times = model_df[model_df['Layer'] == lname]['Total Execution Time (us)']\n",
        "                if not layer_times.empty:\n",
        "                    min_time = layer_times.min() / 1e6  # Convert to seconds\n",
        "                else:\n",
        "                    min_time = 1.0  # Fallback\n",
        "                task_time += min_time\n",
        "            total_expected_time += task_time\n",
        "\n",
        "        print(f\"\\nTotal Expected Execution Time (sum of all tasks' minimal times): {total_expected_time:.4f}s\")\n",
        "\n",
        "        # Apply slack_percent\n",
        "        slack_percent = 0.2\n",
        "        observation_window = total_expected_time * (1.0 + slack_percent)\n",
        "        print(f\"Observation Window (with {slack_percent*100}% slack): {observation_window:.4f}s\")\n",
        "\n",
        "        # Create Scheduler with observation window\n",
        "        scheduler = Scheduler(profiler, nodes, observation_window=observation_window, slack_percent=slack_percent)\n",
        "\n",
        "        # Decompose & allocate all tasks\n",
        "        for task in all_tasks:\n",
        "            scheduler.decompose_and_allocate_task(task, k_stages_max=None)\n",
        "\n",
        "        return profiler, nodes, all_tasks, scheduler\n",
        "\n",
        "    # 2) Runtime phase\n",
        "    def runtime_phase(profiler, nodes, all_tasks, scheduler):\n",
        "        print(\"\\n=== RUNTIME PHASE ===\")\n",
        "\n",
        "        # Build Taskset\n",
        "        taskset = Taskset(all_tasks, scheduler)\n",
        "        evaluator = Evaluator(device='cuda' if torch.cuda.is_available() else 'cpu', print_precision=3)\n",
        "\n",
        "        # Run evaluator\n",
        "        # 1) Naive run\n",
        "        evaluator.run_naive_pytorch(all_tasks)\n",
        "\n",
        "        # Clear caches if desired\n",
        "        if torch.cuda.is_available():\n",
        "            torch.cuda.empty_cache()\n",
        "        gc.collect()\n",
        "\n",
        "        # 2) Pipeline run\n",
        "        print(\"\\n-- Pipeline run with observation window approach --\")\n",
        "        evaluator.run_pipeline(taskset)\n",
        "\n",
        "        # Gather node usage info\n",
        "        usage_info = {scheduler.nodes[i].node_id: scheduler.node_usage[i] for i in range(len(scheduler.nodes))}\n",
        "\n",
        "        evaluator.compare_results(usage_info)\n",
        "\n",
        "        # Stop nodes\n",
        "        for nd in nodes:\n",
        "            nd.stop()\n",
        "\n",
        "        # Print final ProfileDB\n",
        "        print(\"\\nFinal ProfileDB:\")\n",
        "        profiler.print_profile_db()\n",
        "\n",
        "    # actually run\n",
        "    profiler, nodes, all_tasks, scheduler = init_phase()\n",
        "    if profiler is not None and nodes is not None:\n",
        "        runtime_phase(profiler, nodes, all_tasks, scheduler)\n",
        "    else:\n",
        "        print(\"Init phase failed; skipping runtime.\")\n",
        "\n",
        "# Run the main_demo function\n",
        "if __name__ == \"__main__\":\n",
        "    main_demo()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PH0h-kqvxYdj",
        "outputId": "3794be79-d5dd-4075-e039-d98a25b38a53"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== main_demo with Observation Window Logic ===\n",
            "=== INIT PHASE ===\n",
            "\n",
            "-- Profiling models --\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-1-ae3ca7ba0f78>:303: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
            "  df = pd.concat([df, new_row], ignore_index=True)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Total Expected Execution Time (sum of all tasks' minimal times): 170.0000s\n",
            "Observation Window (with 20.0% slack): 204.0000s\n",
            "[Scheduler] Task=TaskFFN1, Model=BiggerFFN_ObsWin, best s in [1..4] => 4, final utilization=0.0082\n",
            "[Scheduler] Stage Decomposition for Task=TaskFFN1:\n",
            "  Stage=TaskFFN1-stage-0, Node=CPU-0, Layers=['net.0'], DependsOn=[]\n",
            "  Stage=TaskFFN1-stage-1, Node=CPU-0, Layers=['net.1', 'net.2'], DependsOn=['TaskFFN1-stage-0']\n",
            "  Stage=TaskFFN1-stage-2, Node=CPU-0, Layers=['net.3', 'net.4'], DependsOn=['TaskFFN1-stage-1']\n",
            "  Stage=TaskFFN1-stage-3, Node=CPU-0, Layers=['net.5', 'net.6'], DependsOn=['TaskFFN1-stage-2']\n",
            "[Scheduler] Task=TaskFFN2, Model=BiggerFFN_ObsWin, best s in [1..4] => 4, final utilization=0.0082\n",
            "[Scheduler] Stage Decomposition for Task=TaskFFN2:\n",
            "  Stage=TaskFFN2-stage-0, Node=CPU-1, Layers=['net.0'], DependsOn=[]\n",
            "  Stage=TaskFFN2-stage-1, Node=CPU-1, Layers=['net.1', 'net.2'], DependsOn=['TaskFFN2-stage-0']\n",
            "  Stage=TaskFFN2-stage-2, Node=CPU-1, Layers=['net.3', 'net.4'], DependsOn=['TaskFFN2-stage-1']\n",
            "  Stage=TaskFFN2-stage-3, Node=CPU-1, Layers=['net.5', 'net.6'], DependsOn=['TaskFFN2-stage-2']\n",
            "[Scheduler] Task=TaskRes1, Model=ResNet18_ObsWin, best s in [1..4] => 4, final utilization=0.0531\n",
            "[Scheduler] Stage Decomposition for Task=TaskRes1:\n",
            "  Stage=TaskRes1-stage-0, Node=GPU-0-CPU-0, Layers=['conv1', 'bn1', 'relu', 'maxpool', 'layer1.0.conv1', 'layer1.0.bn1', 'layer1.0.relu', 'layer1.0.conv2', 'layer1.0.bn2', 'layer1.1.conv1', 'layer1.1.bn1', 'layer1.1.relu', 'layer1.1.conv2'], DependsOn=[]\n",
            "  Stage=TaskRes1-stage-1, Node=GPU-0-CPU-0, Layers=['layer1.1.bn2', 'layer2.0.conv1', 'layer2.0.bn1', 'layer2.0.relu', 'layer2.0.conv2', 'layer2.0.bn2', 'layer2.0.downsample.0', 'layer2.0.downsample.1', 'layer2.1.conv1', 'layer2.1.bn1', 'layer2.1.relu', 'layer2.1.conv2', 'layer2.1.bn2'], DependsOn=['TaskRes1-stage-0']\n",
            "  Stage=TaskRes1-stage-2, Node=GPU-0-CPU-0, Layers=['layer3.0.conv1', 'layer3.0.bn1', 'layer3.0.relu', 'layer3.0.conv2', 'layer3.0.bn2', 'layer3.0.downsample.0', 'layer3.0.downsample.1', 'layer3.1.conv1', 'layer3.1.bn1', 'layer3.1.relu', 'layer3.1.conv2', 'layer3.1.bn2', 'layer4.0.conv1'], DependsOn=['TaskRes1-stage-1']\n",
            "  Stage=TaskRes1-stage-3, Node=GPU-0-CPU-0, Layers=['layer4.0.bn1', 'layer4.0.relu', 'layer4.0.conv2', 'layer4.0.bn2', 'layer4.0.downsample.0', 'layer4.0.downsample.1', 'layer4.1.conv1', 'layer4.1.bn1', 'layer4.1.relu', 'layer4.1.conv2', 'layer4.1.bn2', 'avgpool', 'fc'], DependsOn=['TaskRes1-stage-2']\n",
            "[Scheduler] Task=TaskRes2, Model=ResNet18_ObsWin, best s in [1..4] => 4, final utilization=0.0531\n",
            "[Scheduler] Stage Decomposition for Task=TaskRes2:\n",
            "  Stage=TaskRes2-stage-0, Node=GPU-0-CPU-1, Layers=['conv1', 'bn1', 'relu', 'maxpool', 'layer1.0.conv1', 'layer1.0.bn1', 'layer1.0.relu', 'layer1.0.conv2', 'layer1.0.bn2', 'layer1.1.conv1', 'layer1.1.bn1', 'layer1.1.relu', 'layer1.1.conv2'], DependsOn=[]\n",
            "  Stage=TaskRes2-stage-1, Node=GPU-0-CPU-1, Layers=['layer1.1.bn2', 'layer2.0.conv1', 'layer2.0.bn1', 'layer2.0.relu', 'layer2.0.conv2', 'layer2.0.bn2', 'layer2.0.downsample.0', 'layer2.0.downsample.1', 'layer2.1.conv1', 'layer2.1.bn1', 'layer2.1.relu', 'layer2.1.conv2', 'layer2.1.bn2'], DependsOn=['TaskRes2-stage-0']\n",
            "  Stage=TaskRes2-stage-2, Node=GPU-0-CPU-1, Layers=['layer3.0.conv1', 'layer3.0.bn1', 'layer3.0.relu', 'layer3.0.conv2', 'layer3.0.bn2', 'layer3.0.downsample.0', 'layer3.0.downsample.1', 'layer3.1.conv1', 'layer3.1.bn1', 'layer3.1.relu', 'layer3.1.conv2', 'layer3.1.bn2', 'layer4.0.conv1'], DependsOn=['TaskRes2-stage-1']\n",
            "  Stage=TaskRes2-stage-3, Node=GPU-0-CPU-1, Layers=['layer4.0.bn1', 'layer4.0.relu', 'layer4.0.conv2', 'layer4.0.bn2', 'layer4.0.downsample.0', 'layer4.0.downsample.1', 'layer4.1.conv1', 'layer4.1.bn1', 'layer4.1.relu', 'layer4.1.conv2', 'layer4.1.bn2', 'avgpool', 'fc'], DependsOn=['TaskRes2-stage-2']\n",
            "[Scheduler] Task=TaskRes3, Model=ResNet18_ObsWin, best s in [1..4] => 4, final utilization=0.0817\n",
            "[Scheduler] Stage Decomposition for Task=TaskRes3:\n",
            "  Stage=TaskRes3-stage-0, Node=CPU-0, Layers=['conv1', 'bn1', 'relu', 'maxpool', 'layer1.0.conv1', 'layer1.0.bn1', 'layer1.0.relu', 'layer1.0.conv2', 'layer1.0.bn2', 'layer1.1.conv1', 'layer1.1.bn1', 'layer1.1.relu', 'layer1.1.conv2'], DependsOn=[]\n",
            "  Stage=TaskRes3-stage-1, Node=CPU-0, Layers=['layer1.1.bn2', 'layer2.0.conv1', 'layer2.0.bn1', 'layer2.0.relu', 'layer2.0.conv2', 'layer2.0.bn2', 'layer2.0.downsample.0', 'layer2.0.downsample.1', 'layer2.1.conv1', 'layer2.1.bn1', 'layer2.1.relu', 'layer2.1.conv2', 'layer2.1.bn2'], DependsOn=['TaskRes3-stage-0']\n",
            "  Stage=TaskRes3-stage-2, Node=CPU-0, Layers=['layer3.0.conv1', 'layer3.0.bn1', 'layer3.0.relu', 'layer3.0.conv2', 'layer3.0.bn2', 'layer3.0.downsample.0', 'layer3.0.downsample.1', 'layer3.1.conv1', 'layer3.1.bn1', 'layer3.1.relu', 'layer3.1.conv2', 'layer3.1.bn2', 'layer4.0.conv1'], DependsOn=['TaskRes3-stage-1']\n",
            "  Stage=TaskRes3-stage-3, Node=CPU-0, Layers=['layer4.0.bn1', 'layer4.0.relu', 'layer4.0.conv2', 'layer4.0.bn2', 'layer4.0.downsample.0', 'layer4.0.downsample.1', 'layer4.1.conv1', 'layer4.1.bn1', 'layer4.1.relu', 'layer4.1.conv2', 'layer4.1.bn2', 'avgpool', 'fc'], DependsOn=['TaskRes3-stage-2']\n",
            "\n",
            "=== RUNTIME PHASE ===\n",
            "[Evaluator] Naive run on device='cuda' for 5 tasks => 0.077s\n",
            "\n",
            "-- Pipeline run with observation window approach --\n",
            "[Stage] TaskFFN1-stage-0: No input data provided.\n",
            "[Stage] TaskFFN2-stage-0: No input data provided.\n",
            "[Stage] TaskRes1-stage-0: No input data provided.\n",
            "[Pipeline] Stage TaskRes1-stage-1 encountered an exception: expected 4D input (got 1D input)\n",
            "[Stage] TaskRes1-stage-2: No input data provided.[Stage] TaskRes2-stage-0: No input data provided.\n",
            "\n",
            "[Pipeline] Stage TaskRes1-stage-3 encountered an exception: expected 4D input (got 1D input)\n",
            "[Pipeline] Stage TaskRes2-stage-1 encountered an exception: expected 4D input (got 1D input)\n",
            "[Stage] TaskRes2-stage-2: No input data provided.\n",
            "[Pipeline] Stage TaskRes2-stage-3 encountered an exception: expected 4D input (got 1D input)\n",
            "[Stage] TaskRes3-stage-0: No input data provided.[Pipeline] Stage TaskFFN2-stage-1 encountered an exception: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu! (when checking argument for argument mat1 in method wrapper_CUDA_addmm)\n",
            "[Pipeline] Stage TaskFFN1-stage-1 encountered an exception: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu! (when checking argument for argument mat1 in method wrapper_CUDA_addmm)\n",
            "[Stage] TaskFFN2-stage-2: No input data provided.\n",
            "\n",
            "[Pipeline] Stage TaskRes3-stage-1 encountered an exception: expected 4D input (got 1D input)\n",
            "[Pipeline] Stage TaskFFN2-stage-3 encountered an exception: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu! (when checking argument for argument mat1 in method wrapper_CUDA_addmm)\n",
            "[Stage] TaskFFN1-stage-2: No input data provided.\n",
            "[Stage] TaskRes3-stage-2: No input data provided.\n",
            "[Pipeline] Stage TaskFFN1-stage-3 encountered an exception: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu! (when checking argument for argument mat1 in method wrapper_CUDA_addmm)\n",
            "[Pipeline] Stage TaskRes3-stage-3 encountered an exception: expected 4D input (got 1D input)\n",
            "[Taskset] All tasks completed in 0.09s\n",
            "[Evaluator] Pipeline approach => 0.096s\n",
            "\n",
            "=== Comparison ===\n",
            "Naive total   : 0.077s\n",
            "Pipeline total: 0.096s\n",
            "Speedup = 0.804x\n",
            "\n",
            "--- Output Similarity Checks ---\n",
            "Task=TaskFFN1: No pipeline output found.\n",
            "Task=TaskFFN2: No pipeline output found.\n",
            "Task=TaskRes1: No pipeline output found.\n",
            "Task=TaskRes2: No pipeline output found.\n",
            "Task=TaskRes3: No pipeline output found.\n",
            "\n",
            "Node Usage Info:\n",
            "  Node=CPU-0, usage=0.24101307189542487\n",
            "  Node=CPU-1, usage=0.028594771241830068\n",
            "  Node=GPU-0-CPU-0, usage=0.2124183006535948\n",
            "  Node=GPU-0-CPU-1, usage=0.2124183006535948\n",
            "\n",
            "=== End Comparison ===\n",
            "\n",
            "Final ProfileDB:\n",
            "ProfileDB:\n",
            "           Model                 Layer     Compute  Self CPU (us)  CPU Total (us) CUDA Total (us) Self CPU Mem (bytes) Self CUDA Mem (bytes)  Total Execution Time (us) Total Memory Used (bytes)\n",
            "   InitPhaseFFN          forward_pass       CPU-0     295031.857      296236.796               0                 1032                     0                 296236.796                      1032\n",
            "   InitPhaseFFN                  misc       CPU-0     294941.921      296146.860               0                 1032                     0                 296146.860                      1032\n",
            "   InitPhaseFFN                     0       CPU-0         60.795          60.795               0                    0                     0                     60.795                         0\n",
            "   InitPhaseFFN                     1       CPU-0         29.141          29.141               0                    0                     0                     29.141                         0\n",
            "InitPhaseResNet          forward_pass       CPU-0     193203.951      680341.778               0               602120                     0                 680341.778                    602120\n",
            "InitPhaseResNet                  misc       CPU-0     183640.093      670777.920               0               602120                     0                 670777.920                    602120\n",
            "InitPhaseResNet                 conv1       CPU-0       2088.796        2088.796               0                    0                     0                   2088.796                         0\n",
            "InitPhaseResNet                   bn1       CPU-0         83.126          83.126               0                    0                     0                     83.126                         0\n",
            "InitPhaseResNet                  relu       CPU-0         58.805          58.805               0                    0                     0                     58.805                         0\n",
            "InitPhaseResNet               maxpool       CPU-0         55.847          55.847               0                    0                     0                     55.847                         0\n",
            "InitPhaseResNet        layer1.0.conv1       CPU-0         75.014          75.014               0                    0                     0                     75.014                         0\n",
            "InitPhaseResNet          layer1.0.bn1       CPU-0         57.975          57.975               0                    0                     0                     57.975                         0\n",
            "InitPhaseResNet         layer1.0.relu       CPU-0         99.210          99.210               0                    0                     0                     99.210                         0\n",
            "InitPhaseResNet        layer1.0.conv2       CPU-0         52.238          52.238               0                    0                     0                     52.238                         0\n",
            "InitPhaseResNet          layer1.0.bn2       CPU-0         49.012          49.012               0                    0                     0                     49.012                         0\n",
            "InitPhaseResNet              layer1.0       CPU-0         27.355          27.355               0                    0                     0                     27.355                         0\n",
            "InitPhaseResNet        layer1.1.conv1       CPU-0         58.425          58.425               0                    0                     0                     58.425                         0\n",
            "InitPhaseResNet          layer1.1.bn1       CPU-0         63.750          63.750               0                    0                     0                     63.750                         0\n",
            "InitPhaseResNet         layer1.1.relu       CPU-0       4157.951        4157.951               0                    0                     0                   4157.951                         0\n",
            "InitPhaseResNet        layer1.1.conv2       CPU-0         54.480          54.480               0                    0                     0                     54.480                         0\n",
            "InitPhaseResNet          layer1.1.bn2       CPU-0         49.042          49.042               0                    0                     0                     49.042                         0\n",
            "InitPhaseResNet              layer1.1       CPU-0         49.236          49.236               0                    0                     0                     49.236                         0\n",
            "InitPhaseResNet        layer2.0.conv1       CPU-0         68.862          68.862               0                    0                     0                     68.862                         0\n",
            "InitPhaseResNet          layer2.0.bn1       CPU-0         47.607          47.607               0                    0                     0                     47.607                         0\n",
            "InitPhaseResNet         layer2.0.relu       CPU-0         97.914          97.914               0                    0                     0                     97.914                         0\n",
            "InitPhaseResNet        layer2.0.conv2       CPU-0         52.571          52.571               0                    0                     0                     52.571                         0\n",
            "InitPhaseResNet          layer2.0.bn2       CPU-0         49.792          49.792               0                    0                     0                     49.792                         0\n",
            "InitPhaseResNet layer2.0.downsample.0       CPU-0         83.139          83.139               0                    0                     0                     83.139                         0\n",
            "InitPhaseResNet layer2.0.downsample.1       CPU-0         48.184          48.184               0                    0                     0                     48.184                         0\n",
            "InitPhaseResNet              layer2.0       CPU-0         28.992          28.992               0                    0                     0                     28.992                         0\n",
            "InitPhaseResNet        layer2.1.conv1       CPU-0         54.371          54.371               0                    0                     0                     54.371                         0\n",
            "InitPhaseResNet          layer2.1.bn1       CPU-0         50.950          50.950               0                    0                     0                     50.950                         0\n",
            "InitPhaseResNet         layer2.1.relu       CPU-0         88.578          88.578               0                    0                     0                     88.578                         0\n",
            "InitPhaseResNet        layer2.1.conv2       CPU-0         55.656          55.656               0                    0                     0                     55.656                         0\n",
            "InitPhaseResNet          layer2.1.bn2       CPU-0         94.582          94.582               0                    0                     0                     94.582                         0\n",
            "InitPhaseResNet              layer2.1       CPU-0         27.991          27.991               0                    0                     0                     27.991                         0\n",
            "InitPhaseResNet        layer3.0.conv1       CPU-0         54.895          54.895               0                    0                     0                     54.895                         0\n",
            "InitPhaseResNet          layer3.0.bn1       CPU-0         50.765          50.765               0                    0                     0                     50.765                         0\n",
            "InitPhaseResNet         layer3.0.relu       CPU-0         81.965          81.965               0                    0                     0                     81.965                         0\n",
            "InitPhaseResNet        layer3.0.conv2       CPU-0         52.837          52.837               0                    0                     0                     52.837                         0\n",
            "InitPhaseResNet          layer3.0.bn2       CPU-0         48.158          48.158               0                    0                     0                     48.158                         0\n",
            "InitPhaseResNet layer3.0.downsample.0       CPU-0         50.928          50.928               0                    0                     0                     50.928                         0\n",
            "InitPhaseResNet layer3.0.downsample.1       CPU-0         47.797          47.797               0                    0                     0                     47.797                         0\n",
            "InitPhaseResNet              layer3.0       CPU-0         20.670          20.670               0                    0                     0                     20.670                         0\n",
            "InitPhaseResNet        layer3.1.conv1       CPU-0         55.771          55.771               0                    0                     0                     55.771                         0\n",
            "InitPhaseResNet          layer3.1.bn1       CPU-0         52.943          52.943               0                    0                     0                     52.943                         0\n",
            "InitPhaseResNet         layer3.1.relu       CPU-0         91.436          91.436               0                    0                     0                     91.436                         0\n",
            "InitPhaseResNet        layer3.1.conv2       CPU-0         57.969          57.969               0                    0                     0                     57.969                         0\n",
            "InitPhaseResNet          layer3.1.bn2       CPU-0         60.511          60.511               0                    0                     0                     60.511                         0\n",
            "InitPhaseResNet              layer3.1       CPU-0         28.825          28.825               0                    0                     0                     28.825                         0\n",
            "InitPhaseResNet        layer4.0.conv1       CPU-0         54.577          54.577               0                    0                     0                     54.577                         0\n",
            "InitPhaseResNet          layer4.0.bn1       CPU-0         76.478          76.478               0                    0                     0                     76.478                         0\n",
            "InitPhaseResNet         layer4.0.relu       CPU-0         70.158          70.158               0                    0                     0                     70.158                         0\n",
            "InitPhaseResNet        layer4.0.conv2       CPU-0         55.568          55.568               0                    0                     0                     55.568                         0\n",
            "InitPhaseResNet          layer4.0.bn2       CPU-0         46.357          46.357               0                    0                     0                     46.357                         0\n",
            "InitPhaseResNet layer4.0.downsample.0       CPU-0         58.145          58.145               0                    0                     0                     58.145                         0\n",
            "InitPhaseResNet layer4.0.downsample.1       CPU-0         52.474          52.474               0                    0                     0                     52.474                         0\n",
            "InitPhaseResNet              layer4.0       CPU-0         24.436          24.436               0                    0                     0                     24.436                         0\n",
            "InitPhaseResNet        layer4.1.conv1       CPU-0         52.144          52.144               0                    0                     0                     52.144                         0\n",
            "InitPhaseResNet          layer4.1.bn1       CPU-0         48.079          48.079               0                    0                     0                     48.079                         0\n",
            "InitPhaseResNet         layer4.1.relu       CPU-0         56.584          56.584               0                    0                     0                     56.584                         0\n",
            "InitPhaseResNet        layer4.1.conv2       CPU-0         77.889          77.889               0                    0                     0                     77.889                         0\n",
            "InitPhaseResNet          layer4.1.bn2       CPU-0         65.625          65.625               0                    0                     0                     65.625                         0\n",
            "InitPhaseResNet              layer4.1       CPU-0         24.102          24.102               0                    0                     0                     24.102                         0\n",
            "InitPhaseResNet               avgpool       CPU-0         59.804          59.804               0                    0                     0                     59.804                         0\n",
            "InitPhaseResNet                    fc       CPU-0         56.517          56.517               0                    0                     0                     56.517                         0\n",
            "   InitPhaseFFN          forward_pass       CPU-1        770.653        1157.195               0                 1032                     0                   1157.195                      1032\n",
            "   InitPhaseFFN                  misc       CPU-1        645.702        1032.244               0                 1032                     0                   1032.244                      1032\n",
            "   InitPhaseFFN                     0       CPU-1         78.163          78.163               0                    0                     0                     78.163                         0\n",
            "   InitPhaseFFN                     1       CPU-1         46.788          46.788               0                    0                     0                     46.788                         0\n",
            "InitPhaseResNet          forward_pass       CPU-1     141454.077      505601.966               0               602120                     0                 505601.966                    602120\n",
            "InitPhaseResNet                  misc       CPU-1     136840.575      500988.464               0               602120                     0                 500988.464                    602120\n",
            "InitPhaseResNet                 conv1       CPU-1         82.456          82.456               0                    0                     0                     82.456                         0\n",
            "InitPhaseResNet                   bn1       CPU-1         72.601          72.601               0                    0                     0                     72.601                         0\n",
            "InitPhaseResNet                  relu       CPU-1         98.892          98.892               0                    0                     0                     98.892                         0\n",
            "InitPhaseResNet               maxpool       CPU-1         73.883          73.883               0                    0                     0                     73.883                         0\n",
            "InitPhaseResNet        layer1.0.conv1       CPU-1         76.124          76.124               0                    0                     0                     76.124                         0\n",
            "InitPhaseResNet          layer1.0.bn1       CPU-1         71.938          71.938               0                    0                     0                     71.938                         0\n",
            "InitPhaseResNet         layer1.0.relu       CPU-1        119.023         119.023               0                    0                     0                    119.023                         0\n",
            "InitPhaseResNet        layer1.0.conv2       CPU-1         74.874          74.874               0                    0                     0                     74.874                         0\n",
            "InitPhaseResNet          layer1.0.bn2       CPU-1         69.915          69.915               0                    0                     0                     69.915                         0\n",
            "InitPhaseResNet              layer1.0       CPU-1         43.587          43.587               0                    0                     0                     43.587                         0\n",
            "InitPhaseResNet        layer1.1.conv1       CPU-1         82.901          82.901               0                    0                     0                     82.901                         0\n",
            "InitPhaseResNet          layer1.1.bn1       CPU-1         69.765          69.765               0                    0                     0                     69.765                         0\n",
            "InitPhaseResNet         layer1.1.relu       CPU-1        114.868         114.868               0                    0                     0                    114.868                         0\n",
            "InitPhaseResNet        layer1.1.conv2       CPU-1         72.313          72.313               0                    0                     0                     72.313                         0\n",
            "InitPhaseResNet          layer1.1.bn2       CPU-1         67.065          67.065               0                    0                     0                     67.065                         0\n",
            "InitPhaseResNet              layer1.1       CPU-1         47.401          47.401               0                    0                     0                     47.401                         0\n",
            "InitPhaseResNet        layer2.0.conv1       CPU-1         69.383          69.383               0                    0                     0                     69.383                         0\n",
            "InitPhaseResNet          layer2.0.bn1       CPU-1         66.704          66.704               0                    0                     0                     66.704                         0\n",
            "InitPhaseResNet         layer2.0.relu       CPU-1        117.571         117.571               0                    0                     0                    117.571                         0\n",
            "InitPhaseResNet        layer2.0.conv2       CPU-1         82.465          82.465               0                    0                     0                     82.465                         0\n",
            "InitPhaseResNet          layer2.0.bn2       CPU-1         73.035          73.035               0                    0                     0                     73.035                         0\n",
            "InitPhaseResNet layer2.0.downsample.0       CPU-1         83.413          83.413               0                    0                     0                     83.413                         0\n",
            "InitPhaseResNet layer2.0.downsample.1       CPU-1         68.682          68.682               0                    0                     0                     68.682                         0\n",
            "InitPhaseResNet              layer2.0       CPU-1         44.964          44.964               0                    0                     0                     44.964                         0\n",
            "InitPhaseResNet        layer2.1.conv1       CPU-1        117.081         117.081               0                    0                     0                    117.081                         0\n",
            "InitPhaseResNet          layer2.1.bn1       CPU-1         66.973          66.973               0                    0                     0                     66.973                         0\n",
            "InitPhaseResNet         layer2.1.relu       CPU-1        133.029         133.029               0                    0                     0                    133.029                         0\n",
            "InitPhaseResNet        layer2.1.conv2       CPU-1         70.711          70.711               0                    0                     0                     70.711                         0\n",
            "InitPhaseResNet          layer2.1.bn2       CPU-1         68.082          68.082               0                    0                     0                     68.082                         0\n",
            "InitPhaseResNet              layer2.1       CPU-1         58.661          58.661               0                    0                     0                     58.661                         0\n",
            "InitPhaseResNet        layer3.0.conv1       CPU-1         75.862          75.862               0                    0                     0                     75.862                         0\n",
            "InitPhaseResNet          layer3.0.bn1       CPU-1         69.373          69.373               0                    0                     0                     69.373                         0\n",
            "InitPhaseResNet         layer3.0.relu       CPU-1        113.423         113.423               0                    0                     0                    113.423                         0\n",
            "InitPhaseResNet        layer3.0.conv2       CPU-1         86.381          86.381               0                    0                     0                     86.381                         0\n",
            "InitPhaseResNet          layer3.0.bn2       CPU-1         65.288          65.288               0                    0                     0                     65.288                         0\n",
            "InitPhaseResNet layer3.0.downsample.0       CPU-1         70.080          70.080               0                    0                     0                     70.080                         0\n",
            "InitPhaseResNet layer3.0.downsample.1       CPU-1         62.178          62.178               0                    0                     0                     62.178                         0\n",
            "InitPhaseResNet              layer3.0       CPU-1         42.774          42.774               0                    0                     0                     42.774                         0\n",
            "InitPhaseResNet        layer3.1.conv1       CPU-1         73.148          73.148               0                    0                     0                     73.148                         0\n",
            "InitPhaseResNet          layer3.1.bn1       CPU-1         62.143          62.143               0                    0                     0                     62.143                         0\n",
            "InitPhaseResNet         layer3.1.relu       CPU-1        115.384         115.384               0                    0                     0                    115.384                         0\n",
            "InitPhaseResNet        layer3.1.conv2       CPU-1         82.545          82.545               0                    0                     0                     82.545                         0\n",
            "InitPhaseResNet          layer3.1.bn2       CPU-1         70.919          70.919               0                    0                     0                     70.919                         0\n",
            "InitPhaseResNet              layer3.1       CPU-1         50.824          50.824               0                    0                     0                     50.824                         0\n",
            "InitPhaseResNet        layer4.0.conv1       CPU-1         96.170          96.170               0                    0                     0                     96.170                         0\n",
            "InitPhaseResNet          layer4.0.bn1       CPU-1         66.435          66.435               0                    0                     0                     66.435                         0\n",
            "InitPhaseResNet         layer4.0.relu       CPU-1         93.960          93.960               0                    0                     0                     93.960                         0\n",
            "InitPhaseResNet        layer4.0.conv2       CPU-1         74.547          74.547               0                    0                     0                     74.547                         0\n",
            "InitPhaseResNet          layer4.0.bn2       CPU-1         58.643          58.643               0                    0                     0                     58.643                         0\n",
            "InitPhaseResNet layer4.0.downsample.0       CPU-1         71.138          71.138               0                    0                     0                     71.138                         0\n",
            "InitPhaseResNet layer4.0.downsample.1       CPU-1         58.364          58.364               0                    0                     0                     58.364                         0\n",
            "InitPhaseResNet              layer4.0       CPU-1         59.430          59.430               0                    0                     0                     59.430                         0\n",
            "InitPhaseResNet        layer4.1.conv1       CPU-1         79.705          79.705               0                    0                     0                     79.705                         0\n",
            "InitPhaseResNet          layer4.1.bn1       CPU-1         60.956          60.956               0                    0                     0                     60.956                         0\n",
            "InitPhaseResNet         layer4.1.relu       CPU-1        120.870         120.870               0                    0                     0                    120.870                         0\n",
            "InitPhaseResNet        layer4.1.conv2       CPU-1         97.767          97.767               0                    0                     0                     97.767                         0\n",
            "InitPhaseResNet          layer4.1.bn2       CPU-1         89.303          89.303               0                    0                     0                     89.303                         0\n",
            "InitPhaseResNet              layer4.1       CPU-1         45.755          45.755               0                    0                     0                     45.755                         0\n",
            "InitPhaseResNet               avgpool       CPU-1         68.485          68.485               0                    0                     0                     68.485                         0\n",
            "InitPhaseResNet                    fc       CPU-1         73.292          73.292               0                    0                     0                     73.292                         0\n",
            "   InitPhaseFFN          forward_pass GPU-0-CPU-0       4614.088       12912.621          56.128                 1032                     0                  12968.749                      1032\n",
            "   InitPhaseFFN                  misc GPU-0-CPU-0       4425.379       12723.912          56.128                 1032                     0                  12780.040                      1032\n",
            "   InitPhaseFFN                     0 GPU-0-CPU-0        102.924         102.924               0                    0                     0                    102.924                         0\n",
            "   InitPhaseFFN                     1 GPU-0-CPU-0         85.785          85.785               0                    0                     0                     85.785                         0\n",
            "InitPhaseResNet          forward_pass GPU-0-CPU-0      20282.566       53215.258        20011.62               602120                     0                  73226.878                    602120\n",
            "InitPhaseResNet                  misc GPU-0-CPU-0      16032.464       48965.156        20011.62               602120                     0                  68976.776                    602120\n",
            "InitPhaseResNet                 conv1 GPU-0-CPU-0        101.950         101.950               0                    0                     0                    101.950                         0\n",
            "InitPhaseResNet                   bn1 GPU-0-CPU-0         60.317          60.317               0                    0                     0                     60.317                         0\n",
            "InitPhaseResNet                  relu GPU-0-CPU-0         42.273          42.273               0                    0                     0                     42.273                         0\n",
            "InitPhaseResNet               maxpool GPU-0-CPU-0         42.748          42.748               0                    0                     0                     42.748                         0\n",
            "InitPhaseResNet        layer1.0.conv1 GPU-0-CPU-0         81.773          81.773               0                    0                     0                     81.773                         0\n",
            "InitPhaseResNet          layer1.0.bn1 GPU-0-CPU-0         60.170          60.170               0                    0                     0                     60.170                         0\n",
            "InitPhaseResNet         layer1.0.relu GPU-0-CPU-0         90.716          90.716               0                    0                     0                     90.716                         0\n",
            "InitPhaseResNet        layer1.0.conv2 GPU-0-CPU-0         91.725          91.725               0                    0                     0                     91.725                         0\n",
            "InitPhaseResNet          layer1.0.bn2 GPU-0-CPU-0         72.082          72.082               0                    0                     0                     72.082                         0\n",
            "InitPhaseResNet              layer1.0 GPU-0-CPU-0         36.727          36.727               0                    0                     0                     36.727                         0\n",
            "InitPhaseResNet        layer1.1.conv1 GPU-0-CPU-0         50.452          50.452               0                    0                     0                     50.452                         0\n",
            "InitPhaseResNet          layer1.1.bn1 GPU-0-CPU-0         57.804          57.804               0                    0                     0                     57.804                         0\n",
            "InitPhaseResNet         layer1.1.relu GPU-0-CPU-0         92.316          92.316               0                    0                     0                     92.316                         0\n",
            "InitPhaseResNet        layer1.1.conv2 GPU-0-CPU-0         53.731          53.731               0                    0                     0                     53.731                         0\n",
            "InitPhaseResNet          layer1.1.bn2 GPU-0-CPU-0         58.297          58.297               0                    0                     0                     58.297                         0\n",
            "InitPhaseResNet              layer1.1 GPU-0-CPU-0         51.891          51.891               0                    0                     0                     51.891                         0\n",
            "InitPhaseResNet        layer2.0.conv1 GPU-0-CPU-0         95.225          95.225               0                    0                     0                     95.225                         0\n",
            "InitPhaseResNet          layer2.0.bn1 GPU-0-CPU-0         62.326          62.326               0                    0                     0                     62.326                         0\n",
            "InitPhaseResNet         layer2.0.relu GPU-0-CPU-0         83.784          83.784               0                    0                     0                     83.784                         0\n",
            "InitPhaseResNet        layer2.0.conv2 GPU-0-CPU-0         68.253          68.253               0                    0                     0                     68.253                         0\n",
            "InitPhaseResNet          layer2.0.bn2 GPU-0-CPU-0         59.574          59.574               0                    0                     0                     59.574                         0\n",
            "InitPhaseResNet layer2.0.downsample.0 GPU-0-CPU-0         79.958          79.958               0                    0                     0                     79.958                         0\n",
            "InitPhaseResNet layer2.0.downsample.1 GPU-0-CPU-0         54.536          54.536               0                    0                     0                     54.536                         0\n",
            "InitPhaseResNet              layer2.0 GPU-0-CPU-0         82.359          82.359               0                    0                     0                     82.359                         0\n",
            "InitPhaseResNet        layer2.1.conv1 GPU-0-CPU-0         86.649          86.649               0                    0                     0                     86.649                         0\n",
            "InitPhaseResNet          layer2.1.bn1 GPU-0-CPU-0         61.772          61.772               0                    0                     0                     61.772                         0\n",
            "InitPhaseResNet         layer2.1.relu GPU-0-CPU-0        115.339         115.339               0                    0                     0                    115.339                         0\n",
            "InitPhaseResNet        layer2.1.conv2 GPU-0-CPU-0         60.411          60.411               0                    0                     0                     60.411                         0\n",
            "InitPhaseResNet          layer2.1.bn2 GPU-0-CPU-0         53.494          53.494               0                    0                     0                     53.494                         0\n",
            "InitPhaseResNet              layer2.1 GPU-0-CPU-0         45.855          45.855               0                    0                     0                     45.855                         0\n",
            "InitPhaseResNet        layer3.0.conv1 GPU-0-CPU-0         52.846          52.846               0                    0                     0                     52.846                         0\n",
            "InitPhaseResNet          layer3.0.bn1 GPU-0-CPU-0         92.959          92.959               0                    0                     0                     92.959                         0\n",
            "InitPhaseResNet         layer3.0.relu GPU-0-CPU-0        170.499         170.499               0                    0                     0                    170.499                         0\n",
            "InitPhaseResNet        layer3.0.conv2 GPU-0-CPU-0         59.876          59.876               0                    0                     0                     59.876                         0\n",
            "InitPhaseResNet          layer3.0.bn2 GPU-0-CPU-0         74.136          74.136               0                    0                     0                     74.136                         0\n",
            "InitPhaseResNet layer3.0.downsample.0 GPU-0-CPU-0         61.216          61.216               0                    0                     0                     61.216                         0\n",
            "InitPhaseResNet layer3.0.downsample.1 GPU-0-CPU-0         64.933          64.933               0                    0                     0                     64.933                         0\n",
            "InitPhaseResNet              layer3.0 GPU-0-CPU-0         56.924          56.924               0                    0                     0                     56.924                         0\n",
            "InitPhaseResNet        layer3.1.conv1 GPU-0-CPU-0         55.775          55.775               0                    0                     0                     55.775                         0\n",
            "InitPhaseResNet          layer3.1.bn1 GPU-0-CPU-0         93.766          93.766               0                    0                     0                     93.766                         0\n",
            "InitPhaseResNet         layer3.1.relu GPU-0-CPU-0        133.411         133.411               0                    0                     0                    133.411                         0\n",
            "InitPhaseResNet        layer3.1.conv2 GPU-0-CPU-0         85.134          85.134               0                    0                     0                     85.134                         0\n",
            "InitPhaseResNet          layer3.1.bn2 GPU-0-CPU-0         80.639          80.639               0                    0                     0                     80.639                         0\n",
            "InitPhaseResNet              layer3.1 GPU-0-CPU-0         34.898          34.898               0                    0                     0                     34.898                         0\n",
            "InitPhaseResNet        layer4.0.conv1 GPU-0-CPU-0         46.780          46.780               0                    0                     0                     46.780                         0\n",
            "InitPhaseResNet          layer4.0.bn1 GPU-0-CPU-0         82.794          82.794               0                    0                     0                     82.794                         0\n",
            "InitPhaseResNet         layer4.0.relu GPU-0-CPU-0        106.898         106.898               0                    0                     0                    106.898                         0\n",
            "InitPhaseResNet        layer4.0.conv2 GPU-0-CPU-0         53.907          53.907               0                    0                     0                     53.907                         0\n",
            "InitPhaseResNet          layer4.0.bn2 GPU-0-CPU-0         93.478          93.478               0                    0                     0                     93.478                         0\n",
            "InitPhaseResNet layer4.0.downsample.0 GPU-0-CPU-0         60.395          60.395               0                    0                     0                     60.395                         0\n",
            "InitPhaseResNet layer4.0.downsample.1 GPU-0-CPU-0         52.444          52.444               0                    0                     0                     52.444                         0\n",
            "InitPhaseResNet              layer4.0 GPU-0-CPU-0         35.943          35.943               0                    0                     0                     35.943                         0\n",
            "InitPhaseResNet        layer4.1.conv1 GPU-0-CPU-0         56.959          56.959               0                    0                     0                     56.959                         0\n",
            "InitPhaseResNet          layer4.1.bn1 GPU-0-CPU-0         68.789          68.789               0                    0                     0                     68.789                         0\n",
            "InitPhaseResNet         layer4.1.relu GPU-0-CPU-0         97.328          97.328               0                    0                     0                     97.328                         0\n",
            "InitPhaseResNet        layer4.1.conv2 GPU-0-CPU-0         62.241          62.241               0                    0                     0                     62.241                         0\n",
            "InitPhaseResNet          layer4.1.bn2 GPU-0-CPU-0        126.795         126.795               0                    0                     0                    126.795                         0\n",
            "InitPhaseResNet              layer4.1 GPU-0-CPU-0         35.431          35.431               0                    0                     0                     35.431                         0\n",
            "InitPhaseResNet               avgpool GPU-0-CPU-0         43.384          43.384               0                    0                     0                     43.384                         0\n",
            "InitPhaseResNet                    fc GPU-0-CPU-0         55.017          55.017               0                    0                     0                     55.017                         0\n",
            "   InitPhaseFFN          forward_pass GPU-0-CPU-1       4413.033       12368.926          56.192                 1032                     0                  12425.118                      1032\n",
            "   InitPhaseFFN                  misc GPU-0-CPU-1       4266.982       12222.875          56.192                 1032                     0                  12279.067                      1032\n",
            "   InitPhaseFFN                     0 GPU-0-CPU-1         79.443          79.443               0                    0                     0                     79.443                         0\n",
            "   InitPhaseFFN                     1 GPU-0-CPU-1         66.608          66.608               0                    0                     0                     66.608                         0\n",
            "InitPhaseResNet          forward_pass GPU-0-CPU-1      18473.147       45363.141        20023.03               602120                     0                  65386.171                    602120\n",
            "InitPhaseResNet                  misc GPU-0-CPU-1      13053.948       39943.942        20023.03               602120                     0                  59966.972                    602120\n",
            "InitPhaseResNet                 conv1 GPU-0-CPU-1         81.161          81.161               0                    0                     0                     81.161                         0\n",
            "InitPhaseResNet                   bn1 GPU-0-CPU-1         73.505          73.505               0                    0                     0                     73.505                         0\n",
            "InitPhaseResNet                  relu GPU-0-CPU-1         51.891          51.891               0                    0                     0                     51.891                         0\n",
            "InitPhaseResNet               maxpool GPU-0-CPU-1         83.646          83.646               0                    0                     0                     83.646                         0\n",
            "InitPhaseResNet        layer1.0.conv1 GPU-0-CPU-1         77.190          77.190               0                    0                     0                     77.190                         0\n",
            "InitPhaseResNet          layer1.0.bn1 GPU-0-CPU-1        133.614         133.614               0                    0                     0                    133.614                         0\n",
            "InitPhaseResNet         layer1.0.relu GPU-0-CPU-1        161.111         161.111               0                    0                     0                    161.111                         0\n",
            "InitPhaseResNet        layer1.0.conv2 GPU-0-CPU-1         76.012          76.012               0                    0                     0                     76.012                         0\n",
            "InitPhaseResNet          layer1.0.bn2 GPU-0-CPU-1         77.531          77.531               0                    0                     0                     77.531                         0\n",
            "InitPhaseResNet              layer1.0 GPU-0-CPU-1         46.681          46.681               0                    0                     0                     46.681                         0\n",
            "InitPhaseResNet        layer1.1.conv1 GPU-0-CPU-1         58.755          58.755               0                    0                     0                     58.755                         0\n",
            "InitPhaseResNet          layer1.1.bn1 GPU-0-CPU-1         69.149          69.149               0                    0                     0                     69.149                         0\n",
            "InitPhaseResNet         layer1.1.relu GPU-0-CPU-1        127.523         127.523               0                    0                     0                    127.523                         0\n",
            "InitPhaseResNet        layer1.1.conv2 GPU-0-CPU-1         89.655          89.655               0                    0                     0                     89.655                         0\n",
            "InitPhaseResNet          layer1.1.bn2 GPU-0-CPU-1        138.625         138.625               0                    0                     0                    138.625                         0\n",
            "InitPhaseResNet              layer1.1 GPU-0-CPU-1         45.979          45.979               0                    0                     0                     45.979                         0\n",
            "InitPhaseResNet        layer2.0.conv1 GPU-0-CPU-1         82.696          82.696               0                    0                     0                     82.696                         0\n",
            "InitPhaseResNet          layer2.0.bn1 GPU-0-CPU-1         78.598          78.598               0                    0                     0                     78.598                         0\n",
            "InitPhaseResNet         layer2.0.relu GPU-0-CPU-1        109.029         109.029               0                    0                     0                    109.029                         0\n",
            "InitPhaseResNet        layer2.0.conv2 GPU-0-CPU-1         67.295          67.295               0                    0                     0                     67.295                         0\n",
            "InitPhaseResNet          layer2.0.bn2 GPU-0-CPU-1        114.218         114.218               0                    0                     0                    114.218                         0\n",
            "InitPhaseResNet layer2.0.downsample.0 GPU-0-CPU-1        111.908         111.908               0                    0                     0                    111.908                         0\n",
            "InitPhaseResNet layer2.0.downsample.1 GPU-0-CPU-1         72.434          72.434               0                    0                     0                     72.434                         0\n",
            "InitPhaseResNet              layer2.0 GPU-0-CPU-1         72.595          72.595               0                    0                     0                     72.595                         0\n",
            "InitPhaseResNet        layer2.1.conv1 GPU-0-CPU-1        101.630         101.630               0                    0                     0                    101.630                         0\n",
            "InitPhaseResNet          layer2.1.bn1 GPU-0-CPU-1         64.502          64.502               0                    0                     0                     64.502                         0\n",
            "InitPhaseResNet         layer2.1.relu GPU-0-CPU-1        168.133         168.133               0                    0                     0                    168.133                         0\n",
            "InitPhaseResNet        layer2.1.conv2 GPU-0-CPU-1         60.223          60.223               0                    0                     0                     60.223                         0\n",
            "InitPhaseResNet          layer2.1.bn2 GPU-0-CPU-1        122.923         122.923               0                    0                     0                    122.923                         0\n",
            "InitPhaseResNet              layer2.1 GPU-0-CPU-1         53.486          53.486               0                    0                     0                     53.486                         0\n",
            "InitPhaseResNet        layer3.0.conv1 GPU-0-CPU-1        120.153         120.153               0                    0                     0                    120.153                         0\n",
            "InitPhaseResNet          layer3.0.bn1 GPU-0-CPU-1         72.190          72.190               0                    0                     0                     72.190                         0\n",
            "InitPhaseResNet         layer3.0.relu GPU-0-CPU-1        135.497         135.497               0                    0                     0                    135.497                         0\n",
            "InitPhaseResNet        layer3.0.conv2 GPU-0-CPU-1        107.546         107.546               0                    0                     0                    107.546                         0\n",
            "InitPhaseResNet          layer3.0.bn2 GPU-0-CPU-1        121.970         121.970               0                    0                     0                    121.970                         0\n",
            "InitPhaseResNet layer3.0.downsample.0 GPU-0-CPU-1        104.010         104.010               0                    0                     0                    104.010                         0\n",
            "InitPhaseResNet layer3.0.downsample.1 GPU-0-CPU-1         70.393          70.393               0                    0                     0                     70.393                         0\n",
            "InitPhaseResNet              layer3.0 GPU-0-CPU-1         55.997          55.997               0                    0                     0                     55.997                         0\n",
            "InitPhaseResNet        layer3.1.conv1 GPU-0-CPU-1         62.411          62.411               0                    0                     0                     62.411                         0\n",
            "InitPhaseResNet          layer3.1.bn1 GPU-0-CPU-1         62.183          62.183               0                    0                     0                     62.183                         0\n",
            "InitPhaseResNet         layer3.1.relu GPU-0-CPU-1        156.443         156.443               0                    0                     0                    156.443                         0\n",
            "InitPhaseResNet        layer3.1.conv2 GPU-0-CPU-1         68.732          68.732               0                    0                     0                     68.732                         0\n",
            "InitPhaseResNet          layer3.1.bn2 GPU-0-CPU-1         91.165          91.165               0                    0                     0                     91.165                         0\n",
            "InitPhaseResNet              layer3.1 GPU-0-CPU-1        105.121         105.121               0                    0                     0                    105.121                         0\n",
            "InitPhaseResNet        layer4.0.conv1 GPU-0-CPU-1         74.951          74.951               0                    0                     0                     74.951                         0\n",
            "InitPhaseResNet          layer4.0.bn1 GPU-0-CPU-1         62.994          62.994               0                    0                     0                     62.994                         0\n",
            "InitPhaseResNet         layer4.0.relu GPU-0-CPU-1        201.421         201.421               0                    0                     0                    201.421                         0\n",
            "InitPhaseResNet        layer4.0.conv2 GPU-0-CPU-1         71.081          71.081               0                    0                     0                     71.081                         0\n",
            "InitPhaseResNet          layer4.0.bn2 GPU-0-CPU-1        103.048         103.048               0                    0                     0                    103.048                         0\n",
            "InitPhaseResNet layer4.0.downsample.0 GPU-0-CPU-1         60.444          60.444               0                    0                     0                     60.444                         0\n",
            "InitPhaseResNet layer4.0.downsample.1 GPU-0-CPU-1        113.774         113.774               0                    0                     0                    113.774                         0\n",
            "InitPhaseResNet              layer4.0 GPU-0-CPU-1         45.988          45.988               0                    0                     0                     45.988                         0\n",
            "InitPhaseResNet        layer4.1.conv1 GPU-0-CPU-1         61.265          61.265               0                    0                     0                     61.265                         0\n",
            "InitPhaseResNet          layer4.1.bn1 GPU-0-CPU-1        104.872         104.872               0                    0                     0                    104.872                         0\n",
            "InitPhaseResNet         layer4.1.relu GPU-0-CPU-1        129.721         129.721               0                    0                     0                    129.721                         0\n",
            "InitPhaseResNet        layer4.1.conv2 GPU-0-CPU-1         57.726          57.726               0                    0                     0                     57.726                         0\n",
            "InitPhaseResNet          layer4.1.bn2 GPU-0-CPU-1         66.782          66.782               0                    0                     0                     66.782                         0\n",
            "InitPhaseResNet              layer4.1 GPU-0-CPU-1         84.804          84.804               0                    0                     0                     84.804                         0\n",
            "InitPhaseResNet               avgpool GPU-0-CPU-1        107.661         107.661               0                    0                     0                    107.661                         0\n",
            "InitPhaseResNet                    fc GPU-0-CPU-1         67.158          67.158               0                    0                     0                     67.158                         0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "wTvMnO_Uz3ek"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}