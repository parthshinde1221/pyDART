{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# PyDart Library – Final Evaluation and Makespan Optimization\n",
        "\n",
        "## Overview\n",
        "\n",
        "This iteration represents the **most recent** version of PyDart, aligning closely with the final code used for **PyDart Eval testing**. The primary focus in this version was on **makespan-based scheduling**, which significantly improved results.\n",
        "\n",
        "## Main Contributions\n",
        "\n",
        "1. **Utilizing Makespan as a Load Metric**  \n",
        "   - Defined **makespan** as the total execution time of the system or specific layers.  \n",
        "   - Used makespan to **optimize workload distribution**, leading to improved performance.  \n",
        "\n",
        "2. **Refining the Final Implementation**  \n",
        "   - This version and the final work differ only in:  \n",
        "     - **Aesthetic and clean code improvements.**  \n",
        "     - **Commenting and naming adjustments.**  \n",
        "\n",
        "3. **Experimenting with Blocking Stages and Layers**  \n",
        "   - Analyzed how dependent layers impact overall makespan.  \n",
        "   - Observed the effect of **stage blocking** on execution efficiency.  \n",
        "   - Gained insights into interdependencies within DAG-based execution.  \n",
        "\n",
        "## Key Insights and Next Steps  \n",
        "\n",
        "- Makespan-based scheduling **yielded strong results**, validating its effectiveness in PyDart.  \n",
        "- Future refinements could explore **dynamic load balancing** to further optimize performance.  \n",
        "\n",
        "---\n",
        "\n",
        "**Note**: This iteration is nearly identical to the final version, with only minor refinements. The focus was on **cleaning up the code** and improving clarity in comments and naming conventions.\n"
      ],
      "metadata": {
        "id": "QXLYzAdbwXSo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import io\n",
        "import copy\n",
        "import time\n",
        "import queue\n",
        "import threading\n",
        "import torch\n",
        "import torch.fx as fx\n",
        "import torch.nn as nn\n",
        "import torch.profiler\n",
        "import torch.cuda\n",
        "import networkx as nx\n",
        "import pandas as pd\n",
        "from torch.fx import Node as FxNode\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from torchvision import models\n",
        "from typing import Callable, Any, List, Dict, Optional, Set, Tuple\n",
        "\n",
        "# ----------------------------------------------------------------------\n",
        "# Node Class: Manages task execution on a particular CPU/GPU node.\n",
        "# ----------------------------------------------------------------------\n",
        "class Node:\n",
        "    def __init__(self, node_id: str, cpus=None, gpu=None):\n",
        "        self._node_id = node_id\n",
        "        self._cpus = tuple(cpus or [])\n",
        "        self._gpu = gpu\n",
        "        self._original_affinity = os.sched_getaffinity(0)\n",
        "        self._task_queue = queue.Queue()\n",
        "        self._stop_signal = False\n",
        "        self._worker_thread = threading.Thread(target=self._worker_loop, daemon=True)\n",
        "        self._worker_thread.start()\n",
        "        self.current_load = 0.0\n",
        "        self.assigned_stages = []\n",
        "\n",
        "    @property\n",
        "    def node_id(self):\n",
        "        return self._node_id\n",
        "\n",
        "    @property\n",
        "    def cpus(self):\n",
        "        return self._cpus\n",
        "\n",
        "    @property\n",
        "    def gpu(self):\n",
        "        return self._gpu\n",
        "\n",
        "    def assign_task(self, func: Callable, *args, **kwargs) -> queue.Queue:\n",
        "        result_queue = queue.Queue(maxsize=1)\n",
        "        self._task_queue.put((func, args, kwargs, result_queue))\n",
        "        return result_queue\n",
        "\n",
        "    def stop(self):\n",
        "        self._stop_signal = True\n",
        "        self._task_queue.put(None)\n",
        "        self._worker_thread.join()\n",
        "\n",
        "    def _worker_loop(self):\n",
        "        while not self._stop_signal:\n",
        "            item = self._task_queue.get()\n",
        "            if item is None:\n",
        "                break\n",
        "            func, args, kwargs, result_queue = item\n",
        "            try:\n",
        "                self._set_context()\n",
        "                result = func(*args, **kwargs)\n",
        "            except Exception as e:\n",
        "                result = e\n",
        "            finally:\n",
        "                self._reset_context()\n",
        "            result_queue.put(result)\n",
        "\n",
        "    def _set_context(self):\n",
        "        if self._cpus:\n",
        "            os.sched_setaffinity(0, self._cpus)\n",
        "        if self._gpu is not None and torch.cuda.is_available():\n",
        "            torch.cuda.set_device(self._gpu)\n",
        "            torch.cuda.synchronize(self._gpu)\n",
        "\n",
        "    def _reset_context(self):\n",
        "        os.sched_setaffinity(0, self._original_affinity)\n",
        "        if self._gpu is not None and torch.cuda.is_available():\n",
        "            torch.cuda.synchronize(self._gpu)\n",
        "\n",
        "    @staticmethod\n",
        "    def discover_nodes(disjoint: bool = True) -> List['Node']:\n",
        "        nodes = []\n",
        "        num_cpus = os.cpu_count() or 1\n",
        "        ngpus = torch.cuda.device_count()\n",
        "        if not disjoint:\n",
        "            for core_id in range(num_cpus):\n",
        "                nodes.append(Node(node_id=f\"CPU-{core_id}\", cpus=[core_id]))\n",
        "            for g in range(ngpus):\n",
        "                for core_id in range(num_cpus):\n",
        "                    nodes.append(Node(node_id=f\"GPU-{g}-CPU-{core_id}\", cpus=[core_id], gpu=g))\n",
        "        else:\n",
        "            cpu_nodes = [Node(node_id=f\"CPU-{i}\", cpus=[i]) for i in range(num_cpus)]\n",
        "            gpu_nodes = []\n",
        "            for g in range(ngpus):\n",
        "                if cpu_nodes:\n",
        "                    cpu_node = cpu_nodes.pop()\n",
        "                    gpu_nodes.append(Node(node_id=f\"GPU-{g}-CPU-{cpu_node.cpus[0]}\", cpus=[cpu_node.cpus[0]], gpu=g))\n",
        "                else:\n",
        "                    gpu_nodes.append(Node(node_id=f\"GPU-{g}\", cpus=[], gpu=g))\n",
        "            # To prefer GPU nodes for CUDA operations, we can return GPU nodes first.\n",
        "            nodes = gpu_nodes + cpu_nodes\n",
        "        return nodes\n",
        "\n",
        "    def __repr__(self):\n",
        "        return f\"Node({self._node_id}, cpus={self._cpus}, gpu={self._gpu})\"\n"
      ],
      "metadata": {
        "id": "IMxU62UiMKmO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TjB2dh08nef7"
      },
      "outputs": [],
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"Untitled26.ipynb\n",
        "\n",
        "Automatically generated by Colab.\n",
        "\n",
        "Original file is located at\n",
        "    https://colab.research.google.com/drive/1aBGeWbMHUGe0Y_Fpwx66p5iWLvVqZkDa\n",
        "\"\"\"\n",
        "\n",
        "import os\n",
        "import io\n",
        "import copy\n",
        "import time\n",
        "import queue\n",
        "import threading\n",
        "import torch\n",
        "import torch.fx as fx\n",
        "import torch.nn as nn\n",
        "import torch.profiler\n",
        "import torch.cuda\n",
        "import networkx as nx\n",
        "import pandas as pd\n",
        "from torch.fx import Node as FxNode\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from torchvision import models\n",
        "from typing import Callable, Any, List, Dict, Optional, Set, Tuple\n",
        "\n",
        "# ----------------------------------------------------------------------\n",
        "# Node Class: Manages task execution on a particular CPU/GPU node.\n",
        "# ----------------------------------------------------------------------\n",
        "class Node:\n",
        "    def __init__(self, node_id: str, cpus=None, gpu=None):\n",
        "        self._node_id = node_id\n",
        "        self._cpus = tuple(cpus or [])\n",
        "        self._gpu = gpu\n",
        "        self._original_affinity = os.sched_getaffinity(0)\n",
        "        self._task_queue = queue.Queue()\n",
        "        self._stop_signal = False\n",
        "        self._worker_thread = threading.Thread(target=self._worker_loop, daemon=True)\n",
        "        self._worker_thread.start()\n",
        "        self.current_load = 0.0\n",
        "        self.assigned_stages = []\n",
        "\n",
        "    @property\n",
        "    def node_id(self):\n",
        "        return self._node_id\n",
        "\n",
        "    @property\n",
        "    def cpus(self):\n",
        "        return self._cpus\n",
        "\n",
        "    @property\n",
        "    def gpu(self):\n",
        "        return self._gpu\n",
        "\n",
        "    def assign_task(self, func: Callable, *args, **kwargs) -> queue.Queue:\n",
        "        result_queue = queue.Queue(maxsize=1)\n",
        "        self._task_queue.put((func, args, kwargs, result_queue))\n",
        "        return result_queue\n",
        "\n",
        "    def stop(self):\n",
        "        self._stop_signal = True\n",
        "        self._task_queue.put(None)\n",
        "        self._worker_thread.join()\n",
        "\n",
        "    def _worker_loop(self):\n",
        "        while not self._stop_signal:\n",
        "            item = self._task_queue.get()\n",
        "            if item is None:\n",
        "                break\n",
        "            func, args, kwargs, result_queue = item\n",
        "            try:\n",
        "                self._set_context()\n",
        "                result = func(*args, **kwargs)\n",
        "            except Exception as e:\n",
        "                result = e\n",
        "            finally:\n",
        "                self._reset_context()\n",
        "            result_queue.put(result)\n",
        "\n",
        "    def _set_context(self):\n",
        "        if self._cpus:\n",
        "            os.sched_setaffinity(0, self._cpus)\n",
        "        if self._gpu is not None and torch.cuda.is_available():\n",
        "            torch.cuda.set_device(self._gpu)\n",
        "            torch.cuda.synchronize(self._gpu)\n",
        "\n",
        "    def _reset_context(self):\n",
        "        os.sched_setaffinity(0, self._original_affinity)\n",
        "        if self._gpu is not None and torch.cuda.is_available():\n",
        "            torch.cuda.synchronize(self._gpu)\n",
        "\n",
        "    @staticmethod\n",
        "    def discover_nodes(disjoint: bool = True) -> List['Node']:\n",
        "        nodes = []\n",
        "        num_cpus = os.cpu_count() or 1\n",
        "        ngpus = torch.cuda.device_count()\n",
        "        if not disjoint:\n",
        "            for core_id in range(num_cpus):\n",
        "                nodes.append(Node(node_id=f\"CPU-{core_id}\", cpus=[core_id]))\n",
        "            for g in range(ngpus):\n",
        "                for core_id in range(num_cpus):\n",
        "                    nodes.append(Node(node_id=f\"GPU-{g}-CPU-{core_id}\", cpus=[core_id], gpu=g))\n",
        "        else:\n",
        "            cpu_nodes = [Node(node_id=f\"CPU-{i}\", cpus=[i]) for i in range(num_cpus)]\n",
        "            gpu_nodes = []\n",
        "            for g in range(ngpus):\n",
        "                if cpu_nodes:\n",
        "                    cpu_node = cpu_nodes.pop()\n",
        "                    gpu_nodes.append(Node(node_id=f\"GPU-{g}-CPU-{cpu_node.cpus[0]}\", cpus=[cpu_node.cpus[0]], gpu=g))\n",
        "                else:\n",
        "                    gpu_nodes.append(Node(node_id=f\"GPU-{g}\", cpus=[], gpu=g))\n",
        "            # To prefer GPU nodes for CUDA operations, we can return GPU nodes first.\n",
        "            nodes = gpu_nodes + cpu_nodes\n",
        "        return nodes\n",
        "\n",
        "    def __repr__(self):\n",
        "        return f\"Node({self._node_id}, cpus={self._cpus}, gpu={self._gpu})\"\n",
        "\n",
        "\n",
        "# ----------------------------------------------------------------------\n",
        "# Utility Functions\n",
        "# ----------------------------------------------------------------------\n",
        "def resolve_arg(arg: Any, node_outputs: Dict[str, torch.Tensor]) -> Any:\n",
        "    if isinstance(arg, FxNode):\n",
        "        return node_outputs.get(arg.name, arg)\n",
        "    elif isinstance(arg, (list, tuple)):\n",
        "        return type(arg)(resolve_arg(a, node_outputs) for a in arg)\n",
        "    elif isinstance(arg, dict):\n",
        "        return {k: resolve_arg(v, node_outputs) for k, v in arg.items()}\n",
        "    else:\n",
        "        return arg\n",
        "\n",
        "def move_tensor_to_device(obj, device):\n",
        "    if isinstance(obj, torch.Tensor):\n",
        "        return obj.to(device,non_blocking=True) if obj.device != device else obj\n",
        "    elif isinstance(obj, (list, tuple)):\n",
        "        return type(obj)(move_tensor_to_device(x, device) for x in obj)\n",
        "    elif isinstance(obj, dict):\n",
        "        return {k: move_tensor_to_device(v, device) for k, v in obj.items()}\n",
        "    else:\n",
        "        return obj\n",
        "\n",
        "# ----------------------------------------------------------------------\n",
        "# Function to measure maximum transfer penalty across all node pairs.\n",
        "# This moves a sample tensor from each node's device to every other node's device.\n",
        "# ----------------------------------------------------------------------\n",
        "def measure_max_transfer_penalty(available_nodes: List[Node], sample_tensor: torch.Tensor) -> float:\n",
        "    max_time = 0.0\n",
        "    for src in available_nodes:\n",
        "        # Determine source device.\n",
        "        src_device = torch.device(f\"cuda:{src.gpu}\") if src.gpu is not None and torch.cuda.is_available() else torch.device(\"cpu\")\n",
        "        tensor_on_src = sample_tensor.to(src_device)\n",
        "        for dst in available_nodes:\n",
        "            dst_device = torch.device(f\"cuda:{dst.gpu}\") if dst.gpu is not None and torch.cuda.is_available() else torch.device(\"cpu\")\n",
        "            start = time.time()\n",
        "            _ = tensor_on_src.to(dst_device)\n",
        "            if dst_device.type == 'cuda':\n",
        "                torch.cuda.synchronize(dst_device)\n",
        "            elapsed = time.time() - start\n",
        "            if elapsed > max_time:\n",
        "                max_time = elapsed\n",
        "    return max_time\n",
        "\n",
        "# ----------------------------------------------------------------------\n",
        "# Profiler Class\n",
        "# ----------------------------------------------------------------------\n",
        "class Profiler:\n",
        "    def __init__(self, mode: str, profile_db_path='profiling_results.csv', log_dir='logs'):\n",
        "        assert mode in ['init', 'runtime'], \"Mode must be 'init' or 'runtime'\"\n",
        "        self.mode = mode\n",
        "        self.profile_db_path = profile_db_path\n",
        "        self.log_dir = log_dir\n",
        "        os.makedirs(self.log_dir, exist_ok=True)\n",
        "        self.columns = ['Task_ID', 'Model', 'Layer', 'Compute',\n",
        "                        'Self CPU (us)', 'CPU Total (us)', 'CUDA Total (us)',\n",
        "                        'Self CPU Mem (bytes)', 'Self CUDA Mem (bytes)',\n",
        "                        'Total Execution Time (us)', 'Total Memory Used (bytes)']\n",
        "        if os.path.exists(self.profile_db_path):\n",
        "            self.profile_db = pd.read_csv(self.profile_db_path)\n",
        "        else:\n",
        "            self.profile_db = pd.DataFrame(columns=self.columns)\n",
        "        self.runtime_csv = os.path.join(self.log_dir, 'runtime_results.csv')\n",
        "        if not os.path.exists(self.runtime_csv):\n",
        "            pd.DataFrame(columns=['Task_ID', 'Model', 'Layer', 'Compute', 'Execution Time (us)']).to_csv(self.runtime_csv, index=False)\n",
        "        self.observation_window = 0.0\n",
        "        self.profile_cache: Dict[Tuple[str, str], pd.DataFrame] = {}\n",
        "\n",
        "\n",
        "\n",
        "    def profile_model(self,\n",
        "                      model: nn.Module,\n",
        "                      input_data: Any,\n",
        "                      node: \"Node\",  # ← Instead of passing just node_id, pass the full Node object\n",
        "                      task_id: str,\n",
        "                      warmup_iters: int = 3,\n",
        "                      profile_iters: int = 5):\n",
        "        \"\"\"\n",
        "        Profiles the given model on the specified node, using node-specific CPU affinity\n",
        "        and (optional) GPU selection to ensure the results reflect that node's actual\n",
        "        performance characteristics.\n",
        "\n",
        "        Args:\n",
        "            model (nn.Module): The PyTorch model to profile.\n",
        "            input_data (Any): The input data to feed into the model.\n",
        "            node (Node): The Node object representing the CPU/GPU core(s) for this profile run.\n",
        "            task_id (str): Identifier for the profiling task.\n",
        "            warmup_iters (int): Number of warmup iterations (default=3).\n",
        "            profile_iters (int): Number of iterations for the active profiling period (default=5).\n",
        "        \"\"\"\n",
        "\n",
        "        # 1) Build a key for caching. We’ll reuse the data if we've already profiled this\n",
        "        #    (model_class, node.node_id) pair. Otherwise, we do a new profile.\n",
        "        cache_key = (model.__class__.__name__, node.node_id)\n",
        "        if cache_key in self.profile_cache:\n",
        "            # We have cached results for this model+node. Duplicate them for the new task_id.\n",
        "            cached_data = self.profile_cache[cache_key].copy()\n",
        "            cached_data['Task_ID'] = task_id\n",
        "            self.profile_db = pd.concat([self.profile_db, cached_data], ignore_index=True)\n",
        "            print(f\"[Profiler] Reused cached profiling data for {model.__class__.__name__} on {node.node_id}.\")\n",
        "            return\n",
        "\n",
        "        # 2) Save the current CPU affinity (so we can restore it later).\n",
        "        old_affinity = os.sched_getaffinity(0)\n",
        "\n",
        "        try:\n",
        "            # 3) Pin the current thread to the Node's CPU(s).\n",
        "            if node.cpus:\n",
        "                os.sched_setaffinity(0, node.cpus)\n",
        "\n",
        "            # 4) Select the correct GPU device if the node has one and CUDA is available.\n",
        "            if node.gpu is not None and torch.cuda.is_available():\n",
        "                torch.cuda.set_device(node.gpu)\n",
        "                device = torch.device(f\"cuda:{node.gpu}\")\n",
        "            else:\n",
        "                device = torch.device(\"cpu\")\n",
        "\n",
        "            # 5) Clone & instrument the model as usual.\n",
        "            model_copy = self._clone_model_safely(model)\n",
        "            instrumented_model = self._trace_and_instrument_model(model_copy)\n",
        "            instrumented_model.to(device)\n",
        "            instrumented_model.eval()\n",
        "\n",
        "            # 6) Warmup runs (not recorded).\n",
        "            with torch.no_grad():\n",
        "                for _ in range(warmup_iters):\n",
        "                    _ = instrumented_model(input_data.to(device))\n",
        "\n",
        "            print(f\"[Profiler] Starting profiling for Task '{task_id}' on {node.node_id} (device={device}).\")\n",
        "\n",
        "            # 7) Actual profiling\n",
        "            with torch.profiler.profile(\n",
        "                activities=[torch.profiler.ProfilerActivity.CPU, torch.profiler.ProfilerActivity.CUDA],\n",
        "                schedule=torch.profiler.schedule(wait=1, warmup=1, active=profile_iters),\n",
        "                on_trace_ready=lambda prof: self._trace_handler(prof, task_id, model.__class__.__name__, node.node_id),\n",
        "                record_shapes=True,\n",
        "                profile_memory=True,\n",
        "                with_stack=True\n",
        "            ) as prof:\n",
        "                for _ in range(profile_iters):\n",
        "                    with torch.no_grad():\n",
        "                        _ = instrumented_model(input_data.to(device))\n",
        "\n",
        "                    if device.type == 'cuda':\n",
        "                        torch.cuda.synchronize(device)\n",
        "\n",
        "                    prof.step()\n",
        "\n",
        "            # Optional: tiny sleep to avoid immediate re-profiling collisions\n",
        "            time.sleep(0.0001)\n",
        "\n",
        "            # 8) Cache the newly added rows for future tasks with the same (model, node).\n",
        "            new_rows = self.profile_db[self.profile_db['Task_ID'] == task_id].copy()\n",
        "            self.profile_cache[cache_key] = new_rows\n",
        "\n",
        "            # 9) Save the updated profile DB.\n",
        "            self.profile_db.to_csv(self.profile_db_path, index=False)\n",
        "            print(f\"[Profiler] Profiling complete. Data saved to {self.profile_db_path}.\")\n",
        "\n",
        "        finally:\n",
        "            # Always restore the old CPU affinity, regardless of success/failure\n",
        "            os.sched_setaffinity(0, old_affinity)\n",
        "            # Optionally reset CUDA device if desired (e.g. to device 0):\n",
        "            # torch.cuda.set_device(0)\n",
        "\n",
        "\n",
        "    def _clone_model_safely(self, model: nn.Module) -> nn.Module:\n",
        "        try:\n",
        "            return copy.deepcopy(model)\n",
        "        except Exception as e:\n",
        "            print(f\"[Profiler] deepcopy failed: {e}. Falling back to torch.save/load.\")\n",
        "            buffer = io.BytesIO()\n",
        "            torch.save(model, buffer)\n",
        "            buffer.seek(0)\n",
        "            return torch.load(buffer)\n",
        "\n",
        "    def _trace_and_instrument_model(self, model: nn.Module) -> fx.GraphModule:\n",
        "        tracer = fx.Tracer()\n",
        "        graph = tracer.trace(model)\n",
        "        graph_module = fx.GraphModule(model, graph)\n",
        "        profiler_attr_prefix = \"_profiler_wrapped_\"\n",
        "        for node in list(graph.nodes):\n",
        "            node_name = node.name\n",
        "            if node.op == 'call_function':\n",
        "                func = node.target\n",
        "                wrapped_func_name = f\"{profiler_attr_prefix}{node_name}_{id(func)}\"\n",
        "                def make_wrapped_func(original_func, profile_name):\n",
        "                    def wrapped(*args, **kwargs):\n",
        "                        with torch.profiler.record_function(profile_name):\n",
        "                            return original_func(*args, **kwargs)\n",
        "                    return wrapped\n",
        "                wrapped_func = make_wrapped_func(func, node_name)\n",
        "                setattr(model, wrapped_func_name, wrapped_func)\n",
        "                node.target = getattr(model, wrapped_func_name)\n",
        "            elif node.op == 'call_module':\n",
        "                submodule = dict(model.named_modules())[node.target]\n",
        "                func = submodule.forward\n",
        "                wrapped_func_name = f\"{profiler_attr_prefix}{node_name}_{id(func)}\"\n",
        "                def make_wrapped_forward(original_forward, profile_name):\n",
        "                    def wrapped_forward(*args, **kwargs):\n",
        "                        with torch.profiler.record_function(profile_name):\n",
        "                            return original_forward(*args, **kwargs)\n",
        "                    return wrapped_forward\n",
        "                wrapped_forward = make_wrapped_forward(func, node_name)\n",
        "                setattr(submodule, wrapped_func_name, wrapped_forward)\n",
        "                submodule.forward = getattr(submodule, wrapped_func_name)\n",
        "            elif node.op == 'call_method':\n",
        "                method_name = node.target\n",
        "                obj = node.args[0]\n",
        "                original_method = getattr(obj, method_name, None)\n",
        "                if original_method is None:\n",
        "                    continue\n",
        "                wrapped_method_name = f\"{profiler_attr_prefix}{node_name}_{id(original_method)}\"\n",
        "                def make_wrapped_method(orig_meth, profile_name):\n",
        "                    def wrapped_method(*args, **kwargs):\n",
        "                        with torch.profiler.record_function(profile_name):\n",
        "                            return orig_meth(*args, **kwargs)\n",
        "                    return wrapped_method\n",
        "                wrapped_method = make_wrapped_method(original_method, node_name)\n",
        "                setattr(obj, wrapped_method_name, wrapped_method)\n",
        "            # Placeholders do not need instrumentation.\n",
        "        graph_module.recompile()\n",
        "        return graph_module\n",
        "\n",
        "    def _trace_handler(self, prof, task_id: str, model_name: str, node_id: str):\n",
        "        self._process_profiler_data(prof, task_id, model_name, node_id)\n",
        "\n",
        "    def _process_profiler_data(self, profiler, task_id: str, model_name: str, node_id: str):\n",
        "        aggregated = {}\n",
        "        forward_pass = {\n",
        "            'Task_ID': task_id,\n",
        "            'Model': model_name,\n",
        "            'Layer': 'forward_pass',\n",
        "            'Compute': node_id,\n",
        "            'Self CPU (us)': 0.0,\n",
        "            'CPU Total (us)': 0.0,\n",
        "            'CUDA Total (us)': 0.0,\n",
        "            'Self CPU Mem (bytes)': 0,\n",
        "            'Self CUDA Mem (bytes)': 0,\n",
        "            'Total Execution Time (us)': 0.0,\n",
        "            'Total Memory Used (bytes)': 0\n",
        "        }\n",
        "        events = profiler.key_averages()\n",
        "        for evt in events:\n",
        "            layer_name = evt.key\n",
        "            if layer_name.startswith(\"aten::\"):\n",
        "                continue\n",
        "            forward_pass['Self CPU (us)'] += evt.self_cpu_time_total\n",
        "            forward_pass['CPU Total (us)'] += evt.cpu_time_total\n",
        "            forward_pass['CUDA Total (us)'] += getattr(evt, 'cuda_time_total', 0.0)\n",
        "            forward_pass['Self CPU Mem (bytes)'] += getattr(evt, 'self_cpu_memory_usage', 0)\n",
        "            forward_pass['Self CUDA Mem (bytes)'] += getattr(evt, 'self_cuda_memory_usage', 0)\n",
        "            forward_pass['Total Execution Time (us)'] += evt.cpu_time_total + getattr(evt, 'cuda_time_total', 0.0)\n",
        "            forward_pass['Total Memory Used (bytes)'] += getattr(evt, 'self_cpu_memory_usage', 0) + getattr(evt, 'self_cuda_memory_usage', 0)\n",
        "            if layer_name not in aggregated:\n",
        "                aggregated[layer_name] = {\n",
        "                    'Task_ID': task_id,\n",
        "                    'Model': model_name,\n",
        "                    'Layer': layer_name,\n",
        "                    'Compute': node_id,\n",
        "                    'Self CPU (us)': 0.0,\n",
        "                    'CPU Total (us)': 0.0,\n",
        "                    'CUDA Total (us)': 0.0,\n",
        "                    'Self CPU Mem (bytes)': 0,\n",
        "                    'Self CUDA Mem (bytes)': 0,\n",
        "                    'Total Execution Time (us)': 0.0,\n",
        "                    'Total Memory Used (bytes)': 0\n",
        "                }\n",
        "            aggregated[layer_name]['Self CPU (us)'] += evt.self_cpu_time_total\n",
        "            aggregated[layer_name]['CPU Total (us)'] += evt.cpu_time_total\n",
        "            aggregated[layer_name]['CUDA Total (us)'] += getattr(evt, 'cuda_time_total', 0.0)\n",
        "            aggregated[layer_name]['Self CPU Mem (bytes)'] += getattr(evt, 'self_cpu_memory_usage', 0)\n",
        "            aggregated[layer_name]['Self CUDA Mem (bytes)'] += getattr(evt, 'self_cuda_memory_usage', 0)\n",
        "            aggregated[layer_name]['Total Execution Time (us)'] += evt.cpu_time_total + getattr(evt, 'cuda_time_total', 0.0)\n",
        "            aggregated[layer_name]['Total Memory Used (bytes)'] += getattr(evt, 'self_cpu_memory_usage', 0) + getattr(evt, 'self_cuda_memory_usage', 0)\n",
        "        self.profile_db = self._upsert(self.profile_db, forward_pass)\n",
        "        for data in aggregated.values():\n",
        "            self.profile_db = self._upsert(self.profile_db, data)\n",
        "        self.profile_db.to_csv(self.profile_db_path, index=False)\n",
        "\n",
        "    def _upsert(self, df: pd.DataFrame, row: Dict[str, Any]) -> pd.DataFrame:\n",
        "        mask = (df['Task_ID'] == row['Task_ID']) & (df['Model'] == row['Model']) & (df['Layer'] == row['Layer']) & (df['Compute'] == row['Compute'])\n",
        "        if mask.any():\n",
        "            existing_time = df.loc[mask, 'Total Execution Time (us)'].max()\n",
        "            if row['Total Execution Time (us)'] > existing_time:\n",
        "                for key in self.columns:\n",
        "                    df.loc[mask, key] = row[key]\n",
        "        else:\n",
        "            df = pd.concat([df, pd.DataFrame([row])], ignore_index=True)\n",
        "        return df\n",
        "\n",
        "    def get_profile_db(self) -> pd.DataFrame:\n",
        "        return self.profile_db\n",
        "\n",
        "    def print_profile_db(self):\n",
        "        if self.profile_db.empty:\n",
        "            print(\"ProfileDB is empty.\")\n",
        "        else:\n",
        "            print(\"ProfileDB:\")\n",
        "            print(self.profile_db.to_string(index=False))\n",
        "\n",
        "\n",
        "# ----------------------------------------------------------------------\n",
        "# Stage Class\n",
        "# ----------------------------------------------------------------------\n",
        "class Stage:\n",
        "    def __init__(self, stage_id: str, nodes: List[FxNode], assigned_node: Node, task: 'Task'):\n",
        "        self.stage_id = stage_id\n",
        "        self.nodes = nodes\n",
        "        self.assigned_node = assigned_node\n",
        "        self.dependencies: List[str] = []\n",
        "        self.dependents: List[str] = []\n",
        "        self.execution_time: Optional[float] = None\n",
        "        self.transfer_time: float = 0.0\n",
        "        self.output_data: Optional[torch.Tensor] = None\n",
        "        self.task = task\n",
        "        self.stage_device: str = \"cpu\"\n",
        "\n",
        "    def add_dependency(self, stage_id: str):\n",
        "        self.dependencies.append(stage_id)\n",
        "\n",
        "    def add_dependent(self, stage_id: str):\n",
        "        self.dependents.append(stage_id)\n",
        "\n",
        "    def run_stage(self, node_outputs: Dict[str, torch.Tensor]):\n",
        "        start_time = time.time()\n",
        "        transfer_time = 0.0\n",
        "        if self.assigned_node and self.assigned_node.gpu is not None and torch.cuda.is_available():\n",
        "            device = torch.device(f\"cuda:{self.assigned_node.gpu}\")\n",
        "            self.stage_device = str(device)\n",
        "            # torch.cuda.synchronize(device)\n",
        "        else:\n",
        "            device = torch.device(\"cpu\")\n",
        "            self.stage_device = \"cpu\"\n",
        "        try:\n",
        "            with torch.no_grad():\n",
        "                for fx_node in self.nodes:\n",
        "                    resolved_args = resolve_arg(fx_node.args, node_outputs)\n",
        "                    resolved_kwargs = resolve_arg(fx_node.kwargs, node_outputs)\n",
        "                    t_start = time.time()\n",
        "                    resolved_args = move_tensor_to_device(resolved_args, device)\n",
        "                    resolved_kwargs = move_tensor_to_device(resolved_kwargs, device)\n",
        "                    t_end = time.time()\n",
        "                    transfer_time += (t_end - t_start)\n",
        "                    op = fx_node.op\n",
        "                    if op == \"placeholder\":\n",
        "                        out = self.task.input_data.to(device)\n",
        "                    elif op == \"get_attr\":\n",
        "                        out = getattr(self.task.model, fx_node.target)\n",
        "                    elif op == \"call_module\":\n",
        "                        submodule = self.task.model.get_submodule(fx_node.target)\n",
        "                        submodule.to(device)\n",
        "                        out = submodule(*resolved_args, **resolved_kwargs)\n",
        "                    elif op == \"call_function\":\n",
        "                        out = fx_node.target(*resolved_args, **resolved_kwargs)\n",
        "                    elif op == \"call_method\":\n",
        "                        method = getattr(resolved_args[0], fx_node.target)\n",
        "                        out = method(*resolved_args[1:], **resolved_kwargs)\n",
        "                    elif op == \"output\":\n",
        "                        out = resolved_args[0]\n",
        "                    else:\n",
        "                        raise NotImplementedError(f\"Operation '{op}' is not supported in the executor.\")\n",
        "                    node_outputs[fx_node.name] = out\n",
        "        except Exception as e:\n",
        "            print(f\"[Stage] {self.stage_id} error: {e}\")\n",
        "            self.execution_time = float('inf')\n",
        "            self.transfer_time = float('inf')\n",
        "            node_outputs[self.stage_id] = None\n",
        "            return\n",
        "        finally:\n",
        "            if device.type == 'cuda':\n",
        "                torch.cuda.synchronize(device)\n",
        "            end_time = time.time()\n",
        "            self.execution_time = end_time - start_time\n",
        "            self.transfer_time = transfer_time\n",
        "        self.task.update_busy_time(self.execution_time, self.transfer_time)\n",
        "        if not self.dependents:\n",
        "            final_output_node = next((n for n in self.nodes if n.op == 'output'), None)\n",
        "            if final_output_node:\n",
        "                arg = final_output_node.args[0]\n",
        "                if isinstance(arg, torch.Tensor):\n",
        "                    final_res = arg.cpu()\n",
        "                elif isinstance(arg, FxNode):\n",
        "                    final_res = node_outputs.get(arg.name, None)\n",
        "                else:\n",
        "                    final_res = None\n",
        "                self.task.set_output_data(final_res)\n",
        "            else:\n",
        "                self.task.set_output_data(None)\n",
        "        print(f\"[Stage] {self.stage_id}: Executed on {self.assigned_node.node_id} in {self.execution_time:.6f} s, Transfer: {self.transfer_time:.6f} s.\")\n",
        "\n",
        "    def __repr__(self):\n",
        "        return f\"Stage({self.stage_id}, device={self.stage_device}, node={self.assigned_node.node_id if self.assigned_node else 'None'}, deps={self.dependencies}, exec_time={self.execution_time}, transfer_time={self.transfer_time})\"\n",
        "\n",
        "# import time\n",
        "# import torch\n",
        "# import networkx as nx\n",
        "# from concurrent.futures import ThreadPoolExecutor\n",
        "# from typing import Dict, List, Optional\n",
        "# from torch.fx import Node as FxNode\n",
        "\n",
        "# class Stage:\n",
        "#     def __init__(self, stage_id: str, nodes: List[FxNode], assigned_node, task):\n",
        "#         self.stage_id = stage_id\n",
        "#         self.nodes = nodes\n",
        "#         self.assigned_node = assigned_node\n",
        "#         self.dependencies: List[str] = []\n",
        "#         self.dependents: List[str] = []\n",
        "#         self.execution_time: Optional[float] = None\n",
        "#         self.transfer_time: float = 0.0\n",
        "#         self.output_data: Optional[torch.Tensor] = None\n",
        "#         self.task = task\n",
        "#         self.stage_device: str = \"cpu\"\n",
        "#         self.independent_subgraphs = self._identify_independent_subgraphs()\n",
        "\n",
        "\n",
        "#     def add_dependency(self, stage_id: str):\n",
        "#         self.dependencies.append(stage_id)\n",
        "\n",
        "#     def add_dependent(self, stage_id: str):\n",
        "#         self.dependents.append(stage_id)\n",
        "\n",
        "#     def _identify_independent_subgraphs(self):\n",
        "#         \"\"\"Finds independent subgraphs using NetworkX.\"\"\"\n",
        "#         subgraph_list = []\n",
        "#         G = nx.DiGraph()\n",
        "\n",
        "#         # Build dependency graph\n",
        "#         for node in self.nodes:\n",
        "#             G.add_node(node.name)\n",
        "#         for node in self.nodes:\n",
        "#             for parent in node.all_input_nodes:\n",
        "#                 if parent.name in G.nodes:\n",
        "#                     G.add_edge(parent.name, node.name)\n",
        "\n",
        "#         # Extract independent subgraphs\n",
        "#         for sub_nodes in nx.weakly_connected_components(G):\n",
        "#             subgraph_list.append([n for n in self.nodes if n.name in sub_nodes])\n",
        "\n",
        "#         return subgraph_list\n",
        "\n",
        "#     def execute_subgraph(self, subgraph: List[FxNode], node_outputs: Dict[str, torch.Tensor], stream=None):\n",
        "#         \"\"\"Executes an independent subgraph using CUDA streams for GPU execution.\"\"\"\n",
        "#         device = torch.device(self.stage_device)\n",
        "#         if stream:\n",
        "#             torch.cuda.set_stream(stream)\n",
        "\n",
        "#         for fx_node in subgraph:\n",
        "#             resolved_args = resolve_arg(fx_node.args, node_outputs)\n",
        "#             resolved_kwargs = resolve_arg(fx_node.kwargs, node_outputs)\n",
        "\n",
        "#             # Move tensors to the correct device using `move_tensor_to_device`\n",
        "#             resolved_args = move_tensor_to_device(resolved_args, device)\n",
        "#             resolved_kwargs = move_tensor_to_device(resolved_kwargs, device)\n",
        "\n",
        "#             op = fx_node.op\n",
        "#             out = None\n",
        "\n",
        "#             if op == \"placeholder\":\n",
        "#                 out = move_tensor_to_device(self.task.input_data, device)\n",
        "#             elif op == \"get_attr\":\n",
        "#                 out = getattr(self.task.model, fx_node.target)\n",
        "#             elif op == \"call_module\":\n",
        "#                 submodule = self.task.model.get_submodule(fx_node.target)\n",
        "#                 submodule.to(device, non_blocking=True)\n",
        "#                 out = submodule(*resolved_args, **resolved_kwargs)\n",
        "#             elif op == \"call_function\":\n",
        "#                 out = fx_node.target(*resolved_args, **resolved_kwargs)\n",
        "#             elif op == \"call_method\":\n",
        "#                 method = getattr(resolved_args[0], fx_node.target)\n",
        "#                 out = method(*resolved_args[1:], **resolved_kwargs)\n",
        "#             elif op == \"output\":\n",
        "#                 out = resolved_args[0]\n",
        "\n",
        "#             node_outputs[fx_node.name] = out\n",
        "\n",
        "#         if stream:\n",
        "#             stream.synchronize()\n",
        "\n",
        "#     def run_stage(self, node_outputs: Dict[str, torch.Tensor]):\n",
        "#         \"\"\"Runs the stage, executing independent subgraphs in parallel using CUDA streams or threads.\"\"\"\n",
        "#         start_time = time.time()\n",
        "\n",
        "#         # Select execution device\n",
        "#         if self.assigned_node and self.assigned_node.gpu is not None and torch.cuda.is_available():\n",
        "#             self.stage_device = f\"cuda:{self.assigned_node.gpu}\"\n",
        "#             streams = [torch.cuda.Stream() for _ in self.independent_subgraphs]\n",
        "#         else:\n",
        "#             self.stage_device = \"cpu\"\n",
        "#             streams = [None] * len(self.independent_subgraphs)\n",
        "\n",
        "#         try:\n",
        "#             with torch.no_grad():\n",
        "#                 threads = []\n",
        "#                 with ThreadPoolExecutor(max_workers=len(self.independent_subgraphs)) as executor:\n",
        "#                     for subgraph, stream in zip(self.independent_subgraphs, streams):\n",
        "#                         threads.append(executor.submit(self.execute_subgraph, subgraph, node_outputs, stream))\n",
        "\n",
        "#                     for thread in threads:\n",
        "#                         thread.result()  # Ensure completion\n",
        "\n",
        "#         except Exception as e:\n",
        "#             print(f\"[Stage] {self.stage_id} error: {e}\")\n",
        "#             self.execution_time = float('inf')\n",
        "#             self.transfer_time = float('inf')\n",
        "#             node_outputs[self.stage_id] = None\n",
        "#             return\n",
        "\n",
        "#         finally:\n",
        "#             if \"cuda\" in self.stage_device:\n",
        "#                 torch.cuda.synchronize()\n",
        "\n",
        "#             self.execution_time = time.time() - start_time\n",
        "#             self.task.update_busy_time(self.execution_time, self.transfer_time)\n",
        "\n",
        "#         if not self.dependents:\n",
        "#             final_output_node = next((n for n in self.nodes if n.op == 'output'), None)\n",
        "#             if final_output_node:\n",
        "#                 arg = final_output_node.args[0]\n",
        "#                 if isinstance(arg, torch.Tensor):\n",
        "#                     final_res = arg.cpu()\n",
        "#                 elif isinstance(arg, FxNode):\n",
        "#                     final_res = node_outputs.get(arg.name, None)\n",
        "#                 else:\n",
        "#                     final_res = None\n",
        "#                 self.task.set_output_data(final_res)\n",
        "#             else:\n",
        "#                 self.task.set_output_data(None)\n",
        "\n",
        "#         print(f\"[Stage] {self.stage_id}: Executed on {self.assigned_node.node_id} in {self.execution_time:.6f} s, Transfer: {self.transfer_time:.6f} s.\")\n",
        "\n",
        "#     def __repr__(self):\n",
        "#         return f\"Stage({self.stage_id}, device={self.stage_device}, node={self.assigned_node.node_id if self.assigned_node else 'None'}, deps={self.dependencies}, exec_time={self.execution_time}, transfer_time={self.transfer_time})\"\n",
        "\n",
        "\n",
        "# ----------------------------------------------------------------------\n",
        "# HPCUtilizationMetric Class\n",
        "# ----------------------------------------------------------------------\n",
        "class HPCUtilizationMetric:\n",
        "    def __init__(self, slack_fraction: float = 0.2, transfer_penalty: float = 0.0005):\n",
        "        self.slack_fraction = slack_fraction\n",
        "        self._cached_obs_sec = 0.0\n",
        "        # Transfer penalty (in seconds) to add per stage boundary.\n",
        "        self.transfer_penalty = transfer_penalty\n",
        "\n",
        "    def _ensure_obs_window(self, taskset: \"Taskset\"):\n",
        "        if self._cached_obs_sec == 0.0:\n",
        "            obs_window_sec = taskset.compute_observation_window(self.slack_fraction)\n",
        "            self._cached_obs_sec = max(obs_window_sec, 0.0)\n",
        "        return self._cached_obs_sec\n",
        "\n",
        "    def compute_task(self, task: \"Task\", taskset: \"Taskset\") -> float:\n",
        "        obs_sec = self._ensure_obs_window(taskset)\n",
        "        if obs_sec <= 0:\n",
        "            return 0.0\n",
        "        fwd_sec = task.get_forward_pass_time()\n",
        "        return (fwd_sec / obs_sec) * 100\n",
        "\n",
        "    def compute_layer(self, task: \"Task\", node: Node, fx_node: FxNode) -> float:\n",
        "        obs_sec = self._cached_obs_sec\n",
        "        if obs_sec <= 0:\n",
        "            return 0.0\n",
        "        key = (node.node_id, fx_node.name)\n",
        "        record = task.prof_records.get(key, None)\n",
        "        return record.get(\"Total Execution Time (us)\", 0.0) if record is not None else 0.0\n",
        "\n",
        "\n",
        "# ----------------------------------------------------------------------\n",
        "# Task Class\n",
        "# ----------------------------------------------------------------------\n",
        "class Task:\n",
        "    def __init__(self, task_id: str, model: nn.Module, input_data: torch.Tensor, model_name: str,\n",
        "                 profiler: Profiler, load_metric: Optional[HPCUtilizationMetric] = None):\n",
        "        self.task_id = task_id\n",
        "        self.model = model\n",
        "        self.input_data = input_data\n",
        "        self.model_name = model_name\n",
        "        self.profiler = profiler\n",
        "        self.load_metric = load_metric if load_metric else HPCUtilizationMetric()\n",
        "        self.stages: Dict[str, Stage] = {}\n",
        "        self.graph = nx.DiGraph()\n",
        "        self.start_time: Optional[float] = None\n",
        "        self.finish_time: Optional[float] = None\n",
        "        self.output_data: Optional[torch.Tensor] = None\n",
        "        self.busy_time: float = 0.0\n",
        "        self.computation_time: float = 0.0\n",
        "        self.transfer_time: float = 0.0\n",
        "        self.prof_records: Dict[Tuple[str, str], Dict[str, Any]] = {}\n",
        "        self._available_nodes: List[Node] = []\n",
        "        self.init_traced_graph: List[str] = []\n",
        "        self.placeholder_names: Set[str] = set()\n",
        "        self._initialize_dag()\n",
        "\n",
        "    def _initialize_dag(self):\n",
        "        tracer = fx.symbolic_trace(self.model)\n",
        "        traced_nodes = list(tracer.graph.nodes)\n",
        "        self.init_traced_graph = [node.name for node in traced_nodes]\n",
        "        self.placeholder_names = set(node.name for node in traced_nodes if node.op == \"placeholder\")\n",
        "\n",
        "    def get_forward_pass_time(self, sum_across_compute: bool = False) -> float:\n",
        "        if not self.profiler:\n",
        "            return 0.0\n",
        "        profile_df = self.profiler.get_profile_db()\n",
        "        mask = (profile_df['Task_ID'] == self.task_id) & (profile_df['Model'] == self.model_name) & (profile_df['Layer'] == 'forward_pass')\n",
        "        matched = profile_df.loc[mask]\n",
        "        if matched.empty:\n",
        "            return 0.0\n",
        "        times = matched['Total Execution Time (us)']\n",
        "        return times.sum() if sum_across_compute else times.max()\n",
        "\n",
        "    def run_offline_partition(self, base_loads: List[float]):\n",
        "        self.stages.clear()\n",
        "        self.graph.clear()\n",
        "        if not self._available_nodes:\n",
        "            return\n",
        "        tracer = fx.symbolic_trace(self.model)\n",
        "        fx_nodes = list(tracer.graph.nodes)\n",
        "        L = len(fx_nodes)\n",
        "        K = len(self._available_nodes)\n",
        "        if L == 0:\n",
        "            return\n",
        "        # Build cost table: cost for each layer on each node (in microseconds).\n",
        "        layer_cost = []\n",
        "        for k in range(K):\n",
        "            row = []\n",
        "            node = self._available_nodes[k]\n",
        "            for fx_node in fx_nodes:\n",
        "                c = self.load_metric.compute_layer(self, node, fx_node)\n",
        "                row.append(c)\n",
        "            layer_cost.append(row)\n",
        "        prefix_sum = []\n",
        "        for k in range(K):\n",
        "            ps = [0.0]*(L+1)\n",
        "            for i in range(1, L+1):\n",
        "                ps[i] = ps[i-1] + layer_cost[k][i-1]\n",
        "            prefix_sum.append(ps)\n",
        "        def sum_cost(k, start, end):\n",
        "            return prefix_sum[k][end+1] - prefix_sum[k][start]\n",
        "        # DP table: M[n][k] is the optimal cost for partitioning first n layers among k nodes.\n",
        "        M = [[float(\"inf\")]*(K+1) for _ in range(L+1)]\n",
        "        split_pt = [[0]*(K+1) for _ in range(L+1)]\n",
        "        for n in range(K+1):\n",
        "            M[0][n] = 0.0\n",
        "        for n in range(1, L+1):\n",
        "            M[n][1] = base_loads[0] + sum_cost(0, 0, n-1)\n",
        "            split_pt[n][1] = 0\n",
        "        # Here, when moving to a new node (stage boundary), add the measured transfer penalty.\n",
        "        for k in range(2, K+1):\n",
        "            for n in range(1, L+1):\n",
        "                best_val = float(\"inf\")\n",
        "                best_x = 0\n",
        "                for x in range(0, n+1):\n",
        "                    penalty = self.load_metric.transfer_penalty\n",
        "                    candidate = max(M[x][k-1] + penalty, base_loads[k-1] + sum_cost(k-1, x, n-1) + penalty)\n",
        "                    if candidate < best_val:\n",
        "                        best_val = candidate\n",
        "                        best_x = x\n",
        "                M[n][k] = best_val\n",
        "                split_pt[n][k] = best_x\n",
        "        partition_blocks = []\n",
        "        nlayers_left = L\n",
        "        knodes_left = K\n",
        "        while nlayers_left > 0 and knodes_left > 0:\n",
        "            x = split_pt[nlayers_left][knodes_left]\n",
        "            node_idx = knodes_left - 1\n",
        "            partition_blocks.append((x, nlayers_left-1, node_idx))\n",
        "            nlayers_left = x\n",
        "            knodes_left -= 1\n",
        "        partition_blocks.reverse()\n",
        "        for i, (start, end, nd_idx) in enumerate(partition_blocks):\n",
        "            stage_id = f\"{self.task_id}-stage-{i+1}\"\n",
        "            assigned_node = self._available_nodes[nd_idx]\n",
        "            subset = fx_nodes[start:end+1]\n",
        "            stg = Stage(stage_id, subset, assigned_node, self)\n",
        "            self.add_stage(stg)\n",
        "        for i in range(1, len(partition_blocks)):\n",
        "            prev = f\"{self.task_id}-stage-{i}\"\n",
        "            curr = f\"{self.task_id}-stage-{i+1}\"\n",
        "            self.add_dependency(prev, curr)\n",
        "\n",
        "    def populate_profile_records(self):\n",
        "        if self.profiler:\n",
        "            db = self.profiler.get_profile_db()\n",
        "            for compute in db['Compute'].unique():\n",
        "                records = {}\n",
        "                for layer in self.init_traced_graph:\n",
        "                    mask = (db['Task_ID'] == self.task_id) & (db['Model'] == self.model_name) & (db['Layer'] == layer)\n",
        "                    if pd.notnull(compute):\n",
        "                        mask = mask & (db['Compute'] == compute)\n",
        "                    matched = db.loc[mask]\n",
        "                    if not matched.empty:\n",
        "                        records[(compute, layer)] = matched.iloc[0].to_dict()\n",
        "                    else:\n",
        "                        records[(compute, layer)] = None\n",
        "                self.prof_records.update(records)\n",
        "\n",
        "    def add_stage(self, stage: Stage):\n",
        "        if stage.stage_id in self.stages:\n",
        "            raise ValueError(f\"Stage {stage.stage_id} already exists.\")\n",
        "        self.stages[stage.stage_id] = stage\n",
        "        self.graph.add_node(stage.stage_id, stage=stage)\n",
        "\n",
        "    def add_dependency(self, from_stage: str, to_stage: str):\n",
        "        if from_stage not in self.stages or to_stage not in self.stages:\n",
        "            raise ValueError(\"Stage not found.\")\n",
        "        self.graph.add_edge(from_stage, to_stage)\n",
        "        self.stages[to_stage].add_dependency(from_stage)\n",
        "        self.stages[from_stage].add_dependent(to_stage)\n",
        "\n",
        "    def get_execution_order(self) -> List[str]:\n",
        "        try:\n",
        "            return list(nx.topological_sort(self.graph))\n",
        "        except nx.NetworkXUnfeasible:\n",
        "            raise ValueError(\"Cycle in DAG\")\n",
        "\n",
        "    def get_total_execution_time(self) -> float:\n",
        "        if self.start_time and self.finish_time:\n",
        "            return self.finish_time - self.start_time\n",
        "        return 0.0\n",
        "\n",
        "    def update_busy_time(self, exec_time: float, transfer_time: float = 0.0):\n",
        "        self.busy_time += exec_time\n",
        "        self.transfer_time += transfer_time\n",
        "        self.computation_time += (exec_time - transfer_time)\n",
        "\n",
        "    def set_output_data(self, output: torch.Tensor):\n",
        "        self.output_data = output.cpu() if output is not None else None\n",
        "        self.finish_time = time.time()\n",
        "\n",
        "    def print_stage_allocations(self):\n",
        "        print(f\"=== Stage Allocations for Task {self.task_id} ===\")\n",
        "        for stage in self.stages.values():\n",
        "            layer_names = [node.name for node in stage.nodes]\n",
        "            node_id = stage.assigned_node.node_id if stage.assigned_node else \"Unassigned\"\n",
        "            print(f\"Stage {stage.stage_id}: Node: {node_id}, Layers: {', '.join(layer_names)}, Deps: {stage.dependencies}\")\n",
        "\n",
        "\n",
        "# ----------------------------------------------------------------------\n",
        "# Taskset Class (Offline Allocation with Measured Transfer Cost)\n",
        "# ----------------------------------------------------------------------\n",
        "class Taskset:\n",
        "    def __init__(self, tasks: List[Task], available_nodes: List[Node],\n",
        "                 metric: Optional[HPCUtilizationMetric] = None):\n",
        "        self.tasks = tasks\n",
        "        # Sort available nodes so that GPU nodes come first.\n",
        "        self.available_nodes = sorted(available_nodes, key=lambda n: 0 if n.gpu is not None else 1)\n",
        "        self.metric = metric if metric else HPCUtilizationMetric()\n",
        "        # Measure max transfer penalty using a sample tensor from the first task (if available)\n",
        "        if self.tasks and self.available_nodes:\n",
        "            sample_tensor = self.tasks[0].input_data\n",
        "            penalty = measure_max_transfer_penalty(self.available_nodes, sample_tensor)\n",
        "            print(f\"Measured max transfer penalty: {penalty:.6f} s\")\n",
        "            self.metric.transfer_penalty = penalty\n",
        "        self.total_utilization: float = 0.0\n",
        "        self.average_turnaround_time: float = 0.0\n",
        "        self.throughput: float = 0.0\n",
        "        self.makespan: float = 0.0\n",
        "        self.task_completion_rate: float = 0.0\n",
        "        self.loads: Dict[str, float] = {}\n",
        "        for task in self.tasks:\n",
        "            task.load_metric = self.metric\n",
        "        self._offline_allocate()\n",
        "\n",
        "    def _offline_allocate(self):\n",
        "        for t in self.tasks:\n",
        "            val = self.metric.compute_task(t, self)\n",
        "            self.loads[t.task_id] = val\n",
        "        sorted_tasks = sorted(self.tasks, key=lambda x: self.loads[x.task_id], reverse=True)\n",
        "        base_loads = [0.0] * len(self.available_nodes)\n",
        "        for t in sorted_tasks:\n",
        "            # Assign GPUs first\n",
        "            gpu_nodes = [n for n in self.available_nodes if n.gpu is not None]\n",
        "            cpu_nodes = [n for n in self.available_nodes if n.gpu is None]\n",
        "\n",
        "            if gpu_nodes:\n",
        "                t._available_nodes = gpu_nodes + cpu_nodes  # Prefer GPU first\n",
        "            else:\n",
        "                t._available_nodes = cpu_nodes  # Fallback to CPU\n",
        "\n",
        "            t.run_offline_partition(base_loads)\n",
        "            self._update_base_loads_for_task(t, base_loads)\n",
        "\n",
        "    def _update_base_loads_for_task(self, task: Task, base_loads: List[float]):\n",
        "        for stg in task.stages.values():\n",
        "            nd = stg.assigned_node\n",
        "            if nd is None:\n",
        "                continue\n",
        "            idx = self.available_nodes.index(nd)\n",
        "            stage_cost = sum(self.metric.compute_layer(task, nd, node) for node in stg.nodes)\n",
        "            base_loads[idx] += stage_cost\n",
        "\n",
        "    def compute_observation_window(self, slack_fraction=0.2) -> float:\n",
        "        total_time = sum(t.get_forward_pass_time() for t in self.tasks)\n",
        "        return total_time * (1.0 + slack_fraction)\n",
        "\n",
        "    def execute_all(self):\n",
        "        threads = []\n",
        "        for t in self.tasks:\n",
        "            thr = threading.Thread(target=self._execute_task, args=(t,))\n",
        "            thr.start()\n",
        "            threads.append(thr)\n",
        "        for thr in threads:\n",
        "            thr.join()\n",
        "        self._calculate_metrics()\n",
        "\n",
        "    def _execute_task(self, task: Task):\n",
        "        print(f\"[Taskset] Starting Task {task.task_id}\")\n",
        "        task.start_time = time.time()\n",
        "        try:\n",
        "            order = task.get_execution_order()\n",
        "        except ValueError as e:\n",
        "            print(f\"[Taskset] {task.task_id} error: {e}\")\n",
        "            return\n",
        "        node_outputs = {}\n",
        "        for sid in order:\n",
        "            stg = task.stages[sid]\n",
        "            def run_stage(s=stg):\n",
        "                s.run_stage(node_outputs)\n",
        "            res_q = stg.assigned_node.assign_task(run_stage)\n",
        "            res_q.get()\n",
        "            if stg.execution_time == float('inf'):\n",
        "                print(f\"[Taskset] Stage {sid} failed.\")\n",
        "        task.finish_time = time.time()\n",
        "        print(f\"[Taskset] Completed Task {task.task_id}.\")\n",
        "\n",
        "    def _calculate_metrics(self):\n",
        "        total_busy_time = sum(stg.execution_time for t in self.tasks for stg in t.stages.values())\n",
        "        earliest_start = min((t.start_time for t in self.tasks if t.start_time), default=None)\n",
        "        latest_finish = max((t.finish_time for t in self.tasks if t.finish_time), default=None)\n",
        "        obs = (latest_finish - earliest_start) if earliest_start and latest_finish else 0.0\n",
        "        used_nodes = {stg.assigned_node.node_id for t in self.tasks for stg in t.stages.values() if stg.assigned_node}\n",
        "        total_available_time = obs * len(used_nodes)\n",
        "        self.total_utilization = total_busy_time / total_available_time if total_available_time > 0 else 0.0\n",
        "        ttimes = [t.get_total_execution_time() for t in self.tasks]\n",
        "        self.average_turnaround_time = sum(ttimes) / len(ttimes) if ttimes else 0.0\n",
        "        self.makespan = obs\n",
        "        self.throughput = len(self.tasks) / obs if obs > 0 else 0.0\n",
        "        done = [t for t in self.tasks if t.output_data is not None]\n",
        "        self.task_completion_rate = len(done) / len(self.tasks) if self.tasks else 0.0\n",
        "\n",
        "    def __repr__(self):\n",
        "        return (f\"Taskset(num_tasks={len(self.tasks)}, utilization={self.total_utilization:.2%}, \"\n",
        "                f\"avg_turnaround={self.average_turnaround_time:.4f}s, throughput={self.throughput:.3f} tasks/s, \"\n",
        "                f\"makespan={self.makespan:.4f}s, completion_rate={self.task_completion_rate:.2%})\")\n",
        "\n",
        "\n",
        "# ----------------------------------------------------------------------\n",
        "# Evaluator Class\n",
        "# ----------------------------------------------------------------------\n",
        "class Evaluator:\n",
        "    def __init__(self, taskset: Taskset, profiler: Profiler):\n",
        "        self.taskset = taskset\n",
        "        self.profiler = profiler\n",
        "        self.naive_outputs: Dict[str, torch.Tensor] = {}\n",
        "        self.parallel_outputs: Dict[str, torch.Tensor] = {}\n",
        "        self.naive_execution_times: Dict[str, float] = {}\n",
        "        self.parallel_execution_times: Dict[str, float] = {}\n",
        "        self.naive_completion_times: Dict[str, float] = {}\n",
        "        self.parallel_completion_times: Dict[str, float] = {}\n",
        "        self.naive_makespan: float = 0.0\n",
        "        self.parallel_makespan: float = 0.0\n",
        "\n",
        "        self.speedup_maksepan: float = 0.0\n",
        "        self.throughput_makespan: float = 0.0\n",
        "\n",
        "    def run_naive_execution(self):\n",
        "        print(\"[Evaluator] Starting Naive Execution.\")\n",
        "        naive_start = time.time()\n",
        "        for task in self.taskset.tasks:\n",
        "            device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "            task.model.to(device)\n",
        "            input_tensor = task.input_data.to(device)\n",
        "            t0 = time.time()\n",
        "            with torch.no_grad():\n",
        "                output = task.model(input_tensor)\n",
        "            if device.type == 'cuda':\n",
        "                torch.cuda.synchronize()\n",
        "            t1 = time.time()\n",
        "            exec_time = t1 - t0\n",
        "            self.naive_execution_times[task.task_id] = exec_time\n",
        "            self.naive_completion_times[task.task_id] = time.time() - naive_start\n",
        "            self.naive_outputs[task.task_id] = output.cpu()\n",
        "            print(f\"[Evaluator] Task {task.task_id}: Naive exec time: {exec_time:.6f}s\")\n",
        "        self.naive_makespan = time.time() - naive_start\n",
        "        print(f\"[Evaluator] Naive makespan: {self.naive_makespan:.6f}s\\n\")\n",
        "\n",
        "    def run_parallel_execution(self):\n",
        "        print(\"[Evaluator] Starting Parallel Execution.\")\n",
        "        self.parallel_outputs.clear()\n",
        "        self.parallel_execution_times.clear()\n",
        "        self.parallel_completion_times.clear()\n",
        "        parallel_start = time.time()\n",
        "        self.taskset.execute_all()\n",
        "        parallel_end = time.time()\n",
        "        self.parallel_makespan = parallel_end - parallel_start\n",
        "        for task in self.taskset.tasks:\n",
        "            self.parallel_outputs[task.task_id] = task.output_data.cpu() if task.output_data is not None else None\n",
        "            self.parallel_execution_times[task.task_id] = task.get_total_execution_time()\n",
        "            self.parallel_completion_times[task.task_id] = (task.finish_time - parallel_start) if task.finish_time else float('nan')\n",
        "        print(f\"[Evaluator] Parallel makespan: {self.parallel_makespan:.6f}s\\n\")\n",
        "\n",
        "    def compare_outputs(self):\n",
        "        print(\"[Evaluator] Comparing Outputs.\")\n",
        "        all_match = True\n",
        "        for task_id in self.naive_outputs:\n",
        "            naive_out = self.naive_outputs[task_id]\n",
        "            parallel_out = self.parallel_outputs.get(task_id)\n",
        "            if naive_out is None or parallel_out is None:\n",
        "                print(f\"[Evaluator] Task {task_id} missing output.\")\n",
        "                all_match = False\n",
        "                continue\n",
        "            if torch.equal(naive_out, parallel_out) or torch.allclose(naive_out, parallel_out, atol=1e-5):\n",
        "                print(f\"[Evaluator] Task {task_id}: Outputs match.\")\n",
        "            else:\n",
        "                print(f\"[Evaluator] Task {task_id}: Outputs do NOT match.\")\n",
        "                all_match = False\n",
        "        if all_match:\n",
        "            print(\"[Evaluator] All outputs match.\\n\")\n",
        "        else:\n",
        "            print(\"[Evaluator] Some outputs differ.\\n\")\n",
        "\n",
        "    # def analyze_speedup_throughput(self):\n",
        "    #     print(\"[Evaluator] Analyzing Speedup and Throughput.\\n\")\n",
        "    #     total_naive = sum(self.naive_execution_times.values())\n",
        "    #     total_parallel = sum(self.parallel_execution_times.values())\n",
        "    #     print(\"--- Sum-of-times ---\")\n",
        "    #     print(f\"Naive total: {total_naive:.6f}s, Parallel total: {total_parallel:.6f}s\")\n",
        "    #     speedup_sum = total_naive / total_parallel if total_parallel > 0 else float('inf')\n",
        "    #     num_tasks = len(self.taskset.tasks)\n",
        "    #     print(f\"Speedup (sum-of-times): {speedup_sum:.2f}x\")\n",
        "    #     print(f\"Naive Throughput: {num_tasks/total_naive:.2f} tasks/s, Parallel Throughput: {num_tasks/total_parallel:.2f} tasks/s\\n\")\n",
        "    #     print(\"--- Makespan ---\")\n",
        "    #     print(f\"Naive makespan: {self.naive_makespan:.6f}s, Parallel makespan: {self.parallel_makespan:.6f}s\")\n",
        "    #     self.speedup_makespan = self.naive_makespan / self.parallel_makespan if self.parallel_makespan > 0 else float('inf')\n",
        "\n",
        "    #     print(f\"Speedup (makespan): {self.speedup_makespan:.2f}x\")\n",
        "    #     self.throughput_makespan = num_tasks/self.parallel_makespan\n",
        "    #     print(f\"Naive Throughput (makespan): {num_tasks/self.naive_makespan:.2f} tasks/s, Parallel Throughput (makespan): {num_tasks/self.parallel_makespan:.2f} tasks/s\\n\")\n",
        "    #     print(\"--- Task Completion Times ---\")\n",
        "    #     for task_id in self.naive_completion_times:\n",
        "    #         print(f\"Task {task_id}: Naive finish: {self.naive_completion_times[task_id]:.6f}s, Parallel finish: {self.parallel_completion_times.get(task_id, float('nan')):.6f}s\")\n",
        "    #     print()\n",
        "    def analyze_speedup_throughput(self):\n",
        "        print(\"[Evaluator] Analyzing Speedup and Throughput.\\n\")\n",
        "\n",
        "        # Total execution time for naive and parallel runs\n",
        "        total_naive = sum(self.naive_execution_times.values())\n",
        "        total_parallel = sum(self.parallel_execution_times.values())\n",
        "\n",
        "        print(\"--- Sum-of-times ---\")\n",
        "        print(f\"Naive total: {total_naive:.6f}s, Parallel total: {total_parallel:.6f}s\")\n",
        "        speedup_sum = total_naive / total_parallel if total_parallel > 0 else float('inf')\n",
        "        num_tasks = len(self.taskset.tasks)\n",
        "        print(f\"Speedup (sum-of-times): {speedup_sum:.2f}x\")\n",
        "        print(f\"Naive Throughput: {num_tasks / total_naive:.2f} tasks/s, Parallel Throughput: {num_tasks / total_parallel:.2f} tasks/s\\n\")\n",
        "\n",
        "        print(\"--- Makespan ---\")\n",
        "        print(f\"Naive makespan: {self.naive_makespan:.6f}s, Parallel makespan: {self.parallel_makespan:.6f}s\")\n",
        "        self.speedup_makespan = self.naive_makespan / self.parallel_makespan if self.parallel_makespan > 0 else float('inf')\n",
        "        self.throughput_makespan = num_tasks / self.parallel_makespan if self.parallel_makespan > 0 else 0.0\n",
        "\n",
        "        print(f\"Speedup (makespan): {self.speedup_makespan:.2f}x\")\n",
        "        print(f\"Naive Throughput (makespan): {num_tasks / self.naive_makespan:.2f} tasks/s, Parallel Throughput (makespan): {self.throughput_makespan:.2f} tasks/s\\n\")\n",
        "\n",
        "        print(\"--- Task Completion Times ---\")\n",
        "        for task_id in self.naive_completion_times:\n",
        "            print(f\"Task {task_id}: Naive finish: {self.naive_completion_times[task_id]:.6f}s, Parallel finish: {self.parallel_completion_times.get(task_id, float('nan')):.6f}s\")\n",
        "        print()\n",
        "\n",
        "\n",
        "# ----------------------------------------------------------------------\n",
        "# Test Script\n",
        "# ----------------------------------------------------------------------\n",
        "# if __name__ == \"__main__\":\n",
        "def set_seed(seed: int = 42):\n",
        "    import random, numpy as np\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed(seed)\n",
        "        torch.cuda.manual_seed_all(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "    print(f\"Seed set to {seed}\")\n",
        "\n",
        "# Define SimpleCNN model.\n",
        "class SimpleCNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(SimpleCNN, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 16, 3, padding=1)\n",
        "        self.conv2 = nn.Conv2d(3, 16, 5, padding=2)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.flatten = nn.Flatten()\n",
        "        self.fc = nn.Linear(32 * 28 * 28, 10)\n",
        "    def forward(self, x):\n",
        "        x1 = self.relu(self.conv1(x))\n",
        "        x2 = self.relu(self.conv2(x))\n",
        "        x = torch.cat((x1, x2), dim=1)\n",
        "        x = self.flatten(x)\n",
        "        x = self.fc(x)\n",
        "        return x\n",
        "\n",
        "# Define PretrainedResNet18 model.\n",
        "class PretrainedResNet18(nn.Module):\n",
        "    def __init__(self, num_classes=10):\n",
        "        super(PretrainedResNet18, self).__init__()\n",
        "        self.resnet18 = models.resnet18(pretrained=False)\n",
        "        num_ftrs = self.resnet18.fc.in_features\n",
        "        self.resnet18.fc = nn.Linear(num_ftrs, num_classes)\n",
        "    def forward(self, x):\n",
        "        return self.resnet18(x)\n",
        "\n",
        "# For the Vision Transformer we use the base vit_b_16 directly.\n",
        "def create_vit_model(num_classes=10):\n",
        "    model = models.vit_b_16(pretrained=False)\n",
        "    in_features = model.heads.head.in_features\n",
        "    model.heads.head = nn.Linear(in_features, num_classes)\n",
        "    return model\n",
        "\n",
        "# Create a dataloader utility.\n",
        "def create_dataloader(batch_size: int, num_samples: int, input_size: Tuple[int, int, int]):\n",
        "    inputs = torch.randn(num_samples, *input_size)\n",
        "    targets = torch.randint(0, 10, (num_samples,))\n",
        "    dataset = TensorDataset(inputs, targets)\n",
        "    return DataLoader(dataset, batch_size=batch_size)\n",
        "\n",
        "#     set_seed(42)\n",
        "#     nodes = Node.discover_nodes(disjoint=True)\n",
        "#     print(f\"Discovered Nodes: {nodes}\")\n",
        "\n",
        "#     profiler = Profiler(mode=\"init\")\n",
        "\n",
        "#     simple_tasks = []\n",
        "#     for i in range(5):\n",
        "#         model = SimpleCNN()\n",
        "#         dl = create_dataloader(10, 100, (3, 28, 28))\n",
        "#         input_tensor, _ = next(iter(dl))\n",
        "#         task = Task(task_id=f\"simple_{i+1}\", model=model, input_data=input_tensor,\n",
        "#                     model_name=\"SimpleCNN\", profiler=profiler)\n",
        "#         simple_tasks.append(task)\n",
        "\n",
        "#     resnet_tasks = []\n",
        "#     for i in range(15):\n",
        "#         model = PretrainedResNet18()\n",
        "#         dl = create_dataloader(10, 100, (3, 224, 224))\n",
        "#         input_tensor, _ = next(iter(dl))\n",
        "#         task = Task(task_id=f\"resnet_{i+1}\", model=model, input_data=input_tensor,\n",
        "#                     model_name=\"PretrainedResNet18\", profiler=profiler)\n",
        "#         resnet_tasks.append(task)\n",
        "\n",
        "#     vit_tasks = []\n",
        "#     for i in range(2):\n",
        "#         model = create_vit_model()\n",
        "#         dl = create_dataloader(10, 100, (3, 224, 224))\n",
        "#         input_tensor, _ = next(iter(dl))\n",
        "#         task = Task(task_id=f\"vit_{i+1}\", model=model, input_data=input_tensor,\n",
        "#                     model_name=\"VisionTransformer\", profiler=profiler)\n",
        "#         vit_tasks.append(task)\n",
        "\n",
        "#     all_tasks = simple_tasks + resnet_tasks\n",
        "\n",
        "#     # Profile each task on every node (with reuse when possible).\n",
        "#     for task in all_tasks:\n",
        "#         for node in nodes:\n",
        "#             input_copy = copy.deepcopy(task.input_data)\n",
        "#             profiler.profile_model(model=task.model, input_data=input_copy, node_id=node.node_id, task_id=task.task_id)\n",
        "#             time.sleep(0.05)\n",
        "#     profiler.print_profile_db()\n",
        "#     for task in all_tasks:\n",
        "#         task.populate_profile_records()\n",
        "\n",
        "#     taskset = Taskset(tasks=all_tasks, available_nodes=nodes)\n",
        "#     for task in taskset.tasks:\n",
        "#         task.print_stage_allocations()\n",
        "\n",
        "#     evaluator = Evaluator(taskset, profiler)\n",
        "#     evaluator.run_naive_execution()\n",
        "#     evaluator.run_parallel_execution()\n",
        "#     evaluator.compare_outputs()\n",
        "#     evaluator.analyze_speedup_throughput()\n",
        "\n",
        "#     for node in nodes:\n",
        "#         node.stop()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def run_experiment(k: int, heavy_light_ratio: Tuple[int, int], num_tasks: int = 10):\n",
        "    \"\"\"\n",
        "    Run an experiment with configurable number of cores `k` and heavy-to-light DNN ratio.\n",
        "\n",
        "    Args:\n",
        "        k (int): Number of available cores/nodes.\n",
        "        heavy_light_ratio (Tuple[int, int]): Ratio of heavy to light tasks.\n",
        "        num_tasks (int): Total number of tasks (default: 10).\n",
        "    \"\"\"\n",
        "    set_seed(42)\n",
        "\n",
        "    # **Step 1: Discover Nodes (Limited to `k` cores)**\n",
        "    all_nodes = Node.discover_nodes(disjoint=True)\n",
        "    nodes = all_nodes[:k]  # Limit to `k` nodes\n",
        "    print(f\"Using {len(nodes)} nodes: {nodes}\")\n",
        "\n",
        "    # **Step 2: Initialize Profiler (with reuse)**\n",
        "    profiler = Profiler(mode=\"init\")\n",
        "\n",
        "    # **Step 3: Define Heavy and Light Models**\n",
        "    def create_ffn_model():\n",
        "        return nn.Sequential(\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(784, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(512, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(256, 10)\n",
        "        )\n",
        "\n",
        "    heavy_models = [PretrainedResNet18]\n",
        "    light_models = [SimpleCNN]\n",
        "\n",
        "    # **Step 4: Compute Task Distribution**\n",
        "    heavy_count = (num_tasks * heavy_light_ratio[0]) // (heavy_light_ratio[0] + heavy_light_ratio[1])\n",
        "    light_count = num_tasks - heavy_count\n",
        "\n",
        "    print(f\"Creating {heavy_count} heavy tasks and {light_count} light tasks.\")\n",
        "\n",
        "    # **Step 5: Generate Tasks**\n",
        "    tasks = []\n",
        "\n",
        "    for i in range(heavy_count):\n",
        "        model_class = heavy_models[i % len(heavy_models)]\n",
        "        model = model_class()\n",
        "        dl = create_dataloader(10, 100, (3, 224, 224))\n",
        "        input_tensor, _ = next(iter(dl))\n",
        "        task = Task(task_id=f\"heavy_{i+1}\", model=model, input_data=input_tensor,\n",
        "                    model_name=model.__class__.__name__, profiler=profiler)\n",
        "        tasks.append(task)\n",
        "\n",
        "    for i in range(light_count):\n",
        "        model_class = light_models[i % len(light_models)]\n",
        "        model = model_class()\n",
        "        dl = create_dataloader(10, 100, (3, 28, 28))\n",
        "        input_tensor, _ = next(iter(dl))\n",
        "        task = Task(task_id=f\"light_{i+1}\", model=model, input_data=input_tensor,\n",
        "                    model_name=model.__class__.__name__, profiler=profiler)\n",
        "        tasks.append(task)\n",
        "\n",
        "    # **Step 6: Profile Each Model on Every Node (Reuse If Available)**\n",
        "    for task in tasks:\n",
        "        for node in nodes:\n",
        "            input_copy = copy.deepcopy(task.input_data)\n",
        "            # profiler.profile_model(model=task.model, input_data=input_copy, node_id=node.node_id, task_id=task.task_id)\n",
        "            profiler.profile_model(\n",
        "                                    model=task.model,\n",
        "                                    input_data=input_copy,\n",
        "                                    node=node,           # ← pass the actual Node object\n",
        "                                    task_id=task.task_id\n",
        "                                )\n",
        "\n",
        "            time.sleep(0.05)  # Prevent congestion\n",
        "\n",
        "    # profiler.print_profile_db()\n",
        "\n",
        "    # **Step 7: Populate Task Profile Records**\n",
        "    for task in tasks:\n",
        "        task.populate_profile_records()\n",
        "\n",
        "    # **Step 8: Create Taskset and Run Evaluation**\n",
        "    taskset = Taskset(tasks=tasks, available_nodes=nodes)\n",
        "\n",
        "    for task in taskset.tasks:\n",
        "        task.print_stage_allocations()\n",
        "\n",
        "    evaluator = Evaluator(taskset, profiler)\n",
        "    evaluator.run_naive_execution()\n",
        "    evaluator.run_parallel_execution()\n",
        "    evaluator.compare_outputs()\n",
        "    evaluator.analyze_speedup_throughput()\n",
        "\n",
        "    # **Step 9: Clean Up**\n",
        "    for node in nodes:\n",
        "        node.stop()\n",
        "\n",
        "    return evaluator\n"
      ],
      "metadata": {
        "id": "Rebv19aNomih"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# run_experiment(k=2, heavy_light_ratio=(3, 2), num_tasks=15)"
      ],
      "metadata": {
        "id": "DX1Gg0ky21VF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# run_experiment(k=2, heavy_light_ratio=(10, 5), num_tasks=15)"
      ],
      "metadata": {
        "id": "fL6vF7Ia9LRG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import matplotlib.pyplot as plt\n",
        "\n",
        "# def run_multiple_experiments(k: int, num_tasks: int = 25):\n",
        "#     \"\"\"\n",
        "#     Runs experiments for different heavy-to-light ratios from 0.1 to 0.9 and plots results.\n",
        "#     \"\"\"\n",
        "#     ratios = [round(i / 10, 1) for i in range(1, 10)]  # 0.1 to 0.9\n",
        "#     speedups = []\n",
        "#     throughputs = []\n",
        "#     makespans = []\n",
        "\n",
        "#     for ratio in ratios:\n",
        "#         heavy_count = int(num_tasks * ratio)\n",
        "#         light_count = num_tasks - heavy_count\n",
        "#         print(f\"Running experiment with {heavy_count} heavy and {light_count} light tasks.\")\n",
        "\n",
        "#         evaluator = run_experiment(k=k, heavy_light_ratio=(heavy_count, light_count), num_tasks=num_tasks)\n",
        "\n",
        "#         # Debugging outputs\n",
        "#         print(f\"Speedup for ratio {ratio}: {evaluator.speedup_makespan:.2f}\")\n",
        "#         print(f\"Throughput for ratio {ratio}: {evaluator.throughput_makespan:.2f}\")\n",
        "#         print(f\"Makespan for ratio {ratio}: {evaluator.parallel_makespan:.2f}\")\n",
        "\n",
        "#         speedups.append(evaluator.speedup_makespan)\n",
        "#         throughputs.append(evaluator.throughput_makespan)\n",
        "#         makespans.append(evaluator.parallel_makespan)\n",
        "\n",
        "\n",
        "#     print(speedups,throughputs)\n",
        "#     # Plot results\n",
        "#     plt.figure(figsize=(12, 4))\n",
        "\n",
        "#     # Speedup Plot\n",
        "#     plt.subplot(1, 3, 1)\n",
        "#     plt.plot(ratios, speedups, marker='o', linestyle='-', label='Speedup')\n",
        "#     plt.xlabel(\"Heavy Task Ratio\")\n",
        "#     plt.ylabel(\"Speedup\")\n",
        "#     plt.title(\"Speedup vs Heavy Task Ratio\")\n",
        "#     plt.legend()\n",
        "\n",
        "#     # Throughput Plot\n",
        "#     plt.subplot(1, 3, 2)\n",
        "#     plt.plot(ratios, throughputs, marker='s', linestyle='-', label='Throughput')\n",
        "#     plt.xlabel(\"Heavy Task Ratio\")\n",
        "#     plt.ylabel(\"Throughput (tasks/sec)\")\n",
        "#     plt.title(\"Throughput vs Heavy Task Ratio\")\n",
        "#     plt.legend()\n",
        "\n",
        "#     # Makespan Plot\n",
        "#     plt.subplot(1, 3, 3)\n",
        "#     plt.plot(ratios, makespans, marker='^', linestyle='-', label='Makespan')\n",
        "#     plt.xlabel(\"Heavy Task Ratio\")\n",
        "#     plt.ylabel(\"Makespan (sec)\")\n",
        "#     plt.title(\"Makespan vs Heavy Task Ratio\")\n",
        "#     plt.legend()\n",
        "\n",
        "#     plt.tight_layout()\n",
        "#     plt.show()\n",
        "\n",
        "# # Run experiments with different heavy-to-light ratios and plot results\n",
        "# run_multiple_experiments(k=2, num_tasks=15)"
      ],
      "metadata": {
        "id": "0lfTO3lOAeIf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from collections import defaultdict\n",
        "\n",
        "def run_multiple_experiments(k: int, num_tasks: int = 25):\n",
        "    \"\"\"\n",
        "    Runs experiments for different heavy-to-light ratios from 0.1 to 0.9 and plots results.\n",
        "    \"\"\"\n",
        "    ratios = [round(i / 10, 1) for i in range(1, 10)]  # 0.1 to 0.9\n",
        "    speedups = []\n",
        "    parallel_throughputs = []\n",
        "    naive_throughputs = []\n",
        "    makespans = []\n",
        "    config = defaultdict(dict)\n",
        "\n",
        "    for ratio in ratios:\n",
        "        heavy_count = int(num_tasks * ratio)\n",
        "        light_count = num_tasks - heavy_count\n",
        "        print(f\"Running experiment with {heavy_count} heavy and {light_count} light tasks.\")\n",
        "\n",
        "        evaluator = run_experiment(k=k, heavy_light_ratio=(heavy_count, light_count), num_tasks=num_tasks)\n",
        "\n",
        "        # Debugging outputs\n",
        "        # config = {ratio:[]}\n",
        "        print(f\"Speedup for ratio {ratio}: {evaluator.speedup_makespan:.2f}\")\n",
        "        print(f\"Parallel Throughput for ratio {ratio}: {evaluator.throughput_makespan:.2f}\")\n",
        "        print(f\"Naive Throughput for ratio {ratio}: {num_tasks / evaluator.naive_makespan:.2f}\")\n",
        "        print(f\"Makespan for ratio {ratio}: {evaluator.parallel_makespan:.2f}\")\n",
        "\n",
        "        config[ratio] = {'speedup':evaluator.speedup_makespan}\n",
        "        config[ratio]['parallel_throughput'] = evaluator.throughput_makespan\n",
        "        config[ratio]['naive_throughput'] = num_tasks / evaluator.naive_makespan\n",
        "        config[ratio]['makespan'] = evaluator.parallel_makespan\n",
        "\n",
        "        # Store metrics\n",
        "        speedups.append(evaluator.speedup_makespan)\n",
        "        parallel_throughputs.append(evaluator.throughput_makespan)\n",
        "        naive_throughputs.append(num_tasks / evaluator.naive_makespan)  # Non-parallel throughput\n",
        "        makespans.append(evaluator.parallel_makespan)\n",
        "\n",
        "    print()\n",
        "    print(f\"Speedups: {speedups}\")\n",
        "    print(f\"Parallel Throughputs: {parallel_throughputs}\")\n",
        "    print(f\"Naive Throughputs: {naive_throughputs}\")\n",
        "    print(f\"Makespans: {makespans}\")\n",
        "    print()\n",
        "    print(f\"The best configuration is {max(config.values(), key=lambda x: x['speedup'])}\")\n",
        "    print()\n",
        "\n",
        "    # Plot results\n",
        "    plt.figure(figsize=(12, 4))\n",
        "\n",
        "    # Speedup Plot\n",
        "    plt.subplot(1, 3, 1)\n",
        "    plt.plot(ratios, speedups, marker='o', linestyle='-', label='Speedup')\n",
        "    plt.xlabel(\"Heavy Task Ratio\")\n",
        "    plt.ylabel(\"Speedup\")\n",
        "    plt.title(\"Speedup vs Heavy Task Ratio\")\n",
        "    plt.legend()\n",
        "\n",
        "    # Throughput Plot (Parallel vs Naive)\n",
        "    plt.subplot(1, 3, 2)\n",
        "    plt.plot(ratios, parallel_throughputs, marker='s', linestyle='-', label='Parallel Throughput')\n",
        "    plt.plot(ratios, naive_throughputs, marker='^', linestyle='--', label='Naive Throughput', color='orange')\n",
        "    plt.xlabel(\"Heavy Task Ratio\")\n",
        "    plt.ylabel(\"Throughput (tasks/sec)\")\n",
        "    plt.title(\"Throughput Comparison\")\n",
        "    plt.legend()\n",
        "\n",
        "    # Makespan Plot\n",
        "    plt.subplot(1, 3, 3)\n",
        "    plt.plot(ratios, makespans, marker='d', linestyle='-', label='Makespan')\n",
        "    plt.xlabel(\"Heavy Task Ratio\")\n",
        "    plt.ylabel(\"Makespan (sec)\")\n",
        "    plt.title(\"Makespan vs Heavy Task Ratio\")\n",
        "    plt.legend()\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Run experiments with different heavy-to-light ratios and plot results\n",
        "run_multiple_experiments(k=2, num_tasks=15)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "lyTF3bbN2yMn",
        "outputId": "66d6ef21-3e00-405d-e328-01ea29c3f5b9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running experiment with 1 heavy and 14 light tasks.\n",
            "Seed set to 42\n",
            "Using 2 nodes: [Node(CPU-0, cpus=(0,), gpu=None), Node(CPU-1, cpus=(1,), gpu=None)]\n",
            "Creating 1 heavy tasks and 14 light tasks.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Profiler] Starting profiling for Task 'heavy_1' on CPU-0 (device=cpu).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/autograd/profiler.py:263: UserWarning: CUDA is not available, disabling CUDA profiling\n",
            "  warn(\"CUDA is not available, disabling CUDA profiling\")\n",
            "<ipython-input-1-3ff353520735>:409: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
            "  df = pd.concat([df, pd.DataFrame([row])], ignore_index=True)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Profiler] Profiling complete. Data saved to profiling_results.csv.\n",
            "[Profiler] Starting profiling for Task 'heavy_1' on CPU-1 (device=cpu).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/autograd/profiler.py:263: UserWarning: CUDA is not available, disabling CUDA profiling\n",
            "  warn(\"CUDA is not available, disabling CUDA profiling\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Profiler] Profiling complete. Data saved to profiling_results.csv.\n",
            "[Profiler] Starting profiling for Task 'light_1' on CPU-0 (device=cpu).\n",
            "[Profiler] Profiling complete. Data saved to profiling_results.csv.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/autograd/profiler.py:263: UserWarning: CUDA is not available, disabling CUDA profiling\n",
            "  warn(\"CUDA is not available, disabling CUDA profiling\")\n",
            "/usr/local/lib/python3.11/dist-packages/torch/autograd/profiler.py:263: UserWarning: CUDA is not available, disabling CUDA profiling\n",
            "  warn(\"CUDA is not available, disabling CUDA profiling\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Profiler] Starting profiling for Task 'light_1' on CPU-1 (device=cpu).\n",
            "[Profiler] Profiling complete. Data saved to profiling_results.csv.\n",
            "[Profiler] Reused cached profiling data for SimpleCNN on CPU-0.\n",
            "[Profiler] Reused cached profiling data for SimpleCNN on CPU-1.\n",
            "[Profiler] Reused cached profiling data for SimpleCNN on CPU-0.\n",
            "[Profiler] Reused cached profiling data for SimpleCNN on CPU-1.\n",
            "[Profiler] Reused cached profiling data for SimpleCNN on CPU-0.\n",
            "[Profiler] Reused cached profiling data for SimpleCNN on CPU-1.\n",
            "[Profiler] Reused cached profiling data for SimpleCNN on CPU-0.\n",
            "[Profiler] Reused cached profiling data for SimpleCNN on CPU-1.\n",
            "[Profiler] Reused cached profiling data for SimpleCNN on CPU-0.\n",
            "[Profiler] Reused cached profiling data for SimpleCNN on CPU-1.\n",
            "[Profiler] Reused cached profiling data for SimpleCNN on CPU-0.\n",
            "[Profiler] Reused cached profiling data for SimpleCNN on CPU-1.\n",
            "[Profiler] Reused cached profiling data for SimpleCNN on CPU-0.\n",
            "[Profiler] Reused cached profiling data for SimpleCNN on CPU-1.\n",
            "[Profiler] Reused cached profiling data for SimpleCNN on CPU-0.\n",
            "[Profiler] Reused cached profiling data for SimpleCNN on CPU-1.\n",
            "[Profiler] Reused cached profiling data for SimpleCNN on CPU-0.\n",
            "[Profiler] Reused cached profiling data for SimpleCNN on CPU-1.\n",
            "[Profiler] Reused cached profiling data for SimpleCNN on CPU-0.\n",
            "[Profiler] Reused cached profiling data for SimpleCNN on CPU-1.\n",
            "[Profiler] Reused cached profiling data for SimpleCNN on CPU-0.\n",
            "[Profiler] Reused cached profiling data for SimpleCNN on CPU-1.\n",
            "[Profiler] Reused cached profiling data for SimpleCNN on CPU-0.\n",
            "[Profiler] Reused cached profiling data for SimpleCNN on CPU-1.\n",
            "[Profiler] Reused cached profiling data for SimpleCNN on CPU-0.\n",
            "[Profiler] Reused cached profiling data for SimpleCNN on CPU-1.\n",
            "Measured max transfer penalty: 0.000016 s\n",
            "=== Stage Allocations for Task heavy_1 ===\n",
            "Stage heavy_1-stage-1: Node: CPU-0, Layers: x, resnet18_conv1, resnet18_bn1, resnet18_relu, resnet18_maxpool, resnet18_layer1_0_conv1, resnet18_layer1_0_bn1, resnet18_layer1_0_relu, resnet18_layer1_0_conv2, resnet18_layer1_0_bn2, add, resnet18_layer1_0_relu_1, resnet18_layer1_1_conv1, resnet18_layer1_1_bn1, resnet18_layer1_1_relu, resnet18_layer1_1_conv2, resnet18_layer1_1_bn2, add_1, resnet18_layer1_1_relu_1, Deps: []\n",
            "Stage heavy_1-stage-2: Node: CPU-1, Layers: resnet18_layer2_0_conv1, resnet18_layer2_0_bn1, resnet18_layer2_0_relu, resnet18_layer2_0_conv2, resnet18_layer2_0_bn2, resnet18_layer2_0_downsample_0, resnet18_layer2_0_downsample_1, add_2, resnet18_layer2_0_relu_1, resnet18_layer2_1_conv1, resnet18_layer2_1_bn1, resnet18_layer2_1_relu, resnet18_layer2_1_conv2, resnet18_layer2_1_bn2, add_3, resnet18_layer2_1_relu_1, resnet18_layer3_0_conv1, resnet18_layer3_0_bn1, resnet18_layer3_0_relu, resnet18_layer3_0_conv2, resnet18_layer3_0_bn2, resnet18_layer3_0_downsample_0, resnet18_layer3_0_downsample_1, add_4, resnet18_layer3_0_relu_1, resnet18_layer3_1_conv1, resnet18_layer3_1_bn1, resnet18_layer3_1_relu, resnet18_layer3_1_conv2, resnet18_layer3_1_bn2, add_5, resnet18_layer3_1_relu_1, resnet18_layer4_0_conv1, resnet18_layer4_0_bn1, resnet18_layer4_0_relu, resnet18_layer4_0_conv2, resnet18_layer4_0_bn2, resnet18_layer4_0_downsample_0, resnet18_layer4_0_downsample_1, add_6, resnet18_layer4_0_relu_1, resnet18_layer4_1_conv1, resnet18_layer4_1_bn1, resnet18_layer4_1_relu, resnet18_layer4_1_conv2, resnet18_layer4_1_bn2, add_7, resnet18_layer4_1_relu_1, resnet18_avgpool, flatten, resnet18_fc, output, Deps: ['heavy_1-stage-1']\n",
            "=== Stage Allocations for Task light_1 ===\n",
            "Stage light_1-stage-1: Node: CPU-0, Layers: x, conv1, relu, Deps: []\n",
            "Stage light_1-stage-2: Node: CPU-1, Layers: conv2, relu_1, cat, flatten, fc, output, Deps: ['light_1-stage-1']\n",
            "=== Stage Allocations for Task light_2 ===\n",
            "Stage light_2-stage-1: Node: CPU-0, Layers: x, conv1, relu, conv2, relu_1, Deps: []\n",
            "Stage light_2-stage-2: Node: CPU-1, Layers: cat, flatten, fc, output, Deps: ['light_2-stage-1']\n",
            "=== Stage Allocations for Task light_3 ===\n",
            "Stage light_3-stage-1: Node: CPU-0, Layers: x, conv1, relu, conv2, Deps: []\n",
            "Stage light_3-stage-2: Node: CPU-1, Layers: relu_1, cat, flatten, fc, output, Deps: ['light_3-stage-1']\n",
            "=== Stage Allocations for Task light_4 ===\n",
            "Stage light_4-stage-1: Node: CPU-0, Layers: x, conv1, relu, conv2, Deps: []\n",
            "Stage light_4-stage-2: Node: CPU-1, Layers: relu_1, cat, flatten, fc, output, Deps: ['light_4-stage-1']\n",
            "=== Stage Allocations for Task light_5 ===\n",
            "Stage light_5-stage-1: Node: CPU-0, Layers: x, conv1, relu, conv2, Deps: []\n",
            "Stage light_5-stage-2: Node: CPU-1, Layers: relu_1, cat, flatten, fc, output, Deps: ['light_5-stage-1']\n",
            "=== Stage Allocations for Task light_6 ===\n",
            "Stage light_6-stage-1: Node: CPU-0, Layers: x, conv1, relu, conv2, Deps: []\n",
            "Stage light_6-stage-2: Node: CPU-1, Layers: relu_1, cat, flatten, fc, output, Deps: ['light_6-stage-1']\n",
            "=== Stage Allocations for Task light_7 ===\n",
            "Stage light_7-stage-1: Node: CPU-0, Layers: x, conv1, relu, conv2, Deps: []\n",
            "Stage light_7-stage-2: Node: CPU-1, Layers: relu_1, cat, flatten, fc, output, Deps: ['light_7-stage-1']\n",
            "=== Stage Allocations for Task light_8 ===\n",
            "Stage light_8-stage-1: Node: CPU-0, Layers: x, conv1, relu, conv2, Deps: []\n",
            "Stage light_8-stage-2: Node: CPU-1, Layers: relu_1, cat, flatten, fc, output, Deps: ['light_8-stage-1']\n",
            "=== Stage Allocations for Task light_9 ===\n",
            "Stage light_9-stage-1: Node: CPU-0, Layers: x, conv1, relu, conv2, Deps: []\n",
            "Stage light_9-stage-2: Node: CPU-1, Layers: relu_1, cat, flatten, fc, output, Deps: ['light_9-stage-1']\n",
            "=== Stage Allocations for Task light_10 ===\n",
            "Stage light_10-stage-1: Node: CPU-0, Layers: x, conv1, relu, conv2, Deps: []\n",
            "Stage light_10-stage-2: Node: CPU-1, Layers: relu_1, cat, flatten, fc, output, Deps: ['light_10-stage-1']\n",
            "=== Stage Allocations for Task light_11 ===\n",
            "Stage light_11-stage-1: Node: CPU-0, Layers: x, conv1, relu, conv2, Deps: []\n",
            "Stage light_11-stage-2: Node: CPU-1, Layers: relu_1, cat, flatten, fc, output, Deps: ['light_11-stage-1']\n",
            "=== Stage Allocations for Task light_12 ===\n",
            "Stage light_12-stage-1: Node: CPU-0, Layers: x, conv1, relu, conv2, Deps: []\n",
            "Stage light_12-stage-2: Node: CPU-1, Layers: relu_1, cat, flatten, fc, output, Deps: ['light_12-stage-1']\n",
            "=== Stage Allocations for Task light_13 ===\n",
            "Stage light_13-stage-1: Node: CPU-0, Layers: x, conv1, relu, conv2, Deps: []\n",
            "Stage light_13-stage-2: Node: CPU-1, Layers: relu_1, cat, flatten, fc, output, Deps: ['light_13-stage-1']\n",
            "=== Stage Allocations for Task light_14 ===\n",
            "Stage light_14-stage-1: Node: CPU-0, Layers: x, conv1, relu, Deps: []\n",
            "Stage light_14-stage-2: Node: CPU-1, Layers: conv2, relu_1, cat, flatten, fc, output, Deps: ['light_14-stage-1']\n",
            "[Evaluator] Starting Naive Execution.\n",
            "[Evaluator] Task heavy_1: Naive exec time: 0.989565s\n",
            "[Evaluator] Task light_1: Naive exec time: 0.003211s\n",
            "[Evaluator] Task light_2: Naive exec time: 0.005273s\n",
            "[Evaluator] Task light_3: Naive exec time: 0.003936s\n",
            "[Evaluator] Task light_4: Naive exec time: 0.003410s\n",
            "[Evaluator] Task light_5: Naive exec time: 0.002986s\n",
            "[Evaluator] Task light_6: Naive exec time: 0.003266s\n",
            "[Evaluator] Task light_7: Naive exec time: 0.003258s\n",
            "[Evaluator] Task light_8: Naive exec time: 0.003301s\n",
            "[Evaluator] Task light_9: Naive exec time: 0.003356s\n",
            "[Evaluator] Task light_10: Naive exec time: 0.003060s\n",
            "[Evaluator] Task light_11: Naive exec time: 0.003060s\n",
            "[Evaluator] Task light_12: Naive exec time: 0.003023s\n",
            "[Evaluator] Task light_13: Naive exec time: 0.003878s\n",
            "[Evaluator] Task light_14: Naive exec time: 0.003513s\n",
            "[Evaluator] Naive makespan: 1.073592s\n",
            "\n",
            "[Evaluator] Starting Parallel Execution.\n",
            "[Taskset] Starting Task heavy_1\n",
            "[Taskset] Starting Task light_1\n",
            "[Taskset] Starting Task light_2\n",
            "[Taskset] Starting Task light_3\n",
            "[Taskset] Starting Task light_4[Taskset] Starting Task light_5\n",
            "\n",
            "[Taskset] Starting Task light_6\n",
            "[Taskset] Starting Task light_7\n",
            "[Taskset] Starting Task light_8\n",
            "[Taskset] Starting Task light_9\n",
            "[Taskset] Starting Task light_10\n",
            "[Taskset] Starting Task light_11\n",
            "[Taskset] Starting Task light_12\n",
            "[Taskset] Starting Task light_13\n",
            "[Taskset] Starting Task light_14\n",
            "[Stage] heavy_1-stage-1: Executed on CPU-0 in 0.759564 s, Transfer: 0.000326 s.\n",
            "[Stage] light_1-stage-1: Executed on CPU-0 in 0.008098 s, Transfer: 0.000043 s.\n",
            "[Stage] light_2-stage-1: Executed on CPU-0 in 0.008108 s, Transfer: 0.000052 s.\n",
            "[Stage] light_3-stage-1: Executed on CPU-0 in 0.002553 s, Transfer: 0.000035 s.\n",
            "[Stage] light_5-stage-1: Executed on CPU-0 in 0.011697 s, Transfer: 0.000047 s.\n",
            "[Stage] light_4-stage-1: Executed on CPU-0 in 0.002541 s, Transfer: 0.000035 s.\n",
            "[Stage] light_6-stage-1: Executed on CPU-0 in 0.002383 s, Transfer: 0.000030 s.\n",
            "[Stage] light_7-stage-1: Executed on CPU-0 in 0.019402 s, Transfer: 0.000034 s.\n",
            "[Stage] light_8-stage-1: Executed on CPU-0 in 0.005424 s, Transfer: 0.000038 s.\n",
            "[Stage] light_9-stage-1: Executed on CPU-0 in 0.012347 s, Transfer: 0.000039 s.\n",
            "[Stage] light_10-stage-1: Executed on CPU-0 in 0.007612 s, Transfer: 0.000044 s.\n",
            "[Stage] light_11-stage-1: Executed on CPU-0 in 0.038288 s, Transfer: 0.000041 s.\n",
            "[Stage] light_12-stage-1: Executed on CPU-0 in 0.013173 s, Transfer: 0.000036 s.\n",
            "[Stage] light_13-stage-1: Executed on CPU-0 in 0.032057 s, Transfer: 0.000056 s.\n",
            "[Stage] light_14-stage-1: Executed on CPU-0 in 0.018859 s, Transfer: 0.000024 s.\n",
            "[Stage] heavy_1-stage-2: Executed on CPU-1 in 0.924367 s, Transfer: 0.000740 s.\n",
            "[Taskset] Completed Task heavy_1.\n",
            "[Stage] light_1-stage-2: Executed on CPU-1 in 0.005967 s, Transfer: 0.000069 s.\n",
            "[Taskset] Completed Task light_1.\n",
            "[Stage] light_2-stage-2: Executed on CPU-1 in 0.005211 s, Transfer: 0.000043 s.\n",
            "[Taskset] Completed Task light_2.\n",
            "[Stage] light_3-stage-2: Executed on CPU-1 in 0.005864 s, Transfer: 0.000049 s.\n",
            "[Taskset] Completed Task light_3.\n",
            "[Stage] light_5-stage-2: Executed on CPU-1 in 0.003181 s, Transfer: 0.000043 s.\n",
            "[Taskset] Completed Task light_5.\n",
            "[Stage] light_4-stage-2: Executed on CPU-1 in 0.003563 s, Transfer: 0.000046 s.\n",
            "[Taskset] Completed Task light_4.\n",
            "[Stage] light_6-stage-2: Executed on CPU-1 in 0.003366 s, Transfer: 0.000044 s.\n",
            "[Taskset] Completed Task light_6.\n",
            "[Stage] light_7-stage-2: Executed on CPU-1 in 0.003447 s, Transfer: 0.000046 s.\n",
            "[Taskset] Completed Task light_7.\n",
            "[Stage] light_8-stage-2: Executed on CPU-1 in 0.003333 s, Transfer: 0.000041 s.\n",
            "[Taskset] Completed Task light_8.\n",
            "[Stage] light_9-stage-2: Executed on CPU-1 in 0.003350 s, Transfer: 0.000046 s.\n",
            "[Taskset] Completed Task light_9.\n",
            "[Stage] light_10-stage-2: Executed on CPU-1 in 0.002969 s, Transfer: 0.000055 s.\n",
            "[Taskset] Completed Task light_10.\n",
            "[Stage] light_11-stage-2: Executed on CPU-1 in 0.002961 s, Transfer: 0.000043 s.\n",
            "[Taskset] Completed Task light_11.\n",
            "[Stage] light_12-stage-2: Executed on CPU-1 in 0.003025 s, Transfer: 0.000046 s.\n",
            "[Taskset] Completed Task light_12.\n",
            "[Stage] light_13-stage-2: Executed on CPU-1 in 0.002792 s, Transfer: 0.000053 s.\n",
            "[Taskset] Completed Task light_13.\n",
            "[Stage] light_14-stage-2: Executed on CPU-1 in 0.004034 s, Transfer: 0.000045 s.\n",
            "[Taskset] Completed Task light_14.\n",
            "[Evaluator] Parallel makespan: 1.763162s\n",
            "\n",
            "[Evaluator] Comparing Outputs.\n",
            "[Evaluator] Task heavy_1: Outputs match.\n",
            "[Evaluator] Task light_1: Outputs match.\n",
            "[Evaluator] Task light_2: Outputs match.\n",
            "[Evaluator] Task light_3: Outputs match.\n",
            "[Evaluator] Task light_4: Outputs match.\n",
            "[Evaluator] Task light_5: Outputs match.\n",
            "[Evaluator] Task light_6: Outputs match.\n",
            "[Evaluator] Task light_7: Outputs match.\n",
            "[Evaluator] Task light_8: Outputs match.\n",
            "[Evaluator] Task light_9: Outputs match.\n",
            "[Evaluator] Task light_10: Outputs match.\n",
            "[Evaluator] Task light_11: Outputs match.\n",
            "[Evaluator] Task light_12: Outputs match.\n",
            "[Evaluator] Task light_13: Outputs match.\n",
            "[Evaluator] Task light_14: Outputs match.\n",
            "[Evaluator] All outputs match.\n",
            "\n",
            "[Evaluator] Analyzing Speedup and Throughput.\n",
            "\n",
            "--- Sum-of-times ---\n",
            "Naive total: 1.038094s, Parallel total: 25.528578s\n",
            "Speedup (sum-of-times): 0.04x\n",
            "Naive Throughput: 14.45 tasks/s, Parallel Throughput: 0.59 tasks/s\n",
            "\n",
            "--- Makespan ---\n",
            "Naive makespan: 1.073592s, Parallel makespan: 1.763162s\n",
            "Speedup (makespan): 0.61x\n",
            "Naive Throughput (makespan): 13.97 tasks/s, Parallel Throughput (makespan): 8.51 tasks/s\n",
            "\n",
            "--- Task Completion Times ---\n",
            "Task heavy_1: Naive finish: 1.009032s, Parallel finish: 1.701917s\n",
            "Task light_1: Naive finish: 1.013537s, Parallel finish: 1.708672s\n",
            "Task light_2: Naive finish: 1.019747s, Parallel finish: 1.714843s\n",
            "Task light_3: Naive finish: 1.024741s, Parallel finish: 1.721515s\n",
            "Task light_4: Naive finish: 1.029233s, Parallel finish: 1.729294s\n",
            "Task light_5: Naive finish: 1.033256s, Parallel finish: 1.725656s\n",
            "Task light_6: Naive finish: 1.037521s, Parallel finish: 1.732519s\n",
            "Task light_7: Naive finish: 1.041875s, Parallel finish: 1.736112s\n",
            "Task light_8: Naive finish: 1.046216s, Parallel finish: 1.739185s\n",
            "Task light_9: Naive finish: 1.050592s, Parallel finish: 1.743337s\n",
            "Task light_10: Naive finish: 1.054589s, Parallel finish: 1.747097s\n",
            "Task light_11: Naive finish: 1.058583s, Parallel finish: 1.750839s\n",
            "Task light_12: Naive finish: 1.062513s, Parallel finish: 1.754652s\n",
            "Task light_13: Naive finish: 1.067323s, Parallel finish: 1.758144s\n",
            "Task light_14: Naive finish: 1.072829s, Parallel finish: 1.762817s\n",
            "\n",
            "Speedup for ratio 0.1: 0.61\n",
            "Parallel Throughput for ratio 0.1: 8.51\n",
            "Naive Throughput for ratio 0.1: 13.97\n",
            "Makespan for ratio 0.1: 1.76\n",
            "Running experiment with 3 heavy and 12 light tasks.\n",
            "Seed set to 42\n",
            "Using 2 nodes: [Node(CPU-0, cpus=(0,), gpu=None), Node(CPU-1, cpus=(1,), gpu=None)]\n",
            "Creating 3 heavy tasks and 12 light tasks.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
            "  warnings.warn(msg)\n",
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
            "  warnings.warn(msg)\n",
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Profiler] Starting profiling for Task 'heavy_1' on CPU-0 (device=cpu).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/autograd/profiler.py:263: UserWarning: CUDA is not available, disabling CUDA profiling\n",
            "  warn(\"CUDA is not available, disabling CUDA profiling\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Profiler] Profiling complete. Data saved to profiling_results.csv.\n",
            "[Profiler] Starting profiling for Task 'heavy_1' on CPU-1 (device=cpu).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/autograd/profiler.py:263: UserWarning: CUDA is not available, disabling CUDA profiling\n",
            "  warn(\"CUDA is not available, disabling CUDA profiling\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Profiler] Profiling complete. Data saved to profiling_results.csv.\n",
            "[Profiler] Reused cached profiling data for PretrainedResNet18 on CPU-0.\n",
            "[Profiler] Reused cached profiling data for PretrainedResNet18 on CPU-1.\n",
            "[Profiler] Reused cached profiling data for PretrainedResNet18 on CPU-0.\n",
            "[Profiler] Reused cached profiling data for PretrainedResNet18 on CPU-1.\n",
            "[Profiler] Starting profiling for Task 'light_1' on CPU-0 (device=cpu).\n",
            "[Profiler] Profiling complete. Data saved to profiling_results.csv.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/autograd/profiler.py:263: UserWarning: CUDA is not available, disabling CUDA profiling\n",
            "  warn(\"CUDA is not available, disabling CUDA profiling\")\n",
            "/usr/local/lib/python3.11/dist-packages/torch/autograd/profiler.py:263: UserWarning: CUDA is not available, disabling CUDA profiling\n",
            "  warn(\"CUDA is not available, disabling CUDA profiling\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Profiler] Starting profiling for Task 'light_1' on CPU-1 (device=cpu).\n",
            "[Profiler] Profiling complete. Data saved to profiling_results.csv.\n",
            "[Profiler] Reused cached profiling data for SimpleCNN on CPU-0.\n",
            "[Profiler] Reused cached profiling data for SimpleCNN on CPU-1.\n",
            "[Profiler] Reused cached profiling data for SimpleCNN on CPU-0.\n",
            "[Profiler] Reused cached profiling data for SimpleCNN on CPU-1.\n",
            "[Profiler] Reused cached profiling data for SimpleCNN on CPU-0.\n",
            "[Profiler] Reused cached profiling data for SimpleCNN on CPU-1.\n",
            "[Profiler] Reused cached profiling data for SimpleCNN on CPU-0.\n",
            "[Profiler] Reused cached profiling data for SimpleCNN on CPU-1.\n",
            "[Profiler] Reused cached profiling data for SimpleCNN on CPU-0.\n",
            "[Profiler] Reused cached profiling data for SimpleCNN on CPU-1.\n",
            "[Profiler] Reused cached profiling data for SimpleCNN on CPU-0.\n",
            "[Profiler] Reused cached profiling data for SimpleCNN on CPU-1.\n",
            "[Profiler] Reused cached profiling data for SimpleCNN on CPU-0.\n",
            "[Profiler] Reused cached profiling data for SimpleCNN on CPU-1.\n",
            "[Profiler] Reused cached profiling data for SimpleCNN on CPU-0.\n",
            "[Profiler] Reused cached profiling data for SimpleCNN on CPU-1.\n",
            "[Profiler] Reused cached profiling data for SimpleCNN on CPU-0.\n",
            "[Profiler] Reused cached profiling data for SimpleCNN on CPU-1.\n",
            "[Profiler] Reused cached profiling data for SimpleCNN on CPU-0.\n",
            "[Profiler] Reused cached profiling data for SimpleCNN on CPU-1.\n",
            "[Profiler] Reused cached profiling data for SimpleCNN on CPU-0.\n",
            "[Profiler] Reused cached profiling data for SimpleCNN on CPU-1.\n",
            "Measured max transfer penalty: 0.000022 s\n",
            "=== Stage Allocations for Task heavy_1 ===\n",
            "Stage heavy_1-stage-1: Node: CPU-0, Layers: x, resnet18_conv1, resnet18_bn1, resnet18_relu, resnet18_maxpool, resnet18_layer1_0_conv1, resnet18_layer1_0_bn1, resnet18_layer1_0_relu, resnet18_layer1_0_conv2, resnet18_layer1_0_bn2, add, resnet18_layer1_0_relu_1, resnet18_layer1_1_conv1, resnet18_layer1_1_bn1, resnet18_layer1_1_relu, resnet18_layer1_1_conv2, resnet18_layer1_1_bn2, add_1, resnet18_layer1_1_relu_1, Deps: []\n",
            "Stage heavy_1-stage-2: Node: CPU-1, Layers: resnet18_layer2_0_conv1, resnet18_layer2_0_bn1, resnet18_layer2_0_relu, resnet18_layer2_0_conv2, resnet18_layer2_0_bn2, resnet18_layer2_0_downsample_0, resnet18_layer2_0_downsample_1, add_2, resnet18_layer2_0_relu_1, resnet18_layer2_1_conv1, resnet18_layer2_1_bn1, resnet18_layer2_1_relu, resnet18_layer2_1_conv2, resnet18_layer2_1_bn2, add_3, resnet18_layer2_1_relu_1, resnet18_layer3_0_conv1, resnet18_layer3_0_bn1, resnet18_layer3_0_relu, resnet18_layer3_0_conv2, resnet18_layer3_0_bn2, resnet18_layer3_0_downsample_0, resnet18_layer3_0_downsample_1, add_4, resnet18_layer3_0_relu_1, resnet18_layer3_1_conv1, resnet18_layer3_1_bn1, resnet18_layer3_1_relu, resnet18_layer3_1_conv2, resnet18_layer3_1_bn2, add_5, resnet18_layer3_1_relu_1, resnet18_layer4_0_conv1, resnet18_layer4_0_bn1, resnet18_layer4_0_relu, resnet18_layer4_0_conv2, resnet18_layer4_0_bn2, resnet18_layer4_0_downsample_0, resnet18_layer4_0_downsample_1, add_6, resnet18_layer4_0_relu_1, resnet18_layer4_1_conv1, resnet18_layer4_1_bn1, resnet18_layer4_1_relu, resnet18_layer4_1_conv2, resnet18_layer4_1_bn2, add_7, resnet18_layer4_1_relu_1, resnet18_avgpool, flatten, resnet18_fc, output, Deps: ['heavy_1-stage-1']\n",
            "=== Stage Allocations for Task heavy_2 ===\n",
            "Stage heavy_2-stage-1: Node: CPU-0, Layers: x, resnet18_conv1, resnet18_bn1, resnet18_relu, resnet18_maxpool, resnet18_layer1_0_conv1, resnet18_layer1_0_bn1, resnet18_layer1_0_relu, resnet18_layer1_0_conv2, resnet18_layer1_0_bn2, add, resnet18_layer1_0_relu_1, resnet18_layer1_1_conv1, resnet18_layer1_1_bn1, resnet18_layer1_1_relu, resnet18_layer1_1_conv2, resnet18_layer1_1_bn2, add_1, resnet18_layer1_1_relu_1, Deps: []\n",
            "Stage heavy_2-stage-2: Node: CPU-1, Layers: resnet18_layer2_0_conv1, resnet18_layer2_0_bn1, resnet18_layer2_0_relu, resnet18_layer2_0_conv2, resnet18_layer2_0_bn2, resnet18_layer2_0_downsample_0, resnet18_layer2_0_downsample_1, add_2, resnet18_layer2_0_relu_1, resnet18_layer2_1_conv1, resnet18_layer2_1_bn1, resnet18_layer2_1_relu, resnet18_layer2_1_conv2, resnet18_layer2_1_bn2, add_3, resnet18_layer2_1_relu_1, resnet18_layer3_0_conv1, resnet18_layer3_0_bn1, resnet18_layer3_0_relu, resnet18_layer3_0_conv2, resnet18_layer3_0_bn2, resnet18_layer3_0_downsample_0, resnet18_layer3_0_downsample_1, add_4, resnet18_layer3_0_relu_1, resnet18_layer3_1_conv1, resnet18_layer3_1_bn1, resnet18_layer3_1_relu, resnet18_layer3_1_conv2, resnet18_layer3_1_bn2, add_5, resnet18_layer3_1_relu_1, resnet18_layer4_0_conv1, resnet18_layer4_0_bn1, resnet18_layer4_0_relu, resnet18_layer4_0_conv2, resnet18_layer4_0_bn2, resnet18_layer4_0_downsample_0, resnet18_layer4_0_downsample_1, add_6, resnet18_layer4_0_relu_1, resnet18_layer4_1_conv1, resnet18_layer4_1_bn1, resnet18_layer4_1_relu, resnet18_layer4_1_conv2, resnet18_layer4_1_bn2, add_7, resnet18_layer4_1_relu_1, resnet18_avgpool, flatten, resnet18_fc, output, Deps: ['heavy_2-stage-1']\n",
            "=== Stage Allocations for Task heavy_3 ===\n",
            "Stage heavy_3-stage-1: Node: CPU-0, Layers: x, resnet18_conv1, resnet18_bn1, resnet18_relu, resnet18_maxpool, resnet18_layer1_0_conv1, resnet18_layer1_0_bn1, resnet18_layer1_0_relu, resnet18_layer1_0_conv2, resnet18_layer1_0_bn2, add, resnet18_layer1_0_relu_1, resnet18_layer1_1_conv1, resnet18_layer1_1_bn1, resnet18_layer1_1_relu, resnet18_layer1_1_conv2, resnet18_layer1_1_bn2, add_1, resnet18_layer1_1_relu_1, Deps: []\n",
            "Stage heavy_3-stage-2: Node: CPU-1, Layers: resnet18_layer2_0_conv1, resnet18_layer2_0_bn1, resnet18_layer2_0_relu, resnet18_layer2_0_conv2, resnet18_layer2_0_bn2, resnet18_layer2_0_downsample_0, resnet18_layer2_0_downsample_1, add_2, resnet18_layer2_0_relu_1, resnet18_layer2_1_conv1, resnet18_layer2_1_bn1, resnet18_layer2_1_relu, resnet18_layer2_1_conv2, resnet18_layer2_1_bn2, add_3, resnet18_layer2_1_relu_1, resnet18_layer3_0_conv1, resnet18_layer3_0_bn1, resnet18_layer3_0_relu, resnet18_layer3_0_conv2, resnet18_layer3_0_bn2, resnet18_layer3_0_downsample_0, resnet18_layer3_0_downsample_1, add_4, resnet18_layer3_0_relu_1, resnet18_layer3_1_conv1, resnet18_layer3_1_bn1, resnet18_layer3_1_relu, resnet18_layer3_1_conv2, resnet18_layer3_1_bn2, add_5, resnet18_layer3_1_relu_1, resnet18_layer4_0_conv1, resnet18_layer4_0_bn1, resnet18_layer4_0_relu, resnet18_layer4_0_conv2, resnet18_layer4_0_bn2, resnet18_layer4_0_downsample_0, resnet18_layer4_0_downsample_1, add_6, resnet18_layer4_0_relu_1, resnet18_layer4_1_conv1, resnet18_layer4_1_bn1, resnet18_layer4_1_relu, resnet18_layer4_1_conv2, resnet18_layer4_1_bn2, add_7, resnet18_layer4_1_relu_1, resnet18_avgpool, flatten, resnet18_fc, output, Deps: ['heavy_3-stage-1']\n",
            "=== Stage Allocations for Task light_1 ===\n",
            "Stage light_1-stage-1: Node: CPU-0, Layers: x, conv1, relu, conv2, relu_1, cat, flatten, fc, Deps: []\n",
            "Stage light_1-stage-2: Node: CPU-1, Layers: output, Deps: ['light_1-stage-1']\n",
            "=== Stage Allocations for Task light_2 ===\n",
            "Stage light_2-stage-1: Node: CPU-0, Layers: x, conv1, relu, Deps: []\n",
            "Stage light_2-stage-2: Node: CPU-1, Layers: conv2, relu_1, cat, flatten, fc, output, Deps: ['light_2-stage-1']\n",
            "=== Stage Allocations for Task light_3 ===\n",
            "Stage light_3-stage-1: Node: CPU-0, Layers: x, conv1, relu, conv2, Deps: []\n",
            "Stage light_3-stage-2: Node: CPU-1, Layers: relu_1, cat, flatten, fc, output, Deps: ['light_3-stage-1']\n",
            "=== Stage Allocations for Task light_4 ===\n",
            "Stage light_4-stage-1: Node: CPU-0, Layers: x, conv1, relu, conv2, Deps: []\n",
            "Stage light_4-stage-2: Node: CPU-1, Layers: relu_1, cat, flatten, fc, output, Deps: ['light_4-stage-1']\n",
            "=== Stage Allocations for Task light_5 ===\n",
            "Stage light_5-stage-1: Node: CPU-0, Layers: x, conv1, relu, conv2, Deps: []\n",
            "Stage light_5-stage-2: Node: CPU-1, Layers: relu_1, cat, flatten, fc, output, Deps: ['light_5-stage-1']\n",
            "=== Stage Allocations for Task light_6 ===\n",
            "Stage light_6-stage-1: Node: CPU-0, Layers: x, conv1, relu, conv2, Deps: []\n",
            "Stage light_6-stage-2: Node: CPU-1, Layers: relu_1, cat, flatten, fc, output, Deps: ['light_6-stage-1']\n",
            "=== Stage Allocations for Task light_7 ===\n",
            "Stage light_7-stage-1: Node: CPU-0, Layers: x, conv1, relu, conv2, Deps: []\n",
            "Stage light_7-stage-2: Node: CPU-1, Layers: relu_1, cat, flatten, fc, output, Deps: ['light_7-stage-1']\n",
            "=== Stage Allocations for Task light_8 ===\n",
            "Stage light_8-stage-1: Node: CPU-0, Layers: x, conv1, relu, conv2, Deps: []\n",
            "Stage light_8-stage-2: Node: CPU-1, Layers: relu_1, cat, flatten, fc, output, Deps: ['light_8-stage-1']\n",
            "=== Stage Allocations for Task light_9 ===\n",
            "Stage light_9-stage-1: Node: CPU-0, Layers: x, conv1, relu, conv2, Deps: []\n",
            "Stage light_9-stage-2: Node: CPU-1, Layers: relu_1, cat, flatten, fc, output, Deps: ['light_9-stage-1']\n",
            "=== Stage Allocations for Task light_10 ===\n",
            "Stage light_10-stage-1: Node: CPU-0, Layers: x, conv1, relu, conv2, Deps: []\n",
            "Stage light_10-stage-2: Node: CPU-1, Layers: relu_1, cat, flatten, fc, output, Deps: ['light_10-stage-1']\n",
            "=== Stage Allocations for Task light_11 ===\n",
            "Stage light_11-stage-1: Node: CPU-0, Layers: x, conv1, relu, conv2, Deps: []\n",
            "Stage light_11-stage-2: Node: CPU-1, Layers: relu_1, cat, flatten, fc, output, Deps: ['light_11-stage-1']\n",
            "=== Stage Allocations for Task light_12 ===\n",
            "Stage light_12-stage-1: Node: CPU-0, Layers: x, conv1, relu, conv2, Deps: []\n",
            "Stage light_12-stage-2: Node: CPU-1, Layers: relu_1, cat, flatten, fc, output, Deps: ['light_12-stage-1']\n",
            "[Evaluator] Starting Naive Execution.\n",
            "[Evaluator] Task heavy_1: Naive exec time: 0.863898s\n",
            "[Evaluator] Task heavy_2: Naive exec time: 0.874932s\n",
            "[Evaluator] Task heavy_3: Naive exec time: 0.876954s\n",
            "[Evaluator] Task light_1: Naive exec time: 0.003468s\n",
            "[Evaluator] Task light_2: Naive exec time: 0.002982s\n",
            "[Evaluator] Task light_3: Naive exec time: 0.002958s\n",
            "[Evaluator] Task light_4: Naive exec time: 0.002756s\n",
            "[Evaluator] Task light_5: Naive exec time: 0.002809s\n",
            "[Evaluator] Task light_6: Naive exec time: 0.002654s\n",
            "[Evaluator] Task light_7: Naive exec time: 0.002756s\n",
            "[Evaluator] Task light_8: Naive exec time: 0.003010s\n",
            "[Evaluator] Task light_9: Naive exec time: 0.002546s\n",
            "[Evaluator] Task light_10: Naive exec time: 0.002527s\n",
            "[Evaluator] Task light_11: Naive exec time: 0.002525s\n",
            "[Evaluator] Task light_12: Naive exec time: 0.002865s\n",
            "[Evaluator] Naive makespan: 2.670701s\n",
            "\n",
            "[Evaluator] Starting Parallel Execution.\n",
            "[Taskset] Starting Task heavy_1[Taskset] Starting Task heavy_2\n",
            "\n",
            "[Taskset] Starting Task heavy_3\n",
            "[Taskset] Starting Task light_1\n",
            "[Taskset] Starting Task light_2\n",
            "[Taskset] Starting Task light_3\n",
            "[Taskset] Starting Task light_4\n",
            "[Taskset] Starting Task light_5\n",
            "[Taskset] Starting Task light_6\n",
            "[Taskset] Starting Task light_7\n",
            "[Taskset] Starting Task light_8\n",
            "[Taskset] Starting Task light_9\n",
            "[Taskset] Starting Task light_10\n",
            "[Taskset] Starting Task light_11\n",
            "[Taskset] Starting Task light_12\n",
            "[Stage] heavy_2-stage-1: Executed on CPU-0 in 0.441029 s, Transfer: 0.000260 s.\n",
            "[Stage] heavy_1-stage-1: Executed on CPU-0 in 0.698709 s, Transfer: 0.000314 s.\n",
            "[Stage] heavy_2-stage-2: Executed on CPU-1 in 0.886524 s, Transfer: 0.000880 s.\n",
            "[Taskset] Completed Task heavy_2.\n",
            "[Stage] heavy_3-stage-1: Executed on CPU-0 in 0.818007 s, Transfer: 0.000349 s.\n",
            "[Stage] light_1-stage-1: Executed on CPU-0 in 0.012844 s, Transfer: 0.000100 s.\n",
            "[Stage] light_2-stage-1: Executed on CPU-0 in 0.001421 s, Transfer: 0.000025 s.\n",
            "[Stage] light_3-stage-1: Executed on CPU-0 in 0.005712 s, Transfer: 0.000034 s.\n",
            "[Stage] light_4-stage-1: Executed on CPU-0 in 0.002223 s, Transfer: 0.000024 s.\n",
            "[Stage] light_5-stage-1: Executed on CPU-0 in 0.002637 s, Transfer: 0.000030 s.\n",
            "[Stage] light_6-stage-1: Executed on CPU-0 in 0.002443 s, Transfer: 0.000027 s.\n",
            "[Stage] light_7-stage-1: Executed on CPU-0 in 0.002437 s, Transfer: 0.000026 s.\n",
            "[Stage] light_8-stage-1: Executed on CPU-0 in 0.002857 s, Transfer: 0.000032 s.\n",
            "[Stage] light_9-stage-1: Executed on CPU-0 in 0.002557 s, Transfer: 0.000029 s.\n",
            "[Stage] light_10-stage-1: Executed on CPU-0 in 0.002455 s, Transfer: 0.000027 s.\n",
            "[Stage] light_11-stage-1: Executed on CPU-0 in 0.002744 s, Transfer: 0.000040 s.\n",
            "[Stage] light_12-stage-1: Executed on CPU-0 in 0.002478 s, Transfer: 0.000027 s.\n",
            "[Stage] heavy_1-stage-2: Executed on CPU-1 in 0.944283 s, Transfer: 0.000819 s.\n",
            "[Taskset] Completed Task heavy_1.\n",
            "[Stage] heavy_3-stage-2: Executed on CPU-1 in 0.800587 s, Transfer: 0.000724 s.\n",
            "[Stage] light_1-stage-2: Executed on CPU-1 in 0.000105 s, Transfer: 0.000009 s.[Taskset] Completed Task heavy_3.\n",
            "\n",
            "[Taskset] Completed Task light_1.\n",
            "[Stage] light_2-stage-2: Executed on CPU-1 in 0.011224 s, Transfer: 0.000058 s.\n",
            "[Stage] light_3-stage-2: Executed on CPU-1 in 0.001929 s, Transfer: 0.000037 s.\n",
            "[Taskset] Completed Task light_3.\n",
            "[Stage] light_4-stage-2: Executed on CPU-1 in 0.002631 s, Transfer: 0.000061 s.\n",
            "[Taskset] Completed Task light_2.\n",
            "[Taskset] Completed Task light_4.\n",
            "[Stage] light_5-stage-2: Executed on CPU-1 in 0.007886 s, Transfer: 0.000036 s.\n",
            "[Stage] light_6-stage-2: Executed on CPU-1 in 0.001774 s, Transfer: 0.000036 s.\n",
            "[Stage] light_7-stage-2: Executed on CPU-1 in 0.001726 s, Transfer: 0.000032 s.\n",
            "[Stage] light_8-stage-2: Executed on CPU-1 in 0.001646 s, Transfer: 0.000031 s.\n",
            "[Stage] light_9-stage-2: Executed on CPU-1 in 0.001685 s, Transfer: 0.000032 s.\n",
            "[Taskset] Completed Task light_7.\n",
            "[Taskset] Completed Task light_8.\n",
            "[Taskset] Completed Task light_6.\n",
            "[Taskset] Completed Task light_5.\n",
            "[Taskset] Completed Task light_9.\n",
            "[Stage] light_10-stage-2: Executed on CPU-1 in 0.018579 s, Transfer: 0.000050 s.\n",
            "[Taskset] Completed Task light_10.\n",
            "[Stage] light_12-stage-2: Executed on CPU-1 in 0.002874 s, Transfer: 0.000043 s.\n",
            "[Stage] light_11-stage-2: Executed on CPU-1 in 0.002002 s, Transfer: 0.000040 s.\n",
            "[Taskset] Completed Task light_12.\n",
            "[Taskset] Completed Task light_11.\n",
            "[Evaluator] Parallel makespan: 3.159544s\n",
            "\n",
            "[Evaluator] Comparing Outputs.\n",
            "[Evaluator] Task heavy_1: Outputs match.\n",
            "[Evaluator] Task heavy_2: Outputs match.\n",
            "[Evaluator] Task heavy_3: Outputs match.\n",
            "[Evaluator] Task light_1: Outputs match.\n",
            "[Evaluator] Task light_2: Outputs match.\n",
            "[Evaluator] Task light_3: Outputs match.\n",
            "[Evaluator] Task light_4: Outputs match.\n",
            "[Evaluator] Task light_5: Outputs match.\n",
            "[Evaluator] Task light_6: Outputs match.\n",
            "[Evaluator] Task light_7: Outputs match.\n",
            "[Evaluator] Task light_8: Outputs match.\n",
            "[Evaluator] Task light_9: Outputs match.\n",
            "[Evaluator] Task light_10: Outputs match.\n",
            "[Evaluator] Task light_11: Outputs match.\n",
            "[Evaluator] Task light_12: Outputs match.\n",
            "[Evaluator] All outputs match.\n",
            "\n",
            "[Evaluator] Analyzing Speedup and Throughput.\n",
            "\n",
            "--- Sum-of-times ---\n",
            "Naive total: 2.649640s, Parallel total: 44.137251s\n",
            "Speedup (sum-of-times): 0.06x\n",
            "Naive Throughput: 5.66 tasks/s, Parallel Throughput: 0.34 tasks/s\n",
            "\n",
            "--- Makespan ---\n",
            "Naive makespan: 2.670701s, Parallel makespan: 3.159544s\n",
            "Speedup (makespan): 0.85x\n",
            "Naive Throughput (makespan): 5.62 tasks/s, Parallel Throughput (makespan): 4.75 tasks/s\n",
            "\n",
            "--- Task Completion Times ---\n",
            "Task heavy_1: Naive finish: 0.871990s, Parallel finish: 2.284329s\n",
            "Task heavy_2: Naive finish: 1.748832s, Parallel finish: 1.332746s\n",
            "Task heavy_3: Naive finish: 2.627659s, Parallel finish: 3.090004s\n",
            "Task light_1: Naive finish: 2.631546s, Parallel finish: 3.099012s\n",
            "Task light_2: Naive finish: 2.634880s, Parallel finish: 3.112688s\n",
            "Task light_3: Naive finish: 2.638211s, Parallel finish: 3.110722s\n",
            "Task light_4: Naive finish: 2.642018s, Parallel finish: 3.112983s\n",
            "Task light_5: Naive finish: 2.645856s, Parallel finish: 3.137298s\n",
            "Task light_6: Naive finish: 2.649560s, Parallel finish: 3.137251s\n",
            "Task light_7: Naive finish: 2.653476s, Parallel finish: 3.128080s\n",
            "Task light_8: Naive finish: 2.656849s, Parallel finish: 3.128677s\n",
            "Task light_9: Naive finish: 2.660364s, Parallel finish: 3.137344s\n",
            "Task light_10: Naive finish: 2.663862s, Parallel finish: 3.146602s\n",
            "Task light_11: Naive finish: 2.667370s, Parallel finish: 3.159284s\n",
            "Task light_12: Naive finish: 2.670600s, Parallel finish: 3.158578s\n",
            "\n",
            "Speedup for ratio 0.2: 0.85\n",
            "Parallel Throughput for ratio 0.2: 4.75\n",
            "Naive Throughput for ratio 0.2: 5.62\n",
            "Makespan for ratio 0.2: 3.16\n",
            "Running experiment with 4 heavy and 11 light tasks.\n",
            "Seed set to 42\n",
            "Using 2 nodes: [Node(CPU-0, cpus=(0,), gpu=None), Node(CPU-1, cpus=(1,), gpu=None)]\n",
            "Creating 4 heavy tasks and 11 light tasks.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
            "  warnings.warn(msg)\n",
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
            "  warnings.warn(msg)\n",
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
            "  warnings.warn(msg)\n",
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Profiler] Starting profiling for Task 'heavy_1' on CPU-0 (device=cpu).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/autograd/profiler.py:263: UserWarning: CUDA is not available, disabling CUDA profiling\n",
            "  warn(\"CUDA is not available, disabling CUDA profiling\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Profiler] Profiling complete. Data saved to profiling_results.csv.\n",
            "[Profiler] Starting profiling for Task 'heavy_1' on CPU-1 (device=cpu).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/autograd/profiler.py:263: UserWarning: CUDA is not available, disabling CUDA profiling\n",
            "  warn(\"CUDA is not available, disabling CUDA profiling\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Profiler] Profiling complete. Data saved to profiling_results.csv.\n",
            "[Profiler] Reused cached profiling data for PretrainedResNet18 on CPU-0.\n",
            "[Profiler] Reused cached profiling data for PretrainedResNet18 on CPU-1.\n",
            "[Profiler] Reused cached profiling data for PretrainedResNet18 on CPU-0.\n",
            "[Profiler] Reused cached profiling data for PretrainedResNet18 on CPU-1.\n",
            "[Profiler] Reused cached profiling data for PretrainedResNet18 on CPU-0.\n",
            "[Profiler] Reused cached profiling data for PretrainedResNet18 on CPU-1.\n",
            "[Profiler] Starting profiling for Task 'light_1' on CPU-0 (device=cpu).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/autograd/profiler.py:263: UserWarning: CUDA is not available, disabling CUDA profiling\n",
            "  warn(\"CUDA is not available, disabling CUDA profiling\")\n",
            "/usr/local/lib/python3.11/dist-packages/torch/autograd/profiler.py:263: UserWarning: CUDA is not available, disabling CUDA profiling\n",
            "  warn(\"CUDA is not available, disabling CUDA profiling\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Profiler] Profiling complete. Data saved to profiling_results.csv.\n",
            "[Profiler] Starting profiling for Task 'light_1' on CPU-1 (device=cpu).\n",
            "[Profiler] Profiling complete. Data saved to profiling_results.csv.\n",
            "[Profiler] Reused cached profiling data for SimpleCNN on CPU-0.\n",
            "[Profiler] Reused cached profiling data for SimpleCNN on CPU-1.\n",
            "[Profiler] Reused cached profiling data for SimpleCNN on CPU-0.\n",
            "[Profiler] Reused cached profiling data for SimpleCNN on CPU-1.\n",
            "[Profiler] Reused cached profiling data for SimpleCNN on CPU-0.\n",
            "[Profiler] Reused cached profiling data for SimpleCNN on CPU-1.\n",
            "[Profiler] Reused cached profiling data for SimpleCNN on CPU-0.\n",
            "[Profiler] Reused cached profiling data for SimpleCNN on CPU-1.\n",
            "[Profiler] Reused cached profiling data for SimpleCNN on CPU-0.\n",
            "[Profiler] Reused cached profiling data for SimpleCNN on CPU-1.\n",
            "[Profiler] Reused cached profiling data for SimpleCNN on CPU-0.\n",
            "[Profiler] Reused cached profiling data for SimpleCNN on CPU-1.\n",
            "[Profiler] Reused cached profiling data for SimpleCNN on CPU-0.\n",
            "[Profiler] Reused cached profiling data for SimpleCNN on CPU-1.\n",
            "[Profiler] Reused cached profiling data for SimpleCNN on CPU-0.\n",
            "[Profiler] Reused cached profiling data for SimpleCNN on CPU-1.\n",
            "[Profiler] Reused cached profiling data for SimpleCNN on CPU-0.\n",
            "[Profiler] Reused cached profiling data for SimpleCNN on CPU-1.\n",
            "[Profiler] Reused cached profiling data for SimpleCNN on CPU-0.\n",
            "[Profiler] Reused cached profiling data for SimpleCNN on CPU-1.\n",
            "Measured max transfer penalty: 0.000068 s\n",
            "=== Stage Allocations for Task heavy_1 ===\n",
            "Stage heavy_1-stage-1: Node: CPU-0, Layers: x, resnet18_conv1, resnet18_bn1, resnet18_relu, resnet18_maxpool, resnet18_layer1_0_conv1, resnet18_layer1_0_bn1, resnet18_layer1_0_relu, resnet18_layer1_0_conv2, resnet18_layer1_0_bn2, add, resnet18_layer1_0_relu_1, resnet18_layer1_1_conv1, resnet18_layer1_1_bn1, resnet18_layer1_1_relu, resnet18_layer1_1_conv2, resnet18_layer1_1_bn2, add_1, resnet18_layer1_1_relu_1, Deps: []\n",
            "Stage heavy_1-stage-2: Node: CPU-1, Layers: resnet18_layer2_0_conv1, resnet18_layer2_0_bn1, resnet18_layer2_0_relu, resnet18_layer2_0_conv2, resnet18_layer2_0_bn2, resnet18_layer2_0_downsample_0, resnet18_layer2_0_downsample_1, add_2, resnet18_layer2_0_relu_1, resnet18_layer2_1_conv1, resnet18_layer2_1_bn1, resnet18_layer2_1_relu, resnet18_layer2_1_conv2, resnet18_layer2_1_bn2, add_3, resnet18_layer2_1_relu_1, resnet18_layer3_0_conv1, resnet18_layer3_0_bn1, resnet18_layer3_0_relu, resnet18_layer3_0_conv2, resnet18_layer3_0_bn2, resnet18_layer3_0_downsample_0, resnet18_layer3_0_downsample_1, add_4, resnet18_layer3_0_relu_1, resnet18_layer3_1_conv1, resnet18_layer3_1_bn1, resnet18_layer3_1_relu, resnet18_layer3_1_conv2, resnet18_layer3_1_bn2, add_5, resnet18_layer3_1_relu_1, resnet18_layer4_0_conv1, resnet18_layer4_0_bn1, resnet18_layer4_0_relu, resnet18_layer4_0_conv2, resnet18_layer4_0_bn2, resnet18_layer4_0_downsample_0, resnet18_layer4_0_downsample_1, add_6, resnet18_layer4_0_relu_1, resnet18_layer4_1_conv1, resnet18_layer4_1_bn1, resnet18_layer4_1_relu, resnet18_layer4_1_conv2, resnet18_layer4_1_bn2, add_7, resnet18_layer4_1_relu_1, resnet18_avgpool, flatten, resnet18_fc, output, Deps: ['heavy_1-stage-1']\n",
            "=== Stage Allocations for Task heavy_2 ===\n",
            "Stage heavy_2-stage-1: Node: CPU-0, Layers: x, resnet18_conv1, resnet18_bn1, resnet18_relu, resnet18_maxpool, resnet18_layer1_0_conv1, resnet18_layer1_0_bn1, resnet18_layer1_0_relu, resnet18_layer1_0_conv2, resnet18_layer1_0_bn2, add, resnet18_layer1_0_relu_1, resnet18_layer1_1_conv1, resnet18_layer1_1_bn1, resnet18_layer1_1_relu, resnet18_layer1_1_conv2, resnet18_layer1_1_bn2, add_1, resnet18_layer1_1_relu_1, Deps: []\n",
            "Stage heavy_2-stage-2: Node: CPU-1, Layers: resnet18_layer2_0_conv1, resnet18_layer2_0_bn1, resnet18_layer2_0_relu, resnet18_layer2_0_conv2, resnet18_layer2_0_bn2, resnet18_layer2_0_downsample_0, resnet18_layer2_0_downsample_1, add_2, resnet18_layer2_0_relu_1, resnet18_layer2_1_conv1, resnet18_layer2_1_bn1, resnet18_layer2_1_relu, resnet18_layer2_1_conv2, resnet18_layer2_1_bn2, add_3, resnet18_layer2_1_relu_1, resnet18_layer3_0_conv1, resnet18_layer3_0_bn1, resnet18_layer3_0_relu, resnet18_layer3_0_conv2, resnet18_layer3_0_bn2, resnet18_layer3_0_downsample_0, resnet18_layer3_0_downsample_1, add_4, resnet18_layer3_0_relu_1, resnet18_layer3_1_conv1, resnet18_layer3_1_bn1, resnet18_layer3_1_relu, resnet18_layer3_1_conv2, resnet18_layer3_1_bn2, add_5, resnet18_layer3_1_relu_1, resnet18_layer4_0_conv1, resnet18_layer4_0_bn1, resnet18_layer4_0_relu, resnet18_layer4_0_conv2, resnet18_layer4_0_bn2, resnet18_layer4_0_downsample_0, resnet18_layer4_0_downsample_1, add_6, resnet18_layer4_0_relu_1, resnet18_layer4_1_conv1, resnet18_layer4_1_bn1, resnet18_layer4_1_relu, resnet18_layer4_1_conv2, resnet18_layer4_1_bn2, add_7, resnet18_layer4_1_relu_1, resnet18_avgpool, flatten, resnet18_fc, output, Deps: ['heavy_2-stage-1']\n",
            "=== Stage Allocations for Task heavy_3 ===\n",
            "Stage heavy_3-stage-1: Node: CPU-0, Layers: x, resnet18_conv1, resnet18_bn1, resnet18_relu, resnet18_maxpool, resnet18_layer1_0_conv1, resnet18_layer1_0_bn1, resnet18_layer1_0_relu, resnet18_layer1_0_conv2, resnet18_layer1_0_bn2, add, resnet18_layer1_0_relu_1, resnet18_layer1_1_conv1, resnet18_layer1_1_bn1, resnet18_layer1_1_relu, resnet18_layer1_1_conv2, resnet18_layer1_1_bn2, add_1, resnet18_layer1_1_relu_1, Deps: []\n",
            "Stage heavy_3-stage-2: Node: CPU-1, Layers: resnet18_layer2_0_conv1, resnet18_layer2_0_bn1, resnet18_layer2_0_relu, resnet18_layer2_0_conv2, resnet18_layer2_0_bn2, resnet18_layer2_0_downsample_0, resnet18_layer2_0_downsample_1, add_2, resnet18_layer2_0_relu_1, resnet18_layer2_1_conv1, resnet18_layer2_1_bn1, resnet18_layer2_1_relu, resnet18_layer2_1_conv2, resnet18_layer2_1_bn2, add_3, resnet18_layer2_1_relu_1, resnet18_layer3_0_conv1, resnet18_layer3_0_bn1, resnet18_layer3_0_relu, resnet18_layer3_0_conv2, resnet18_layer3_0_bn2, resnet18_layer3_0_downsample_0, resnet18_layer3_0_downsample_1, add_4, resnet18_layer3_0_relu_1, resnet18_layer3_1_conv1, resnet18_layer3_1_bn1, resnet18_layer3_1_relu, resnet18_layer3_1_conv2, resnet18_layer3_1_bn2, add_5, resnet18_layer3_1_relu_1, resnet18_layer4_0_conv1, resnet18_layer4_0_bn1, resnet18_layer4_0_relu, resnet18_layer4_0_conv2, resnet18_layer4_0_bn2, resnet18_layer4_0_downsample_0, resnet18_layer4_0_downsample_1, add_6, resnet18_layer4_0_relu_1, resnet18_layer4_1_conv1, resnet18_layer4_1_bn1, resnet18_layer4_1_relu, resnet18_layer4_1_conv2, resnet18_layer4_1_bn2, add_7, resnet18_layer4_1_relu_1, resnet18_avgpool, flatten, resnet18_fc, output, Deps: ['heavy_3-stage-1']\n",
            "=== Stage Allocations for Task heavy_4 ===\n",
            "Stage heavy_4-stage-1: Node: CPU-0, Layers: x, resnet18_conv1, resnet18_bn1, resnet18_relu, resnet18_maxpool, resnet18_layer1_0_conv1, resnet18_layer1_0_bn1, resnet18_layer1_0_relu, resnet18_layer1_0_conv2, resnet18_layer1_0_bn2, add, resnet18_layer1_0_relu_1, resnet18_layer1_1_conv1, resnet18_layer1_1_bn1, resnet18_layer1_1_relu, resnet18_layer1_1_conv2, resnet18_layer1_1_bn2, add_1, resnet18_layer1_1_relu_1, Deps: []\n",
            "Stage heavy_4-stage-2: Node: CPU-1, Layers: resnet18_layer2_0_conv1, resnet18_layer2_0_bn1, resnet18_layer2_0_relu, resnet18_layer2_0_conv2, resnet18_layer2_0_bn2, resnet18_layer2_0_downsample_0, resnet18_layer2_0_downsample_1, add_2, resnet18_layer2_0_relu_1, resnet18_layer2_1_conv1, resnet18_layer2_1_bn1, resnet18_layer2_1_relu, resnet18_layer2_1_conv2, resnet18_layer2_1_bn2, add_3, resnet18_layer2_1_relu_1, resnet18_layer3_0_conv1, resnet18_layer3_0_bn1, resnet18_layer3_0_relu, resnet18_layer3_0_conv2, resnet18_layer3_0_bn2, resnet18_layer3_0_downsample_0, resnet18_layer3_0_downsample_1, add_4, resnet18_layer3_0_relu_1, resnet18_layer3_1_conv1, resnet18_layer3_1_bn1, resnet18_layer3_1_relu, resnet18_layer3_1_conv2, resnet18_layer3_1_bn2, add_5, resnet18_layer3_1_relu_1, resnet18_layer4_0_conv1, resnet18_layer4_0_bn1, resnet18_layer4_0_relu, resnet18_layer4_0_conv2, resnet18_layer4_0_bn2, resnet18_layer4_0_downsample_0, resnet18_layer4_0_downsample_1, add_6, resnet18_layer4_0_relu_1, resnet18_layer4_1_conv1, resnet18_layer4_1_bn1, resnet18_layer4_1_relu, resnet18_layer4_1_conv2, resnet18_layer4_1_bn2, add_7, resnet18_layer4_1_relu_1, resnet18_avgpool, flatten, resnet18_fc, output, Deps: ['heavy_4-stage-1']\n",
            "=== Stage Allocations for Task light_1 ===\n",
            "Stage light_1-stage-1: Node: CPU-0, Layers: x, conv1, relu, conv2, relu_1, cat, flatten, fc, Deps: []\n",
            "Stage light_1-stage-2: Node: CPU-1, Layers: output, Deps: ['light_1-stage-1']\n",
            "=== Stage Allocations for Task light_2 ===\n",
            "Stage light_2-stage-1: Node: CPU-0, Layers: x, conv1, relu, conv2, relu_1, cat, flatten, fc, Deps: []\n",
            "Stage light_2-stage-2: Node: CPU-1, Layers: output, Deps: ['light_2-stage-1']\n",
            "=== Stage Allocations for Task light_3 ===\n",
            "Stage light_3-stage-1: Node: CPU-0, Layers: x, conv1, relu, conv2, relu_1, cat, flatten, fc, Deps: []\n",
            "Stage light_3-stage-2: Node: CPU-1, Layers: output, Deps: ['light_3-stage-1']\n",
            "=== Stage Allocations for Task light_4 ===\n",
            "Stage light_4-stage-1: Node: CPU-0, Layers: x, conv1, relu, conv2, relu_1, cat, flatten, fc, Deps: []\n",
            "Stage light_4-stage-2: Node: CPU-1, Layers: output, Deps: ['light_4-stage-1']\n",
            "=== Stage Allocations for Task light_5 ===\n",
            "Stage light_5-stage-1: Node: CPU-0, Layers: x, conv1, relu, conv2, relu_1, cat, flatten, fc, Deps: []\n",
            "Stage light_5-stage-2: Node: CPU-1, Layers: output, Deps: ['light_5-stage-1']\n",
            "=== Stage Allocations for Task light_6 ===\n",
            "Stage light_6-stage-1: Node: CPU-0, Layers: x, conv1, relu, conv2, relu_1, cat, flatten, Deps: []\n",
            "Stage light_6-stage-2: Node: CPU-1, Layers: fc, output, Deps: ['light_6-stage-1']\n",
            "=== Stage Allocations for Task light_7 ===\n",
            "Stage light_7-stage-1: Node: CPU-0, Layers: x, conv1, relu, conv2, relu_1, Deps: []\n",
            "Stage light_7-stage-2: Node: CPU-1, Layers: cat, flatten, fc, output, Deps: ['light_7-stage-1']\n",
            "=== Stage Allocations for Task light_8 ===\n",
            "Stage light_8-stage-1: Node: CPU-0, Layers: x, conv1, relu, conv2, Deps: []\n",
            "Stage light_8-stage-2: Node: CPU-1, Layers: relu_1, cat, flatten, fc, output, Deps: ['light_8-stage-1']\n",
            "=== Stage Allocations for Task light_9 ===\n",
            "Stage light_9-stage-1: Node: CPU-0, Layers: x, conv1, relu, conv2, Deps: []\n",
            "Stage light_9-stage-2: Node: CPU-1, Layers: relu_1, cat, flatten, fc, output, Deps: ['light_9-stage-1']\n",
            "=== Stage Allocations for Task light_10 ===\n",
            "Stage light_10-stage-1: Node: CPU-0, Layers: x, conv1, relu, conv2, Deps: []\n",
            "Stage light_10-stage-2: Node: CPU-1, Layers: relu_1, cat, flatten, fc, output, Deps: ['light_10-stage-1']\n",
            "=== Stage Allocations for Task light_11 ===\n",
            "Stage light_11-stage-1: Node: CPU-0, Layers: x, conv1, relu, conv2, Deps: []\n",
            "Stage light_11-stage-2: Node: CPU-1, Layers: relu_1, cat, flatten, fc, output, Deps: ['light_11-stage-1']\n",
            "[Evaluator] Starting Naive Execution.\n",
            "[Evaluator] Task heavy_1: Naive exec time: 0.864565s\n",
            "[Evaluator] Task heavy_2: Naive exec time: 0.875103s\n",
            "[Evaluator] Task heavy_3: Naive exec time: 0.873412s\n",
            "[Evaluator] Task heavy_4: Naive exec time: 0.876272s\n",
            "[Evaluator] Task light_1: Naive exec time: 0.003430s\n",
            "[Evaluator] Task light_2: Naive exec time: 0.002719s\n",
            "[Evaluator] Task light_3: Naive exec time: 0.002883s\n",
            "[Evaluator] Task light_4: Naive exec time: 0.002552s\n",
            "[Evaluator] Task light_5: Naive exec time: 0.002494s\n",
            "[Evaluator] Task light_6: Naive exec time: 0.002578s\n",
            "[Evaluator] Task light_7: Naive exec time: 0.003213s\n",
            "[Evaluator] Task light_8: Naive exec time: 0.002506s\n",
            "[Evaluator] Task light_9: Naive exec time: 0.002570s\n",
            "[Evaluator] Task light_10: Naive exec time: 0.002511s\n",
            "[Evaluator] Task light_11: Naive exec time: 0.002903s\n",
            "[Evaluator] Naive makespan: 3.547377s\n",
            "\n",
            "[Evaluator] Starting Parallel Execution.\n",
            "[Taskset] Starting Task heavy_1\n",
            "[Taskset] Starting Task heavy_2\n",
            "[Taskset] Starting Task heavy_3\n",
            "[Taskset] Starting Task heavy_4\n",
            "[Taskset] Starting Task light_1\n",
            "[Taskset] Starting Task light_2\n",
            "[Taskset] Starting Task light_3\n",
            "[Taskset] Starting Task light_4\n",
            "[Taskset] Starting Task light_5\n",
            "[Taskset] Starting Task light_6\n",
            "[Taskset] Starting Task light_7\n",
            "[Taskset] Starting Task light_8\n",
            "[Taskset] Starting Task light_9\n",
            "[Taskset] Starting Task light_10\n",
            "[Taskset] Starting Task light_11\n",
            "[Stage] heavy_1-stage-1: Executed on CPU-0 in 0.508514 s, Transfer: 0.000311 s.\n",
            "[Stage] heavy_2-stage-1: Executed on CPU-0 in 0.918418 s, Transfer: 0.000354 s.\n",
            "[Stage] heavy_1-stage-2: Executed on CPU-1 in 1.393441 s, Transfer: 0.000820 s.\n",
            "[Taskset] Completed Task heavy_1.\n",
            "[Stage] heavy_3-stage-1: Executed on CPU-0 in 0.975963 s, Transfer: 0.000343 s.\n",
            "[Stage] heavy_2-stage-2: Executed on CPU-1 in 1.211780 s, Transfer: 0.000826 s.\n",
            "[Taskset] Completed Task heavy_2.\n",
            "[Stage] heavy_4-stage-1: Executed on CPU-0 in 0.976199 s, Transfer: 0.000385 s.\n",
            "[Stage] light_1-stage-1: Executed on CPU-0 in 0.004921 s, Transfer: 0.000071 s.\n",
            "[Stage] light_2-stage-1: Executed on CPU-0 in 0.008582 s, Transfer: 0.000065 s.\n",
            "[Stage] light_3-stage-1: Executed on CPU-0 in 0.009272 s, Transfer: 0.000062 s.\n",
            "[Stage] light_4-stage-1: Executed on CPU-0 in 0.006522 s, Transfer: 0.000077 s.\n",
            "[Stage] light_5-stage-1: Executed on CPU-0 in 0.010154 s, Transfer: 0.000076 s.\n",
            "[Stage] light_6-stage-1: Executed on CPU-0 in 0.003756 s, Transfer: 0.000068 s.\n",
            "[Stage] light_7-stage-1: Executed on CPU-0 in 0.013367 s, Transfer: 0.000051 s.\n",
            "[Stage] light_8-stage-1: Executed on CPU-0 in 0.002421 s, Transfer: 0.000027 s.\n",
            "[Stage] light_9-stage-1: Executed on CPU-0 in 0.007807 s, Transfer: 0.000038 s.\n",
            "[Stage] light_10-stage-1: Executed on CPU-0 in 0.010211 s, Transfer: 0.000052 s.\n",
            "[Stage] light_11-stage-1: Executed on CPU-0 in 0.010067 s, Transfer: 0.000048 s.\n",
            "[Stage] heavy_3-stage-2: Executed on CPU-1 in 0.901426 s, Transfer: 0.000814 s.\n",
            "[Taskset] Completed Task heavy_3.\n",
            "[Stage] heavy_4-stage-2: Executed on CPU-1 in 0.702533 s, Transfer: 0.000764 s.\n",
            "[Stage] light_1-stage-2: Executed on CPU-1 in 0.000047 s, Transfer: 0.000004 s.\n",
            "[Stage] light_2-stage-2: Executed on CPU-1 in 0.000038 s, Transfer: 0.000004 s.\n",
            "[Stage] light_3-stage-2: Executed on CPU-1 in 0.000035 s, Transfer: 0.000007 s.[Taskset] Completed Task heavy_4.\n",
            "\n",
            "[Taskset] Completed Task light_1.\n",
            "[Taskset] Completed Task light_3.[Taskset] Completed Task light_2.[Stage] light_4-stage-2: Executed on CPU-1 in 0.000048 s, Transfer: 0.000005 s.\n",
            "[Stage] light_5-stage-2: Executed on CPU-1 in 0.000119 s, Transfer: 0.000010 s.\n",
            "[Taskset] Completed Task light_4.\n",
            "\n",
            "\n",
            "[Taskset] Completed Task light_5.\n",
            "[Stage] light_6-stage-2: Executed on CPU-1 in 0.007199 s, Transfer: 0.000021 s.\n",
            "[Taskset] Completed Task light_6.\n",
            "[Stage] light_7-stage-2: Executed on CPU-1 in 0.004006 s, Transfer: 0.000042 s.\n",
            "[Taskset] Completed Task light_7.\n",
            "[Stage] light_8-stage-2: Executed on CPU-1 in 0.003123 s, Transfer: 0.000043 s.\n",
            "[Taskset] Completed Task light_8.\n",
            "[Stage] light_9-stage-2: Executed on CPU-1 in 0.003064 s, Transfer: 0.000047 s.\n",
            "[Taskset] Completed Task light_9.\n",
            "[Stage] light_10-stage-2: Executed on CPU-1 in 0.003039 s, Transfer: 0.000050 s.\n",
            "[Taskset] Completed Task light_10.\n",
            "[Stage] light_11-stage-2: Executed on CPU-1 in 0.003060 s, Transfer: 0.000046 s.\n",
            "[Taskset] Completed Task light_11.\n",
            "[Evaluator] Parallel makespan: 4.765548s\n",
            "\n",
            "[Evaluator] Comparing Outputs.\n",
            "[Evaluator] Task heavy_1: Outputs match.\n",
            "[Evaluator] Task heavy_2: Outputs match.\n",
            "[Evaluator] Task heavy_3: Outputs match.\n",
            "[Evaluator] Task heavy_4: Outputs match.\n",
            "[Evaluator] Task light_1: Outputs match.\n",
            "[Evaluator] Task light_2: Outputs match.\n",
            "[Evaluator] Task light_3: Outputs match.\n",
            "[Evaluator] Task light_4: Outputs match.\n",
            "[Evaluator] Task light_5: Outputs match.\n",
            "[Evaluator] Task light_6: Outputs match.\n",
            "[Evaluator] Task light_7: Outputs match.\n",
            "[Evaluator] Task light_8: Outputs match.\n",
            "[Evaluator] Task light_9: Outputs match.\n",
            "[Evaluator] Task light_10: Outputs match.\n",
            "[Evaluator] Task light_11: Outputs match.\n",
            "[Evaluator] All outputs match.\n",
            "\n",
            "[Evaluator] Analyzing Speedup and Throughput.\n",
            "\n",
            "--- Sum-of-times ---\n",
            "Naive total: 3.519712s, Parallel total: 65.891056s\n",
            "Speedup (sum-of-times): 0.05x\n",
            "Naive Throughput: 4.26 tasks/s, Parallel Throughput: 0.23 tasks/s\n",
            "\n",
            "--- Makespan ---\n",
            "Naive makespan: 3.547377s, Parallel makespan: 4.765548s\n",
            "Speedup (makespan): 0.74x\n",
            "Naive Throughput (makespan): 4.23 tasks/s, Parallel Throughput (makespan): 3.15 tasks/s\n",
            "\n",
            "--- Task Completion Times ---\n",
            "Task heavy_1: Naive finish: 0.877915s, Parallel finish: 1.906578s\n",
            "Task heavy_2: Naive finish: 1.754884s, Parallel finish: 3.119540s\n",
            "Task heavy_3: Naive finish: 2.630195s, Parallel finish: 4.026754s\n",
            "Task heavy_4: Naive finish: 3.508341s, Parallel finish: 4.729671s\n",
            "Task light_1: Naive finish: 3.512185s, Parallel finish: 4.729928s\n",
            "Task light_2: Naive finish: 3.515817s, Parallel finish: 4.739520s\n",
            "Task light_3: Naive finish: 3.519059s, Parallel finish: 4.739431s\n",
            "Task light_4: Naive finish: 3.522532s, Parallel finish: 4.743906s\n",
            "Task light_5: Naive finish: 3.526022s, Parallel finish: 4.744431s\n",
            "Task light_6: Naive finish: 3.529563s, Parallel finish: 4.748188s\n",
            "Task light_7: Naive finish: 3.533069s, Parallel finish: 4.752238s\n",
            "Task light_8: Naive finish: 3.536958s, Parallel finish: 4.755489s\n",
            "Task light_9: Naive finish: 3.540518s, Parallel finish: 4.758662s\n",
            "Task light_10: Naive finish: 3.544022s, Parallel finish: 4.761876s\n",
            "Task light_11: Naive finish: 3.547291s, Parallel finish: 4.764827s\n",
            "\n",
            "Speedup for ratio 0.3: 0.74\n",
            "Parallel Throughput for ratio 0.3: 3.15\n",
            "Naive Throughput for ratio 0.3: 4.23\n",
            "Makespan for ratio 0.3: 4.77\n",
            "Running experiment with 6 heavy and 9 light tasks.\n",
            "Seed set to 42\n",
            "Using 2 nodes: [Node(CPU-0, cpus=(0,), gpu=None), Node(CPU-1, cpus=(1,), gpu=None)]\n",
            "Creating 6 heavy tasks and 9 light tasks.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
            "  warnings.warn(msg)\n",
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
            "  warnings.warn(msg)\n",
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
            "  warnings.warn(msg)\n",
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
            "  warnings.warn(msg)\n",
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
            "  warnings.warn(msg)\n",
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Profiler] Starting profiling for Task 'heavy_1' on CPU-0 (device=cpu).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/autograd/profiler.py:263: UserWarning: CUDA is not available, disabling CUDA profiling\n",
            "  warn(\"CUDA is not available, disabling CUDA profiling\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Profiler] Profiling complete. Data saved to profiling_results.csv.\n",
            "[Profiler] Starting profiling for Task 'heavy_1' on CPU-1 (device=cpu).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/autograd/profiler.py:263: UserWarning: CUDA is not available, disabling CUDA profiling\n",
            "  warn(\"CUDA is not available, disabling CUDA profiling\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Profiler] Profiling complete. Data saved to profiling_results.csv.\n",
            "[Profiler] Reused cached profiling data for PretrainedResNet18 on CPU-0.\n",
            "[Profiler] Reused cached profiling data for PretrainedResNet18 on CPU-1.\n",
            "[Profiler] Reused cached profiling data for PretrainedResNet18 on CPU-0.\n",
            "[Profiler] Reused cached profiling data for PretrainedResNet18 on CPU-1.\n",
            "[Profiler] Reused cached profiling data for PretrainedResNet18 on CPU-0.\n",
            "[Profiler] Reused cached profiling data for PretrainedResNet18 on CPU-1.\n",
            "[Profiler] Reused cached profiling data for PretrainedResNet18 on CPU-0.\n",
            "[Profiler] Reused cached profiling data for PretrainedResNet18 on CPU-1.\n",
            "[Profiler] Reused cached profiling data for PretrainedResNet18 on CPU-0.\n",
            "[Profiler] Reused cached profiling data for PretrainedResNet18 on CPU-1.\n",
            "[Profiler] Starting profiling for Task 'light_1' on CPU-0 (device=cpu).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/autograd/profiler.py:263: UserWarning: CUDA is not available, disabling CUDA profiling\n",
            "  warn(\"CUDA is not available, disabling CUDA profiling\")\n",
            "/usr/local/lib/python3.11/dist-packages/torch/autograd/profiler.py:263: UserWarning: CUDA is not available, disabling CUDA profiling\n",
            "  warn(\"CUDA is not available, disabling CUDA profiling\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Profiler] Profiling complete. Data saved to profiling_results.csv.\n",
            "[Profiler] Starting profiling for Task 'light_1' on CPU-1 (device=cpu).\n",
            "[Profiler] Profiling complete. Data saved to profiling_results.csv.\n",
            "[Profiler] Reused cached profiling data for SimpleCNN on CPU-0.\n",
            "[Profiler] Reused cached profiling data for SimpleCNN on CPU-1.\n",
            "[Profiler] Reused cached profiling data for SimpleCNN on CPU-0.\n",
            "[Profiler] Reused cached profiling data for SimpleCNN on CPU-1.\n",
            "[Profiler] Reused cached profiling data for SimpleCNN on CPU-0.\n",
            "[Profiler] Reused cached profiling data for SimpleCNN on CPU-1.\n",
            "[Profiler] Reused cached profiling data for SimpleCNN on CPU-0.\n",
            "[Profiler] Reused cached profiling data for SimpleCNN on CPU-1.\n",
            "[Profiler] Reused cached profiling data for SimpleCNN on CPU-0.\n",
            "[Profiler] Reused cached profiling data for SimpleCNN on CPU-1.\n",
            "[Profiler] Reused cached profiling data for SimpleCNN on CPU-0.\n",
            "[Profiler] Reused cached profiling data for SimpleCNN on CPU-1.\n",
            "[Profiler] Reused cached profiling data for SimpleCNN on CPU-0.\n",
            "[Profiler] Reused cached profiling data for SimpleCNN on CPU-1.\n",
            "[Profiler] Reused cached profiling data for SimpleCNN on CPU-0.\n",
            "[Profiler] Reused cached profiling data for SimpleCNN on CPU-1.\n",
            "Measured max transfer penalty: 0.000017 s\n",
            "=== Stage Allocations for Task heavy_1 ===\n",
            "Stage heavy_1-stage-1: Node: CPU-0, Layers: x, resnet18_conv1, resnet18_bn1, resnet18_relu, resnet18_maxpool, resnet18_layer1_0_conv1, resnet18_layer1_0_bn1, resnet18_layer1_0_relu, resnet18_layer1_0_conv2, resnet18_layer1_0_bn2, add, resnet18_layer1_0_relu_1, resnet18_layer1_1_conv1, resnet18_layer1_1_bn1, resnet18_layer1_1_relu, resnet18_layer1_1_conv2, resnet18_layer1_1_bn2, add_1, resnet18_layer1_1_relu_1, Deps: []\n",
            "Stage heavy_1-stage-2: Node: CPU-1, Layers: resnet18_layer2_0_conv1, resnet18_layer2_0_bn1, resnet18_layer2_0_relu, resnet18_layer2_0_conv2, resnet18_layer2_0_bn2, resnet18_layer2_0_downsample_0, resnet18_layer2_0_downsample_1, add_2, resnet18_layer2_0_relu_1, resnet18_layer2_1_conv1, resnet18_layer2_1_bn1, resnet18_layer2_1_relu, resnet18_layer2_1_conv2, resnet18_layer2_1_bn2, add_3, resnet18_layer2_1_relu_1, resnet18_layer3_0_conv1, resnet18_layer3_0_bn1, resnet18_layer3_0_relu, resnet18_layer3_0_conv2, resnet18_layer3_0_bn2, resnet18_layer3_0_downsample_0, resnet18_layer3_0_downsample_1, add_4, resnet18_layer3_0_relu_1, resnet18_layer3_1_conv1, resnet18_layer3_1_bn1, resnet18_layer3_1_relu, resnet18_layer3_1_conv2, resnet18_layer3_1_bn2, add_5, resnet18_layer3_1_relu_1, resnet18_layer4_0_conv1, resnet18_layer4_0_bn1, resnet18_layer4_0_relu, resnet18_layer4_0_conv2, resnet18_layer4_0_bn2, resnet18_layer4_0_downsample_0, resnet18_layer4_0_downsample_1, add_6, resnet18_layer4_0_relu_1, resnet18_layer4_1_conv1, resnet18_layer4_1_bn1, resnet18_layer4_1_relu, resnet18_layer4_1_conv2, resnet18_layer4_1_bn2, add_7, resnet18_layer4_1_relu_1, resnet18_avgpool, flatten, resnet18_fc, output, Deps: ['heavy_1-stage-1']\n",
            "=== Stage Allocations for Task heavy_2 ===\n",
            "Stage heavy_2-stage-1: Node: CPU-0, Layers: x, resnet18_conv1, resnet18_bn1, resnet18_relu, resnet18_maxpool, resnet18_layer1_0_conv1, resnet18_layer1_0_bn1, resnet18_layer1_0_relu, resnet18_layer1_0_conv2, resnet18_layer1_0_bn2, add, resnet18_layer1_0_relu_1, resnet18_layer1_1_conv1, resnet18_layer1_1_bn1, resnet18_layer1_1_relu, resnet18_layer1_1_conv2, resnet18_layer1_1_bn2, add_1, resnet18_layer1_1_relu_1, Deps: []\n",
            "Stage heavy_2-stage-2: Node: CPU-1, Layers: resnet18_layer2_0_conv1, resnet18_layer2_0_bn1, resnet18_layer2_0_relu, resnet18_layer2_0_conv2, resnet18_layer2_0_bn2, resnet18_layer2_0_downsample_0, resnet18_layer2_0_downsample_1, add_2, resnet18_layer2_0_relu_1, resnet18_layer2_1_conv1, resnet18_layer2_1_bn1, resnet18_layer2_1_relu, resnet18_layer2_1_conv2, resnet18_layer2_1_bn2, add_3, resnet18_layer2_1_relu_1, resnet18_layer3_0_conv1, resnet18_layer3_0_bn1, resnet18_layer3_0_relu, resnet18_layer3_0_conv2, resnet18_layer3_0_bn2, resnet18_layer3_0_downsample_0, resnet18_layer3_0_downsample_1, add_4, resnet18_layer3_0_relu_1, resnet18_layer3_1_conv1, resnet18_layer3_1_bn1, resnet18_layer3_1_relu, resnet18_layer3_1_conv2, resnet18_layer3_1_bn2, add_5, resnet18_layer3_1_relu_1, resnet18_layer4_0_conv1, resnet18_layer4_0_bn1, resnet18_layer4_0_relu, resnet18_layer4_0_conv2, resnet18_layer4_0_bn2, resnet18_layer4_0_downsample_0, resnet18_layer4_0_downsample_1, add_6, resnet18_layer4_0_relu_1, resnet18_layer4_1_conv1, resnet18_layer4_1_bn1, resnet18_layer4_1_relu, resnet18_layer4_1_conv2, resnet18_layer4_1_bn2, add_7, resnet18_layer4_1_relu_1, resnet18_avgpool, flatten, resnet18_fc, output, Deps: ['heavy_2-stage-1']\n",
            "=== Stage Allocations for Task heavy_3 ===\n",
            "Stage heavy_3-stage-1: Node: CPU-0, Layers: x, resnet18_conv1, resnet18_bn1, resnet18_relu, resnet18_maxpool, resnet18_layer1_0_conv1, resnet18_layer1_0_bn1, resnet18_layer1_0_relu, resnet18_layer1_0_conv2, resnet18_layer1_0_bn2, add, resnet18_layer1_0_relu_1, resnet18_layer1_1_conv1, resnet18_layer1_1_bn1, resnet18_layer1_1_relu, resnet18_layer1_1_conv2, resnet18_layer1_1_bn2, add_1, resnet18_layer1_1_relu_1, Deps: []\n",
            "Stage heavy_3-stage-2: Node: CPU-1, Layers: resnet18_layer2_0_conv1, resnet18_layer2_0_bn1, resnet18_layer2_0_relu, resnet18_layer2_0_conv2, resnet18_layer2_0_bn2, resnet18_layer2_0_downsample_0, resnet18_layer2_0_downsample_1, add_2, resnet18_layer2_0_relu_1, resnet18_layer2_1_conv1, resnet18_layer2_1_bn1, resnet18_layer2_1_relu, resnet18_layer2_1_conv2, resnet18_layer2_1_bn2, add_3, resnet18_layer2_1_relu_1, resnet18_layer3_0_conv1, resnet18_layer3_0_bn1, resnet18_layer3_0_relu, resnet18_layer3_0_conv2, resnet18_layer3_0_bn2, resnet18_layer3_0_downsample_0, resnet18_layer3_0_downsample_1, add_4, resnet18_layer3_0_relu_1, resnet18_layer3_1_conv1, resnet18_layer3_1_bn1, resnet18_layer3_1_relu, resnet18_layer3_1_conv2, resnet18_layer3_1_bn2, add_5, resnet18_layer3_1_relu_1, resnet18_layer4_0_conv1, resnet18_layer4_0_bn1, resnet18_layer4_0_relu, resnet18_layer4_0_conv2, resnet18_layer4_0_bn2, resnet18_layer4_0_downsample_0, resnet18_layer4_0_downsample_1, add_6, resnet18_layer4_0_relu_1, resnet18_layer4_1_conv1, resnet18_layer4_1_bn1, resnet18_layer4_1_relu, resnet18_layer4_1_conv2, resnet18_layer4_1_bn2, add_7, resnet18_layer4_1_relu_1, resnet18_avgpool, flatten, resnet18_fc, output, Deps: ['heavy_3-stage-1']\n",
            "=== Stage Allocations for Task heavy_4 ===\n",
            "Stage heavy_4-stage-1: Node: CPU-0, Layers: x, resnet18_conv1, resnet18_bn1, resnet18_relu, resnet18_maxpool, resnet18_layer1_0_conv1, resnet18_layer1_0_bn1, resnet18_layer1_0_relu, resnet18_layer1_0_conv2, resnet18_layer1_0_bn2, add, resnet18_layer1_0_relu_1, resnet18_layer1_1_conv1, resnet18_layer1_1_bn1, resnet18_layer1_1_relu, resnet18_layer1_1_conv2, resnet18_layer1_1_bn2, add_1, resnet18_layer1_1_relu_1, Deps: []\n",
            "Stage heavy_4-stage-2: Node: CPU-1, Layers: resnet18_layer2_0_conv1, resnet18_layer2_0_bn1, resnet18_layer2_0_relu, resnet18_layer2_0_conv2, resnet18_layer2_0_bn2, resnet18_layer2_0_downsample_0, resnet18_layer2_0_downsample_1, add_2, resnet18_layer2_0_relu_1, resnet18_layer2_1_conv1, resnet18_layer2_1_bn1, resnet18_layer2_1_relu, resnet18_layer2_1_conv2, resnet18_layer2_1_bn2, add_3, resnet18_layer2_1_relu_1, resnet18_layer3_0_conv1, resnet18_layer3_0_bn1, resnet18_layer3_0_relu, resnet18_layer3_0_conv2, resnet18_layer3_0_bn2, resnet18_layer3_0_downsample_0, resnet18_layer3_0_downsample_1, add_4, resnet18_layer3_0_relu_1, resnet18_layer3_1_conv1, resnet18_layer3_1_bn1, resnet18_layer3_1_relu, resnet18_layer3_1_conv2, resnet18_layer3_1_bn2, add_5, resnet18_layer3_1_relu_1, resnet18_layer4_0_conv1, resnet18_layer4_0_bn1, resnet18_layer4_0_relu, resnet18_layer4_0_conv2, resnet18_layer4_0_bn2, resnet18_layer4_0_downsample_0, resnet18_layer4_0_downsample_1, add_6, resnet18_layer4_0_relu_1, resnet18_layer4_1_conv1, resnet18_layer4_1_bn1, resnet18_layer4_1_relu, resnet18_layer4_1_conv2, resnet18_layer4_1_bn2, add_7, resnet18_layer4_1_relu_1, resnet18_avgpool, flatten, resnet18_fc, output, Deps: ['heavy_4-stage-1']\n",
            "=== Stage Allocations for Task heavy_5 ===\n",
            "Stage heavy_5-stage-1: Node: CPU-0, Layers: x, resnet18_conv1, resnet18_bn1, resnet18_relu, resnet18_maxpool, resnet18_layer1_0_conv1, resnet18_layer1_0_bn1, resnet18_layer1_0_relu, resnet18_layer1_0_conv2, resnet18_layer1_0_bn2, add, resnet18_layer1_0_relu_1, resnet18_layer1_1_conv1, resnet18_layer1_1_bn1, resnet18_layer1_1_relu, resnet18_layer1_1_conv2, resnet18_layer1_1_bn2, add_1, resnet18_layer1_1_relu_1, resnet18_layer2_0_conv1, Deps: []\n",
            "Stage heavy_5-stage-2: Node: CPU-1, Layers: resnet18_layer2_0_bn1, resnet18_layer2_0_relu, resnet18_layer2_0_conv2, resnet18_layer2_0_bn2, resnet18_layer2_0_downsample_0, resnet18_layer2_0_downsample_1, add_2, resnet18_layer2_0_relu_1, resnet18_layer2_1_conv1, resnet18_layer2_1_bn1, resnet18_layer2_1_relu, resnet18_layer2_1_conv2, resnet18_layer2_1_bn2, add_3, resnet18_layer2_1_relu_1, resnet18_layer3_0_conv1, resnet18_layer3_0_bn1, resnet18_layer3_0_relu, resnet18_layer3_0_conv2, resnet18_layer3_0_bn2, resnet18_layer3_0_downsample_0, resnet18_layer3_0_downsample_1, add_4, resnet18_layer3_0_relu_1, resnet18_layer3_1_conv1, resnet18_layer3_1_bn1, resnet18_layer3_1_relu, resnet18_layer3_1_conv2, resnet18_layer3_1_bn2, add_5, resnet18_layer3_1_relu_1, resnet18_layer4_0_conv1, resnet18_layer4_0_bn1, resnet18_layer4_0_relu, resnet18_layer4_0_conv2, resnet18_layer4_0_bn2, resnet18_layer4_0_downsample_0, resnet18_layer4_0_downsample_1, add_6, resnet18_layer4_0_relu_1, resnet18_layer4_1_conv1, resnet18_layer4_1_bn1, resnet18_layer4_1_relu, resnet18_layer4_1_conv2, resnet18_layer4_1_bn2, add_7, resnet18_layer4_1_relu_1, resnet18_avgpool, flatten, resnet18_fc, output, Deps: ['heavy_5-stage-1']\n",
            "=== Stage Allocations for Task heavy_6 ===\n",
            "Stage heavy_6-stage-1: Node: CPU-0, Layers: x, resnet18_conv1, resnet18_bn1, resnet18_relu, resnet18_maxpool, resnet18_layer1_0_conv1, resnet18_layer1_0_bn1, resnet18_layer1_0_relu, resnet18_layer1_0_conv2, resnet18_layer1_0_bn2, add, resnet18_layer1_0_relu_1, resnet18_layer1_1_conv1, resnet18_layer1_1_bn1, resnet18_layer1_1_relu, resnet18_layer1_1_conv2, resnet18_layer1_1_bn2, add_1, resnet18_layer1_1_relu_1, Deps: []\n",
            "Stage heavy_6-stage-2: Node: CPU-1, Layers: resnet18_layer2_0_conv1, resnet18_layer2_0_bn1, resnet18_layer2_0_relu, resnet18_layer2_0_conv2, resnet18_layer2_0_bn2, resnet18_layer2_0_downsample_0, resnet18_layer2_0_downsample_1, add_2, resnet18_layer2_0_relu_1, resnet18_layer2_1_conv1, resnet18_layer2_1_bn1, resnet18_layer2_1_relu, resnet18_layer2_1_conv2, resnet18_layer2_1_bn2, add_3, resnet18_layer2_1_relu_1, resnet18_layer3_0_conv1, resnet18_layer3_0_bn1, resnet18_layer3_0_relu, resnet18_layer3_0_conv2, resnet18_layer3_0_bn2, resnet18_layer3_0_downsample_0, resnet18_layer3_0_downsample_1, add_4, resnet18_layer3_0_relu_1, resnet18_layer3_1_conv1, resnet18_layer3_1_bn1, resnet18_layer3_1_relu, resnet18_layer3_1_conv2, resnet18_layer3_1_bn2, add_5, resnet18_layer3_1_relu_1, resnet18_layer4_0_conv1, resnet18_layer4_0_bn1, resnet18_layer4_0_relu, resnet18_layer4_0_conv2, resnet18_layer4_0_bn2, resnet18_layer4_0_downsample_0, resnet18_layer4_0_downsample_1, add_6, resnet18_layer4_0_relu_1, resnet18_layer4_1_conv1, resnet18_layer4_1_bn1, resnet18_layer4_1_relu, resnet18_layer4_1_conv2, resnet18_layer4_1_bn2, add_7, resnet18_layer4_1_relu_1, resnet18_avgpool, flatten, resnet18_fc, output, Deps: ['heavy_6-stage-1']\n",
            "=== Stage Allocations for Task light_1 ===\n",
            "Stage light_1-stage-1: Node: CPU-0, Layers: x, conv1, relu, conv2, relu_1, cat, flatten, fc, Deps: []\n",
            "Stage light_1-stage-2: Node: CPU-1, Layers: output, Deps: ['light_1-stage-1']\n",
            "=== Stage Allocations for Task light_2 ===\n",
            "Stage light_2-stage-1: Node: CPU-0, Layers: x, conv1, relu, conv2, relu_1, cat, flatten, fc, Deps: []\n",
            "Stage light_2-stage-2: Node: CPU-1, Layers: output, Deps: ['light_2-stage-1']\n",
            "=== Stage Allocations for Task light_3 ===\n",
            "Stage light_3-stage-1: Node: CPU-0, Layers: x, conv1, relu, conv2, relu_1, cat, Deps: []\n",
            "Stage light_3-stage-2: Node: CPU-1, Layers: flatten, fc, output, Deps: ['light_3-stage-1']\n",
            "=== Stage Allocations for Task light_4 ===\n",
            "Stage light_4-stage-1: Node: CPU-0, Layers: x, conv1, relu, conv2, Deps: []\n",
            "Stage light_4-stage-2: Node: CPU-1, Layers: relu_1, cat, flatten, fc, output, Deps: ['light_4-stage-1']\n",
            "=== Stage Allocations for Task light_5 ===\n",
            "Stage light_5-stage-1: Node: CPU-0, Layers: x, conv1, relu, conv2, Deps: []\n",
            "Stage light_5-stage-2: Node: CPU-1, Layers: relu_1, cat, flatten, fc, output, Deps: ['light_5-stage-1']\n",
            "=== Stage Allocations for Task light_6 ===\n",
            "Stage light_6-stage-1: Node: CPU-0, Layers: x, conv1, relu, conv2, Deps: []\n",
            "Stage light_6-stage-2: Node: CPU-1, Layers: relu_1, cat, flatten, fc, output, Deps: ['light_6-stage-1']\n",
            "=== Stage Allocations for Task light_7 ===\n",
            "Stage light_7-stage-1: Node: CPU-0, Layers: x, conv1, relu, conv2, Deps: []\n",
            "Stage light_7-stage-2: Node: CPU-1, Layers: relu_1, cat, flatten, fc, output, Deps: ['light_7-stage-1']\n",
            "=== Stage Allocations for Task light_8 ===\n",
            "Stage light_8-stage-1: Node: CPU-0, Layers: x, conv1, relu, conv2, Deps: []\n",
            "Stage light_8-stage-2: Node: CPU-1, Layers: relu_1, cat, flatten, fc, output, Deps: ['light_8-stage-1']\n",
            "=== Stage Allocations for Task light_9 ===\n",
            "Stage light_9-stage-1: Node: CPU-0, Layers: x, conv1, relu, conv2, Deps: []\n",
            "Stage light_9-stage-2: Node: CPU-1, Layers: relu_1, cat, flatten, fc, output, Deps: ['light_9-stage-1']\n",
            "[Evaluator] Starting Naive Execution.\n",
            "[Evaluator] Task heavy_1: Naive exec time: 0.874420s\n",
            "[Evaluator] Task heavy_2: Naive exec time: 1.151049s\n",
            "[Evaluator] Task heavy_3: Naive exec time: 1.326436s\n",
            "[Evaluator] Task heavy_4: Naive exec time: 1.347782s\n",
            "[Evaluator] Task heavy_5: Naive exec time: 0.867827s\n",
            "[Evaluator] Task heavy_6: Naive exec time: 0.848784s\n",
            "[Evaluator] Task light_1: Naive exec time: 0.003392s\n",
            "[Evaluator] Task light_2: Naive exec time: 0.003289s\n",
            "[Evaluator] Task light_3: Naive exec time: 0.003215s\n",
            "[Evaluator] Task light_4: Naive exec time: 0.003308s\n",
            "[Evaluator] Task light_5: Naive exec time: 0.002818s\n",
            "[Evaluator] Task light_6: Naive exec time: 0.002407s\n",
            "[Evaluator] Task light_7: Naive exec time: 0.002774s\n",
            "[Evaluator] Task light_8: Naive exec time: 0.002690s\n",
            "[Evaluator] Task light_9: Naive exec time: 0.002394s\n",
            "[Evaluator] Naive makespan: 6.467500s\n",
            "\n",
            "[Evaluator] Starting Parallel Execution.\n",
            "[Taskset] Starting Task heavy_1\n",
            "[Taskset] Starting Task heavy_2[Taskset] Starting Task heavy_3\n",
            "\n",
            "[Taskset] Starting Task heavy_4\n",
            "[Taskset] Starting Task heavy_5\n",
            "[Taskset] Starting Task heavy_6\n",
            "[Taskset] Starting Task light_1\n",
            "[Taskset] Starting Task light_2\n",
            "[Taskset] Starting Task light_3\n",
            "[Taskset] Starting Task light_4\n",
            "[Taskset] Starting Task light_5\n",
            "[Taskset] Starting Task light_6\n",
            "[Taskset] Starting Task light_7\n",
            "[Taskset] Starting Task light_8\n",
            "[Taskset] Starting Task light_9\n",
            "[Stage] heavy_1-stage-1: Executed on CPU-0 in 0.428015 s, Transfer: 0.000274 s.\n",
            "[Stage] heavy_3-stage-1: Executed on CPU-0 in 0.760925 s, Transfer: 0.000334 s.\n",
            "[Stage] heavy_1-stage-2: Executed on CPU-1 in 0.861864 s, Transfer: 0.000717 s.\n",
            "[Taskset] Completed Task heavy_1.\n",
            "[Stage] heavy_2-stage-1: Executed on CPU-0 in 0.653006 s, Transfer: 0.000290 s.\n",
            "[Stage] heavy_3-stage-2: Executed on CPU-1 in 0.852695 s, Transfer: 0.000744 s.\n",
            "[Taskset] Completed Task heavy_3.\n",
            "[Stage] heavy_4-stage-1: Executed on CPU-0 in 0.649194 s, Transfer: 0.000324 s.\n",
            "[Stage] heavy_2-stage-2: Executed on CPU-1 in 0.821458 s, Transfer: 0.000731 s.\n",
            "[Taskset] Completed Task heavy_2.\n",
            "[Stage] heavy_5-stage-1: Executed on CPU-0 in 0.690950 s, Transfer: 0.000334 s.\n",
            "[Stage] heavy_4-stage-2: Executed on CPU-1 in 0.852664 s, Transfer: 0.000739 s.\n",
            "[Taskset] Completed Task heavy_4.\n",
            "[Stage] heavy_6-stage-1: Executed on CPU-0 in 0.647114 s, Transfer: 0.000310 s.\n",
            "[Stage] light_1-stage-1: Executed on CPU-0 in 0.004598 s, Transfer: 0.000065 s.\n",
            "[Stage] light_2-stage-1: Executed on CPU-0 in 0.004011 s, Transfer: 0.000058 s.\n",
            "[Stage] light_3-stage-1: Executed on CPU-0 in 0.002671 s, Transfer: 0.000048 s.\n",
            "[Stage] light_4-stage-1: Executed on CPU-0 in 0.002193 s, Transfer: 0.000027 s.\n",
            "[Stage] light_5-stage-1: Executed on CPU-0 in 0.002450 s, Transfer: 0.000027 s.\n",
            "[Stage] light_6-stage-1: Executed on CPU-0 in 0.002857 s, Transfer: 0.000032 s.\n",
            "[Stage] light_7-stage-1: Executed on CPU-0 in 0.002869 s, Transfer: 0.000034 s.\n",
            "[Stage] light_8-stage-1: Executed on CPU-0 in 0.002785 s, Transfer: 0.000031 s.\n",
            "[Stage] light_9-stage-1: Executed on CPU-0 in 0.002784 s, Transfer: 0.000031 s.\n",
            "[Stage] heavy_5-stage-2: Executed on CPU-1 in 0.512214 s, Transfer: 0.000608 s.\n",
            "[Taskset] Completed Task heavy_5.\n",
            "[Stage] heavy_6-stage-2: Executed on CPU-1 in 0.496034 s, Transfer: 0.000551 s.\n",
            "[Stage] light_1-stage-2: Executed on CPU-1 in 0.000044 s, Transfer: 0.000004 s.\n",
            "[Stage] light_2-stage-2: Executed on CPU-1 in 0.000032 s, Transfer: 0.000004 s.\n",
            "[Taskset] Completed Task heavy_6.\n",
            "[Taskset] Completed Task light_2.[Taskset] Completed Task light_1.\n",
            "\n",
            "[Stage] light_3-stage-2: Executed on CPU-1 in 0.004788 s, Transfer: 0.000026 s.\n",
            "[Taskset] Completed Task light_3.\n",
            "[Stage] light_4-stage-2: Executed on CPU-1 in 0.022792 s, Transfer: 0.000047 s.\n",
            "[Taskset] Completed Task light_4.\n",
            "[Stage] light_5-stage-2: Executed on CPU-1 in 0.003963 s, Transfer: 0.000041 s.\n",
            "[Taskset] Completed Task light_5.\n",
            "[Stage] light_6-stage-2: Executed on CPU-1 in 0.002975 s, Transfer: 0.000032 s.\n",
            "[Taskset] Completed Task light_6.\n",
            "[Stage] light_7-stage-2: Executed on CPU-1 in 0.002888 s, Transfer: 0.000037 s.\n",
            "[Taskset] Completed Task light_7.\n",
            "[Stage] light_8-stage-2: Executed on CPU-1 in 0.002815 s, Transfer: 0.000035 s.\n",
            "[Taskset] Completed Task light_8.\n",
            "[Stage] light_9-stage-2: Executed on CPU-1 in 0.002200 s, Transfer: 0.000034 s.\n",
            "[Taskset] Completed Task light_9.\n",
            "[Evaluator] Parallel makespan: 4.884259s\n",
            "\n",
            "[Evaluator] Comparing Outputs.\n",
            "[Evaluator] Task heavy_1: Outputs match.\n",
            "[Evaluator] Task heavy_2: Outputs match.\n",
            "[Evaluator] Task heavy_3: Outputs match.\n",
            "[Evaluator] Task heavy_4: Outputs match.\n",
            "[Evaluator] Task heavy_5: Outputs match.\n",
            "[Evaluator] Task heavy_6: Outputs match.\n",
            "[Evaluator] Task light_1: Outputs match.\n",
            "[Evaluator] Task light_2: Outputs match.\n",
            "[Evaluator] Task light_3: Outputs match.\n",
            "[Evaluator] Task light_4: Outputs match.\n",
            "[Evaluator] Task light_5: Outputs match.\n",
            "[Evaluator] Task light_6: Outputs match.\n",
            "[Evaluator] Task light_7: Outputs match.\n",
            "[Evaluator] Task light_8: Outputs match.\n",
            "[Evaluator] Task light_9: Outputs match.\n",
            "[Evaluator] All outputs match.\n",
            "\n",
            "[Evaluator] Analyzing Speedup and Throughput.\n",
            "\n",
            "--- Sum-of-times ---\n",
            "Naive total: 6.442584s, Parallel total: 63.026895s\n",
            "Speedup (sum-of-times): 0.10x\n",
            "Naive Throughput: 2.33 tasks/s, Parallel Throughput: 0.24 tasks/s\n",
            "\n",
            "--- Makespan ---\n",
            "Naive makespan: 6.467500s, Parallel makespan: 4.884259s\n",
            "Speedup (makespan): 1.32x\n",
            "Naive Throughput (makespan): 2.32 tasks/s, Parallel Throughput (makespan): 3.07 tasks/s\n",
            "\n",
            "--- Task Completion Times ---\n",
            "Task heavy_1: Naive finish: 0.880650s, Parallel finish: 1.294018s\n",
            "Task heavy_2: Naive finish: 2.033756s, Parallel finish: 2.970496s\n",
            "Task heavy_3: Naive finish: 3.362528s, Parallel finish: 2.147790s\n",
            "Task heavy_4: Naive finish: 4.712660s, Parallel finish: 3.824437s\n",
            "Task heavy_5: Naive finish: 5.582518s, Parallel finish: 4.336891s\n",
            "Task heavy_6: Naive finish: 6.433190s, Parallel finish: 4.833267s\n",
            "Task light_1: Naive finish: 6.437986s, Parallel finish: 4.835471s\n",
            "Task light_2: Naive finish: 6.442285s, Parallel finish: 4.833344s\n",
            "Task light_3: Naive finish: 6.446500s, Parallel finish: 4.839100s\n",
            "Task light_4: Naive finish: 6.450814s, Parallel finish: 4.861835s\n",
            "Task light_5: Naive finish: 6.454622s, Parallel finish: 4.865856s\n",
            "Task light_6: Naive finish: 6.458002s, Parallel finish: 4.868983s\n",
            "Task light_7: Naive finish: 6.461141s, Parallel finish: 4.871963s\n",
            "Task light_8: Naive finish: 6.464160s, Parallel finish: 4.874867s\n",
            "Task light_9: Naive finish: 6.467423s, Parallel finish: 4.877396s\n",
            "\n",
            "Speedup for ratio 0.4: 1.32\n",
            "Parallel Throughput for ratio 0.4: 3.07\n",
            "Naive Throughput for ratio 0.4: 2.32\n",
            "Makespan for ratio 0.4: 4.88\n",
            "Running experiment with 7 heavy and 8 light tasks.\n",
            "Seed set to 42\n",
            "Using 2 nodes: [Node(CPU-0, cpus=(0,), gpu=None), Node(CPU-1, cpus=(1,), gpu=None)]\n",
            "Creating 7 heavy tasks and 8 light tasks.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
            "  warnings.warn(msg)\n",
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
            "  warnings.warn(msg)\n",
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
            "  warnings.warn(msg)\n",
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
            "  warnings.warn(msg)\n",
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
            "  warnings.warn(msg)\n",
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
            "  warnings.warn(msg)\n",
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Profiler] Starting profiling for Task 'heavy_1' on CPU-0 (device=cpu).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/autograd/profiler.py:263: UserWarning: CUDA is not available, disabling CUDA profiling\n",
            "  warn(\"CUDA is not available, disabling CUDA profiling\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Profiler] Profiling complete. Data saved to profiling_results.csv.\n",
            "[Profiler] Starting profiling for Task 'heavy_1' on CPU-1 (device=cpu).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/autograd/profiler.py:263: UserWarning: CUDA is not available, disabling CUDA profiling\n",
            "  warn(\"CUDA is not available, disabling CUDA profiling\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Profiler] Profiling complete. Data saved to profiling_results.csv.\n",
            "[Profiler] Reused cached profiling data for PretrainedResNet18 on CPU-0.\n",
            "[Profiler] Reused cached profiling data for PretrainedResNet18 on CPU-1.\n",
            "[Profiler] Reused cached profiling data for PretrainedResNet18 on CPU-0.\n",
            "[Profiler] Reused cached profiling data for PretrainedResNet18 on CPU-1.\n",
            "[Profiler] Reused cached profiling data for PretrainedResNet18 on CPU-0.\n",
            "[Profiler] Reused cached profiling data for PretrainedResNet18 on CPU-1.\n",
            "[Profiler] Reused cached profiling data for PretrainedResNet18 on CPU-0.\n",
            "[Profiler] Reused cached profiling data for PretrainedResNet18 on CPU-1.\n",
            "[Profiler] Reused cached profiling data for PretrainedResNet18 on CPU-0.\n",
            "[Profiler] Reused cached profiling data for PretrainedResNet18 on CPU-1.\n",
            "[Profiler] Reused cached profiling data for PretrainedResNet18 on CPU-0.\n",
            "[Profiler] Reused cached profiling data for PretrainedResNet18 on CPU-1.\n",
            "[Profiler] Starting profiling for Task 'light_1' on CPU-0 (device=cpu).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/autograd/profiler.py:263: UserWarning: CUDA is not available, disabling CUDA profiling\n",
            "  warn(\"CUDA is not available, disabling CUDA profiling\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Profiler] Profiling complete. Data saved to profiling_results.csv.\n",
            "[Profiler] Starting profiling for Task 'light_1' on CPU-1 (device=cpu).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/autograd/profiler.py:263: UserWarning: CUDA is not available, disabling CUDA profiling\n",
            "  warn(\"CUDA is not available, disabling CUDA profiling\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Profiler] Profiling complete. Data saved to profiling_results.csv.\n",
            "[Profiler] Reused cached profiling data for SimpleCNN on CPU-0.\n",
            "[Profiler] Reused cached profiling data for SimpleCNN on CPU-1.\n",
            "[Profiler] Reused cached profiling data for SimpleCNN on CPU-0.\n",
            "[Profiler] Reused cached profiling data for SimpleCNN on CPU-1.\n",
            "[Profiler] Reused cached profiling data for SimpleCNN on CPU-0.\n",
            "[Profiler] Reused cached profiling data for SimpleCNN on CPU-1.\n",
            "[Profiler] Reused cached profiling data for SimpleCNN on CPU-0.\n",
            "[Profiler] Reused cached profiling data for SimpleCNN on CPU-1.\n",
            "[Profiler] Reused cached profiling data for SimpleCNN on CPU-0.\n",
            "[Profiler] Reused cached profiling data for SimpleCNN on CPU-1.\n",
            "[Profiler] Reused cached profiling data for SimpleCNN on CPU-0.\n",
            "[Profiler] Reused cached profiling data for SimpleCNN on CPU-1.\n",
            "[Profiler] Reused cached profiling data for SimpleCNN on CPU-0.\n",
            "[Profiler] Reused cached profiling data for SimpleCNN on CPU-1.\n",
            "Measured max transfer penalty: 0.000017 s\n",
            "=== Stage Allocations for Task heavy_1 ===\n",
            "Stage heavy_1-stage-1: Node: CPU-0, Layers: x, resnet18_conv1, resnet18_bn1, resnet18_relu, resnet18_maxpool, resnet18_layer1_0_conv1, resnet18_layer1_0_bn1, resnet18_layer1_0_relu, resnet18_layer1_0_conv2, resnet18_layer1_0_bn2, add, resnet18_layer1_0_relu_1, resnet18_layer1_1_conv1, resnet18_layer1_1_bn1, resnet18_layer1_1_relu, resnet18_layer1_1_conv2, resnet18_layer1_1_bn2, add_1, resnet18_layer1_1_relu_1, resnet18_layer2_0_conv1, resnet18_layer2_0_bn1, resnet18_layer2_0_relu, resnet18_layer2_0_conv2, Deps: []\n",
            "Stage heavy_1-stage-2: Node: CPU-1, Layers: resnet18_layer2_0_bn2, resnet18_layer2_0_downsample_0, resnet18_layer2_0_downsample_1, add_2, resnet18_layer2_0_relu_1, resnet18_layer2_1_conv1, resnet18_layer2_1_bn1, resnet18_layer2_1_relu, resnet18_layer2_1_conv2, resnet18_layer2_1_bn2, add_3, resnet18_layer2_1_relu_1, resnet18_layer3_0_conv1, resnet18_layer3_0_bn1, resnet18_layer3_0_relu, resnet18_layer3_0_conv2, resnet18_layer3_0_bn2, resnet18_layer3_0_downsample_0, resnet18_layer3_0_downsample_1, add_4, resnet18_layer3_0_relu_1, resnet18_layer3_1_conv1, resnet18_layer3_1_bn1, resnet18_layer3_1_relu, resnet18_layer3_1_conv2, resnet18_layer3_1_bn2, add_5, resnet18_layer3_1_relu_1, resnet18_layer4_0_conv1, resnet18_layer4_0_bn1, resnet18_layer4_0_relu, resnet18_layer4_0_conv2, resnet18_layer4_0_bn2, resnet18_layer4_0_downsample_0, resnet18_layer4_0_downsample_1, add_6, resnet18_layer4_0_relu_1, resnet18_layer4_1_conv1, resnet18_layer4_1_bn1, resnet18_layer4_1_relu, resnet18_layer4_1_conv2, resnet18_layer4_1_bn2, add_7, resnet18_layer4_1_relu_1, resnet18_avgpool, flatten, resnet18_fc, output, Deps: ['heavy_1-stage-1']\n",
            "=== Stage Allocations for Task heavy_2 ===\n",
            "Stage heavy_2-stage-1: Node: CPU-0, Layers: x, resnet18_conv1, resnet18_bn1, resnet18_relu, resnet18_maxpool, resnet18_layer1_0_conv1, resnet18_layer1_0_bn1, resnet18_layer1_0_relu, resnet18_layer1_0_conv2, resnet18_layer1_0_bn2, add, resnet18_layer1_0_relu_1, resnet18_layer1_1_conv1, resnet18_layer1_1_bn1, resnet18_layer1_1_relu, resnet18_layer1_1_conv2, resnet18_layer1_1_bn2, add_1, Deps: []\n",
            "Stage heavy_2-stage-2: Node: CPU-1, Layers: resnet18_layer1_1_relu_1, resnet18_layer2_0_conv1, resnet18_layer2_0_bn1, resnet18_layer2_0_relu, resnet18_layer2_0_conv2, resnet18_layer2_0_bn2, resnet18_layer2_0_downsample_0, resnet18_layer2_0_downsample_1, add_2, resnet18_layer2_0_relu_1, resnet18_layer2_1_conv1, resnet18_layer2_1_bn1, resnet18_layer2_1_relu, resnet18_layer2_1_conv2, resnet18_layer2_1_bn2, add_3, resnet18_layer2_1_relu_1, resnet18_layer3_0_conv1, resnet18_layer3_0_bn1, resnet18_layer3_0_relu, resnet18_layer3_0_conv2, resnet18_layer3_0_bn2, resnet18_layer3_0_downsample_0, resnet18_layer3_0_downsample_1, add_4, resnet18_layer3_0_relu_1, resnet18_layer3_1_conv1, resnet18_layer3_1_bn1, resnet18_layer3_1_relu, resnet18_layer3_1_conv2, resnet18_layer3_1_bn2, add_5, resnet18_layer3_1_relu_1, resnet18_layer4_0_conv1, resnet18_layer4_0_bn1, resnet18_layer4_0_relu, resnet18_layer4_0_conv2, resnet18_layer4_0_bn2, resnet18_layer4_0_downsample_0, resnet18_layer4_0_downsample_1, add_6, resnet18_layer4_0_relu_1, resnet18_layer4_1_conv1, resnet18_layer4_1_bn1, resnet18_layer4_1_relu, resnet18_layer4_1_conv2, resnet18_layer4_1_bn2, add_7, resnet18_layer4_1_relu_1, resnet18_avgpool, flatten, resnet18_fc, output, Deps: ['heavy_2-stage-1']\n",
            "=== Stage Allocations for Task heavy_3 ===\n",
            "Stage heavy_3-stage-1: Node: CPU-0, Layers: x, resnet18_conv1, resnet18_bn1, resnet18_relu, resnet18_maxpool, resnet18_layer1_0_conv1, resnet18_layer1_0_bn1, resnet18_layer1_0_relu, resnet18_layer1_0_conv2, resnet18_layer1_0_bn2, add, resnet18_layer1_0_relu_1, resnet18_layer1_1_conv1, resnet18_layer1_1_bn1, resnet18_layer1_1_relu, resnet18_layer1_1_conv2, resnet18_layer1_1_bn2, add_1, resnet18_layer1_1_relu_1, Deps: []\n",
            "Stage heavy_3-stage-2: Node: CPU-1, Layers: resnet18_layer2_0_conv1, resnet18_layer2_0_bn1, resnet18_layer2_0_relu, resnet18_layer2_0_conv2, resnet18_layer2_0_bn2, resnet18_layer2_0_downsample_0, resnet18_layer2_0_downsample_1, add_2, resnet18_layer2_0_relu_1, resnet18_layer2_1_conv1, resnet18_layer2_1_bn1, resnet18_layer2_1_relu, resnet18_layer2_1_conv2, resnet18_layer2_1_bn2, add_3, resnet18_layer2_1_relu_1, resnet18_layer3_0_conv1, resnet18_layer3_0_bn1, resnet18_layer3_0_relu, resnet18_layer3_0_conv2, resnet18_layer3_0_bn2, resnet18_layer3_0_downsample_0, resnet18_layer3_0_downsample_1, add_4, resnet18_layer3_0_relu_1, resnet18_layer3_1_conv1, resnet18_layer3_1_bn1, resnet18_layer3_1_relu, resnet18_layer3_1_conv2, resnet18_layer3_1_bn2, add_5, resnet18_layer3_1_relu_1, resnet18_layer4_0_conv1, resnet18_layer4_0_bn1, resnet18_layer4_0_relu, resnet18_layer4_0_conv2, resnet18_layer4_0_bn2, resnet18_layer4_0_downsample_0, resnet18_layer4_0_downsample_1, add_6, resnet18_layer4_0_relu_1, resnet18_layer4_1_conv1, resnet18_layer4_1_bn1, resnet18_layer4_1_relu, resnet18_layer4_1_conv2, resnet18_layer4_1_bn2, add_7, resnet18_layer4_1_relu_1, resnet18_avgpool, flatten, resnet18_fc, output, Deps: ['heavy_3-stage-1']\n",
            "=== Stage Allocations for Task heavy_4 ===\n",
            "Stage heavy_4-stage-1: Node: CPU-0, Layers: x, resnet18_conv1, resnet18_bn1, resnet18_relu, resnet18_maxpool, resnet18_layer1_0_conv1, resnet18_layer1_0_bn1, resnet18_layer1_0_relu, resnet18_layer1_0_conv2, resnet18_layer1_0_bn2, add, resnet18_layer1_0_relu_1, resnet18_layer1_1_conv1, resnet18_layer1_1_bn1, resnet18_layer1_1_relu, resnet18_layer1_1_conv2, resnet18_layer1_1_bn2, add_1, resnet18_layer1_1_relu_1, Deps: []\n",
            "Stage heavy_4-stage-2: Node: CPU-1, Layers: resnet18_layer2_0_conv1, resnet18_layer2_0_bn1, resnet18_layer2_0_relu, resnet18_layer2_0_conv2, resnet18_layer2_0_bn2, resnet18_layer2_0_downsample_0, resnet18_layer2_0_downsample_1, add_2, resnet18_layer2_0_relu_1, resnet18_layer2_1_conv1, resnet18_layer2_1_bn1, resnet18_layer2_1_relu, resnet18_layer2_1_conv2, resnet18_layer2_1_bn2, add_3, resnet18_layer2_1_relu_1, resnet18_layer3_0_conv1, resnet18_layer3_0_bn1, resnet18_layer3_0_relu, resnet18_layer3_0_conv2, resnet18_layer3_0_bn2, resnet18_layer3_0_downsample_0, resnet18_layer3_0_downsample_1, add_4, resnet18_layer3_0_relu_1, resnet18_layer3_1_conv1, resnet18_layer3_1_bn1, resnet18_layer3_1_relu, resnet18_layer3_1_conv2, resnet18_layer3_1_bn2, add_5, resnet18_layer3_1_relu_1, resnet18_layer4_0_conv1, resnet18_layer4_0_bn1, resnet18_layer4_0_relu, resnet18_layer4_0_conv2, resnet18_layer4_0_bn2, resnet18_layer4_0_downsample_0, resnet18_layer4_0_downsample_1, add_6, resnet18_layer4_0_relu_1, resnet18_layer4_1_conv1, resnet18_layer4_1_bn1, resnet18_layer4_1_relu, resnet18_layer4_1_conv2, resnet18_layer4_1_bn2, add_7, resnet18_layer4_1_relu_1, resnet18_avgpool, flatten, resnet18_fc, output, Deps: ['heavy_4-stage-1']\n",
            "=== Stage Allocations for Task heavy_5 ===\n",
            "Stage heavy_5-stage-1: Node: CPU-0, Layers: x, resnet18_conv1, resnet18_bn1, resnet18_relu, resnet18_maxpool, resnet18_layer1_0_conv1, resnet18_layer1_0_bn1, resnet18_layer1_0_relu, resnet18_layer1_0_conv2, resnet18_layer1_0_bn2, add, resnet18_layer1_0_relu_1, resnet18_layer1_1_conv1, resnet18_layer1_1_bn1, resnet18_layer1_1_relu, resnet18_layer1_1_conv2, resnet18_layer1_1_bn2, add_1, resnet18_layer1_1_relu_1, Deps: []\n",
            "Stage heavy_5-stage-2: Node: CPU-1, Layers: resnet18_layer2_0_conv1, resnet18_layer2_0_bn1, resnet18_layer2_0_relu, resnet18_layer2_0_conv2, resnet18_layer2_0_bn2, resnet18_layer2_0_downsample_0, resnet18_layer2_0_downsample_1, add_2, resnet18_layer2_0_relu_1, resnet18_layer2_1_conv1, resnet18_layer2_1_bn1, resnet18_layer2_1_relu, resnet18_layer2_1_conv2, resnet18_layer2_1_bn2, add_3, resnet18_layer2_1_relu_1, resnet18_layer3_0_conv1, resnet18_layer3_0_bn1, resnet18_layer3_0_relu, resnet18_layer3_0_conv2, resnet18_layer3_0_bn2, resnet18_layer3_0_downsample_0, resnet18_layer3_0_downsample_1, add_4, resnet18_layer3_0_relu_1, resnet18_layer3_1_conv1, resnet18_layer3_1_bn1, resnet18_layer3_1_relu, resnet18_layer3_1_conv2, resnet18_layer3_1_bn2, add_5, resnet18_layer3_1_relu_1, resnet18_layer4_0_conv1, resnet18_layer4_0_bn1, resnet18_layer4_0_relu, resnet18_layer4_0_conv2, resnet18_layer4_0_bn2, resnet18_layer4_0_downsample_0, resnet18_layer4_0_downsample_1, add_6, resnet18_layer4_0_relu_1, resnet18_layer4_1_conv1, resnet18_layer4_1_bn1, resnet18_layer4_1_relu, resnet18_layer4_1_conv2, resnet18_layer4_1_bn2, add_7, resnet18_layer4_1_relu_1, resnet18_avgpool, flatten, resnet18_fc, output, Deps: ['heavy_5-stage-1']\n",
            "=== Stage Allocations for Task heavy_6 ===\n",
            "Stage heavy_6-stage-1: Node: CPU-0, Layers: x, resnet18_conv1, resnet18_bn1, resnet18_relu, resnet18_maxpool, resnet18_layer1_0_conv1, resnet18_layer1_0_bn1, resnet18_layer1_0_relu, resnet18_layer1_0_conv2, resnet18_layer1_0_bn2, add, resnet18_layer1_0_relu_1, resnet18_layer1_1_conv1, resnet18_layer1_1_bn1, resnet18_layer1_1_relu, resnet18_layer1_1_conv2, resnet18_layer1_1_bn2, add_1, resnet18_layer1_1_relu_1, resnet18_layer2_0_conv1, Deps: []\n",
            "Stage heavy_6-stage-2: Node: CPU-1, Layers: resnet18_layer2_0_bn1, resnet18_layer2_0_relu, resnet18_layer2_0_conv2, resnet18_layer2_0_bn2, resnet18_layer2_0_downsample_0, resnet18_layer2_0_downsample_1, add_2, resnet18_layer2_0_relu_1, resnet18_layer2_1_conv1, resnet18_layer2_1_bn1, resnet18_layer2_1_relu, resnet18_layer2_1_conv2, resnet18_layer2_1_bn2, add_3, resnet18_layer2_1_relu_1, resnet18_layer3_0_conv1, resnet18_layer3_0_bn1, resnet18_layer3_0_relu, resnet18_layer3_0_conv2, resnet18_layer3_0_bn2, resnet18_layer3_0_downsample_0, resnet18_layer3_0_downsample_1, add_4, resnet18_layer3_0_relu_1, resnet18_layer3_1_conv1, resnet18_layer3_1_bn1, resnet18_layer3_1_relu, resnet18_layer3_1_conv2, resnet18_layer3_1_bn2, add_5, resnet18_layer3_1_relu_1, resnet18_layer4_0_conv1, resnet18_layer4_0_bn1, resnet18_layer4_0_relu, resnet18_layer4_0_conv2, resnet18_layer4_0_bn2, resnet18_layer4_0_downsample_0, resnet18_layer4_0_downsample_1, add_6, resnet18_layer4_0_relu_1, resnet18_layer4_1_conv1, resnet18_layer4_1_bn1, resnet18_layer4_1_relu, resnet18_layer4_1_conv2, resnet18_layer4_1_bn2, add_7, resnet18_layer4_1_relu_1, resnet18_avgpool, flatten, resnet18_fc, output, Deps: ['heavy_6-stage-1']\n",
            "=== Stage Allocations for Task heavy_7 ===\n",
            "Stage heavy_7-stage-1: Node: CPU-0, Layers: x, resnet18_conv1, resnet18_bn1, resnet18_relu, resnet18_maxpool, resnet18_layer1_0_conv1, resnet18_layer1_0_bn1, resnet18_layer1_0_relu, resnet18_layer1_0_conv2, resnet18_layer1_0_bn2, add, resnet18_layer1_0_relu_1, resnet18_layer1_1_conv1, resnet18_layer1_1_bn1, resnet18_layer1_1_relu, resnet18_layer1_1_conv2, resnet18_layer1_1_bn2, add_1, resnet18_layer1_1_relu_1, Deps: []\n",
            "Stage heavy_7-stage-2: Node: CPU-1, Layers: resnet18_layer2_0_conv1, resnet18_layer2_0_bn1, resnet18_layer2_0_relu, resnet18_layer2_0_conv2, resnet18_layer2_0_bn2, resnet18_layer2_0_downsample_0, resnet18_layer2_0_downsample_1, add_2, resnet18_layer2_0_relu_1, resnet18_layer2_1_conv1, resnet18_layer2_1_bn1, resnet18_layer2_1_relu, resnet18_layer2_1_conv2, resnet18_layer2_1_bn2, add_3, resnet18_layer2_1_relu_1, resnet18_layer3_0_conv1, resnet18_layer3_0_bn1, resnet18_layer3_0_relu, resnet18_layer3_0_conv2, resnet18_layer3_0_bn2, resnet18_layer3_0_downsample_0, resnet18_layer3_0_downsample_1, add_4, resnet18_layer3_0_relu_1, resnet18_layer3_1_conv1, resnet18_layer3_1_bn1, resnet18_layer3_1_relu, resnet18_layer3_1_conv2, resnet18_layer3_1_bn2, add_5, resnet18_layer3_1_relu_1, resnet18_layer4_0_conv1, resnet18_layer4_0_bn1, resnet18_layer4_0_relu, resnet18_layer4_0_conv2, resnet18_layer4_0_bn2, resnet18_layer4_0_downsample_0, resnet18_layer4_0_downsample_1, add_6, resnet18_layer4_0_relu_1, resnet18_layer4_1_conv1, resnet18_layer4_1_bn1, resnet18_layer4_1_relu, resnet18_layer4_1_conv2, resnet18_layer4_1_bn2, add_7, resnet18_layer4_1_relu_1, resnet18_avgpool, flatten, resnet18_fc, output, Deps: ['heavy_7-stage-1']\n",
            "=== Stage Allocations for Task light_1 ===\n",
            "Stage light_1-stage-1: Node: CPU-0, Layers: x, conv1, relu, conv2, relu_1, cat, flatten, fc, Deps: []\n",
            "Stage light_1-stage-2: Node: CPU-1, Layers: output, Deps: ['light_1-stage-1']\n",
            "=== Stage Allocations for Task light_2 ===\n",
            "Stage light_2-stage-1: Node: CPU-0, Layers: x, conv1, relu, conv2, relu_1, cat, flatten, fc, Deps: []\n",
            "Stage light_2-stage-2: Node: CPU-1, Layers: output, Deps: ['light_2-stage-1']\n",
            "=== Stage Allocations for Task light_3 ===\n",
            "Stage light_3-stage-1: Node: CPU-0, Layers: x, conv1, relu, conv2, relu_1, cat, flatten, fc, Deps: []\n",
            "Stage light_3-stage-2: Node: CPU-1, Layers: output, Deps: ['light_3-stage-1']\n",
            "=== Stage Allocations for Task light_4 ===\n",
            "Stage light_4-stage-1: Node: CPU-0, Layers: x, conv1, relu, conv2, relu_1, cat, Deps: []\n",
            "Stage light_4-stage-2: Node: CPU-1, Layers: flatten, fc, output, Deps: ['light_4-stage-1']\n",
            "=== Stage Allocations for Task light_5 ===\n",
            "Stage light_5-stage-1: Node: CPU-0, Layers: x, conv1, relu, conv2, Deps: []\n",
            "Stage light_5-stage-2: Node: CPU-1, Layers: relu_1, cat, flatten, fc, output, Deps: ['light_5-stage-1']\n",
            "=== Stage Allocations for Task light_6 ===\n",
            "Stage light_6-stage-1: Node: CPU-0, Layers: x, conv1, relu, conv2, Deps: []\n",
            "Stage light_6-stage-2: Node: CPU-1, Layers: relu_1, cat, flatten, fc, output, Deps: ['light_6-stage-1']\n",
            "=== Stage Allocations for Task light_7 ===\n",
            "Stage light_7-stage-1: Node: CPU-0, Layers: x, conv1, relu, conv2, Deps: []\n",
            "Stage light_7-stage-2: Node: CPU-1, Layers: relu_1, cat, flatten, fc, output, Deps: ['light_7-stage-1']\n",
            "=== Stage Allocations for Task light_8 ===\n",
            "Stage light_8-stage-1: Node: CPU-0, Layers: x, conv1, relu, conv2, Deps: []\n",
            "Stage light_8-stage-2: Node: CPU-1, Layers: relu_1, cat, flatten, fc, output, Deps: ['light_8-stage-1']\n",
            "[Evaluator] Starting Naive Execution.\n",
            "[Evaluator] Task heavy_1: Naive exec time: 0.876709s\n",
            "[Evaluator] Task heavy_2: Naive exec time: 0.876000s\n",
            "[Evaluator] Task heavy_3: Naive exec time: 0.875393s\n",
            "[Evaluator] Task heavy_4: Naive exec time: 0.879740s\n",
            "[Evaluator] Task heavy_5: Naive exec time: 0.860522s\n",
            "[Evaluator] Task heavy_6: Naive exec time: 1.097742s\n",
            "[Evaluator] Task heavy_7: Naive exec time: 1.291354s\n",
            "[Evaluator] Task light_1: Naive exec time: 0.003489s\n",
            "[Evaluator] Task light_2: Naive exec time: 0.003367s\n",
            "[Evaluator] Task light_3: Naive exec time: 0.003345s\n",
            "[Evaluator] Task light_4: Naive exec time: 0.003361s\n",
            "[Evaluator] Task light_5: Naive exec time: 0.003340s\n",
            "[Evaluator] Task light_6: Naive exec time: 0.003129s\n",
            "[Evaluator] Task light_7: Naive exec time: 0.003399s\n",
            "[Evaluator] Task light_8: Naive exec time: 0.003304s\n",
            "[Evaluator] Naive makespan: 6.812132s\n",
            "\n",
            "[Evaluator] Starting Parallel Execution.\n",
            "[Taskset] Starting Task heavy_1\n",
            "[Taskset] Starting Task heavy_2\n",
            "[Taskset] Starting Task heavy_3\n",
            "[Taskset] Starting Task heavy_4\n",
            "[Taskset] Starting Task heavy_5\n",
            "[Taskset] Starting Task heavy_6\n",
            "[Taskset] Starting Task heavy_7\n",
            "[Taskset] Starting Task light_1\n",
            "[Taskset] Starting Task light_2\n",
            "[Taskset] Starting Task light_3\n",
            "[Taskset] Starting Task light_4\n",
            "[Taskset] Starting Task light_5\n",
            "[Taskset] Starting Task light_6\n",
            "[Taskset] Starting Task light_7\n",
            "[Taskset] Starting Task light_8\n",
            "[Stage] heavy_1-stage-1: Executed on CPU-0 in 0.707855 s, Transfer: 0.000414 s.\n",
            "[Stage] heavy_2-stage-1: Executed on CPU-0 in 0.889487 s, Transfer: 0.000319 s.\n",
            "[Stage] heavy_1-stage-2: Executed on CPU-1 in 0.988391 s, Transfer: 0.000746 s.\n",
            "[Taskset] Completed Task heavy_1.\n",
            "[Stage] heavy_3-stage-1: Executed on CPU-0 in 0.604939 s, Transfer: 0.000313 s.\n",
            "[Stage] heavy_2-stage-2: Executed on CPU-1 in 0.854200 s, Transfer: 0.000715 s.\n",
            "[Taskset] Completed Task heavy_2.\n",
            "[Stage] heavy_4-stage-1: Executed on CPU-0 in 0.599191 s, Transfer: 0.000307 s.\n",
            "[Stage] heavy_3-stage-2: Executed on CPU-1 in 0.834011 s, Transfer: 0.000733 s.\n",
            "[Taskset] Completed Task heavy_3.\n",
            "[Stage] heavy_5-stage-1: Executed on CPU-0 in 0.637769 s, Transfer: 0.000337 s.\n",
            "[Stage] heavy_6-stage-1: Executed on CPU-0 in 0.660171 s, Transfer: 0.000319 s.\n",
            "[Stage] heavy_4-stage-2: Executed on CPU-1 in 0.872351 s, Transfer: 0.000732 s.\n",
            "[Taskset] Completed Task heavy_4.\n",
            "[Stage] heavy_7-stage-1: Executed on CPU-0 in 0.683997 s, Transfer: 0.000327 s.\n",
            "[Stage] light_1-stage-1: Executed on CPU-0 in 0.005424 s, Transfer: 0.000091 s.\n",
            "[Stage] light_2-stage-1: Executed on CPU-0 in 0.004813 s, Transfer: 0.000077 s.\n",
            "[Stage] light_3-stage-1: Executed on CPU-0 in 0.004411 s, Transfer: 0.000060 s.\n",
            "[Stage] light_4-stage-1: Executed on CPU-0 in 0.004143 s, Transfer: 0.000052 s.\n",
            "[Stage] light_5-stage-1: Executed on CPU-0 in 0.003499 s, Transfer: 0.000034 s.\n",
            "[Stage] light_6-stage-1: Executed on CPU-0 in 0.002960 s, Transfer: 0.000035 s.\n",
            "[Stage] light_7-stage-1: Executed on CPU-0 in 0.003871 s, Transfer: 0.000033 s.\n",
            "[Stage] light_8-stage-1: Executed on CPU-0 in 0.002853 s, Transfer: 0.000035 s.\n",
            "[Stage] heavy_5-stage-2: Executed on CPU-1 in 0.747161 s, Transfer: 0.000664 s.\n",
            "[Taskset] Completed Task heavy_5.\n",
            "[Stage] heavy_6-stage-2: Executed on CPU-1 in 0.474030 s, Transfer: 0.000561 s.\n",
            "[Taskset] Completed Task heavy_6.\n",
            "[Stage] heavy_7-stage-2: Executed on CPU-1 in 0.499047 s, Transfer: 0.000585 s.\n",
            "[Stage] light_1-stage-2: Executed on CPU-1 in 0.000042 s, Transfer: 0.000004 s.\n",
            "[Stage] light_2-stage-2: Executed on CPU-1 in 0.000033 s, Transfer: 0.000005 s.[Taskset] Completed Task heavy_7.\n",
            "[Taskset] Completed Task light_1.\n",
            "\n",
            "[Stage] light_3-stage-2: Executed on CPU-1 in 0.000104 s, Transfer: 0.000009 s.\n",
            "[Taskset] Completed Task light_2.[Taskset] Completed Task light_3.\n",
            "\n",
            "[Stage] light_4-stage-2: Executed on CPU-1 in 0.004240 s, Transfer: 0.000027 s.\n",
            "[Taskset] Completed Task light_4.\n",
            "[Stage] light_5-stage-2: Executed on CPU-1 in 0.003109 s, Transfer: 0.000042 s.\n",
            "[Taskset] Completed Task light_5.\n",
            "[Stage] light_6-stage-2: Executed on CPU-1 in 0.002979 s, Transfer: 0.000038 s.\n",
            "[Taskset] Completed Task light_6.\n",
            "[Stage] light_7-stage-2: Executed on CPU-1 in 0.004706 s, Transfer: 0.000045 s.\n",
            "[Taskset] Completed Task light_7.\n",
            "[Stage] light_8-stage-2: Executed on CPU-1 in 0.001968 s, Transfer: 0.000039 s.\n",
            "[Taskset] Completed Task light_8.\n",
            "[Evaluator] Parallel makespan: 6.015441s\n",
            "\n",
            "[Evaluator] Comparing Outputs.\n",
            "[Evaluator] Task heavy_1: Outputs match.\n",
            "[Evaluator] Task heavy_2: Outputs match.\n",
            "[Evaluator] Task heavy_3: Outputs match.\n",
            "[Evaluator] Task heavy_4: Outputs match.\n",
            "[Evaluator] Task heavy_5: Outputs match.\n",
            "[Evaluator] Task heavy_6: Outputs match.\n",
            "[Evaluator] Task heavy_7: Outputs match.\n",
            "[Evaluator] Task light_1: Outputs match.\n",
            "[Evaluator] Task light_2: Outputs match.\n",
            "[Evaluator] Task light_3: Outputs match.\n",
            "[Evaluator] Task light_4: Outputs match.\n",
            "[Evaluator] Task light_5: Outputs match.\n",
            "[Evaluator] Task light_6: Outputs match.\n",
            "[Evaluator] Task light_7: Outputs match.\n",
            "[Evaluator] Task light_8: Outputs match.\n",
            "[Evaluator] All outputs match.\n",
            "\n",
            "[Evaluator] Analyzing Speedup and Throughput.\n",
            "\n",
            "--- Sum-of-times ---\n",
            "Naive total: 6.784195s, Parallel total: 76.219190s\n",
            "Speedup (sum-of-times): 0.09x\n",
            "Naive Throughput: 2.21 tasks/s, Parallel Throughput: 0.20 tasks/s\n",
            "\n",
            "--- Makespan ---\n",
            "Naive makespan: 6.812132s, Parallel makespan: 6.015441s\n",
            "Speedup (makespan): 1.13x\n",
            "Naive Throughput (makespan): 2.20 tasks/s, Parallel Throughput (makespan): 2.49 tasks/s\n",
            "\n",
            "--- Task Completion Times ---\n",
            "Task heavy_1: Naive finish: 0.883990s, Parallel finish: 1.700177s\n",
            "Task heavy_2: Naive finish: 1.762003s, Parallel finish: 2.555506s\n",
            "Task heavy_3: Naive finish: 2.639196s, Parallel finish: 3.390784s\n",
            "Task heavy_4: Naive finish: 3.520727s, Parallel finish: 4.263980s\n",
            "Task heavy_5: Naive finish: 4.383115s, Parallel finish: 5.011380s\n",
            "Task heavy_6: Naive finish: 5.482726s, Parallel finish: 5.485660s\n",
            "Task heavy_7: Naive finish: 6.776337s, Parallel finish: 5.984924s\n",
            "Task light_1: Naive finish: 6.781268s, Parallel finish: 5.987525s\n",
            "Task light_2: Naive finish: 6.785629s, Parallel finish: 5.996158s\n",
            "Task light_3: Naive finish: 6.789960s, Parallel finish: 5.996205s\n",
            "Task light_4: Naive finish: 6.794319s, Parallel finish: 6.000550s\n",
            "Task light_5: Naive finish: 6.798661s, Parallel finish: 6.003775s\n",
            "Task light_6: Naive finish: 6.802724s, Parallel finish: 6.006392s\n",
            "Task light_7: Naive finish: 6.807024s, Parallel finish: 6.012142s\n",
            "Task light_8: Naive finish: 6.811320s, Parallel finish: 6.013015s\n",
            "\n",
            "Speedup for ratio 0.5: 1.13\n",
            "Parallel Throughput for ratio 0.5: 2.49\n",
            "Naive Throughput for ratio 0.5: 2.20\n",
            "Makespan for ratio 0.5: 6.02\n",
            "Running experiment with 9 heavy and 6 light tasks.\n",
            "Seed set to 42\n",
            "Using 2 nodes: [Node(CPU-0, cpus=(0,), gpu=None), Node(CPU-1, cpus=(1,), gpu=None)]\n",
            "Creating 9 heavy tasks and 6 light tasks.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
            "  warnings.warn(msg)\n",
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
            "  warnings.warn(msg)\n",
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
            "  warnings.warn(msg)\n",
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
            "  warnings.warn(msg)\n",
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
            "  warnings.warn(msg)\n",
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
            "  warnings.warn(msg)\n",
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
            "  warnings.warn(msg)\n",
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
            "  warnings.warn(msg)\n",
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Profiler] Starting profiling for Task 'heavy_1' on CPU-0 (device=cpu).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/autograd/profiler.py:263: UserWarning: CUDA is not available, disabling CUDA profiling\n",
            "  warn(\"CUDA is not available, disabling CUDA profiling\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Profiler] Profiling complete. Data saved to profiling_results.csv.\n",
            "[Profiler] Starting profiling for Task 'heavy_1' on CPU-1 (device=cpu).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/autograd/profiler.py:263: UserWarning: CUDA is not available, disabling CUDA profiling\n",
            "  warn(\"CUDA is not available, disabling CUDA profiling\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Profiler] Profiling complete. Data saved to profiling_results.csv.\n",
            "[Profiler] Reused cached profiling data for PretrainedResNet18 on CPU-0.\n",
            "[Profiler] Reused cached profiling data for PretrainedResNet18 on CPU-1.\n",
            "[Profiler] Reused cached profiling data for PretrainedResNet18 on CPU-0.\n",
            "[Profiler] Reused cached profiling data for PretrainedResNet18 on CPU-1.\n",
            "[Profiler] Reused cached profiling data for PretrainedResNet18 on CPU-0.\n",
            "[Profiler] Reused cached profiling data for PretrainedResNet18 on CPU-1.\n",
            "[Profiler] Reused cached profiling data for PretrainedResNet18 on CPU-0.\n",
            "[Profiler] Reused cached profiling data for PretrainedResNet18 on CPU-1.\n",
            "[Profiler] Reused cached profiling data for PretrainedResNet18 on CPU-0.\n",
            "[Profiler] Reused cached profiling data for PretrainedResNet18 on CPU-1.\n",
            "[Profiler] Reused cached profiling data for PretrainedResNet18 on CPU-0.\n",
            "[Profiler] Reused cached profiling data for PretrainedResNet18 on CPU-1.\n",
            "[Profiler] Reused cached profiling data for PretrainedResNet18 on CPU-0.\n",
            "[Profiler] Reused cached profiling data for PretrainedResNet18 on CPU-1.\n",
            "[Profiler] Reused cached profiling data for PretrainedResNet18 on CPU-0.\n",
            "[Profiler] Reused cached profiling data for PretrainedResNet18 on CPU-1.\n",
            "[Profiler] Starting profiling for Task 'light_1' on CPU-0 (device=cpu).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/autograd/profiler.py:263: UserWarning: CUDA is not available, disabling CUDA profiling\n",
            "  warn(\"CUDA is not available, disabling CUDA profiling\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Profiler] Profiling complete. Data saved to profiling_results.csv.\n",
            "[Profiler] Starting profiling for Task 'light_1' on CPU-1 (device=cpu).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/autograd/profiler.py:263: UserWarning: CUDA is not available, disabling CUDA profiling\n",
            "  warn(\"CUDA is not available, disabling CUDA profiling\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Profiler] Profiling complete. Data saved to profiling_results.csv.\n",
            "[Profiler] Reused cached profiling data for SimpleCNN on CPU-0.\n",
            "[Profiler] Reused cached profiling data for SimpleCNN on CPU-1.\n",
            "[Profiler] Reused cached profiling data for SimpleCNN on CPU-0.\n",
            "[Profiler] Reused cached profiling data for SimpleCNN on CPU-1.\n",
            "[Profiler] Reused cached profiling data for SimpleCNN on CPU-0.\n",
            "[Profiler] Reused cached profiling data for SimpleCNN on CPU-1.\n",
            "[Profiler] Reused cached profiling data for SimpleCNN on CPU-0.\n",
            "[Profiler] Reused cached profiling data for SimpleCNN on CPU-1.\n",
            "[Profiler] Reused cached profiling data for SimpleCNN on CPU-0.\n",
            "[Profiler] Reused cached profiling data for SimpleCNN on CPU-1.\n",
            "Measured max transfer penalty: 0.000024 s\n",
            "=== Stage Allocations for Task heavy_1 ===\n",
            "Stage heavy_1-stage-1: Node: CPU-0, Layers: x, resnet18_conv1, resnet18_bn1, resnet18_relu, resnet18_maxpool, resnet18_layer1_0_conv1, resnet18_layer1_0_bn1, resnet18_layer1_0_relu, resnet18_layer1_0_conv2, resnet18_layer1_0_bn2, add, resnet18_layer1_0_relu_1, resnet18_layer1_1_conv1, resnet18_layer1_1_bn1, resnet18_layer1_1_relu, resnet18_layer1_1_conv2, resnet18_layer1_1_bn2, add_1, resnet18_layer1_1_relu_1, resnet18_layer2_0_conv1, resnet18_layer2_0_bn1, resnet18_layer2_0_relu, resnet18_layer2_0_conv2, Deps: []\n",
            "Stage heavy_1-stage-2: Node: CPU-1, Layers: resnet18_layer2_0_bn2, resnet18_layer2_0_downsample_0, resnet18_layer2_0_downsample_1, add_2, resnet18_layer2_0_relu_1, resnet18_layer2_1_conv1, resnet18_layer2_1_bn1, resnet18_layer2_1_relu, resnet18_layer2_1_conv2, resnet18_layer2_1_bn2, add_3, resnet18_layer2_1_relu_1, resnet18_layer3_0_conv1, resnet18_layer3_0_bn1, resnet18_layer3_0_relu, resnet18_layer3_0_conv2, resnet18_layer3_0_bn2, resnet18_layer3_0_downsample_0, resnet18_layer3_0_downsample_1, add_4, resnet18_layer3_0_relu_1, resnet18_layer3_1_conv1, resnet18_layer3_1_bn1, resnet18_layer3_1_relu, resnet18_layer3_1_conv2, resnet18_layer3_1_bn2, add_5, resnet18_layer3_1_relu_1, resnet18_layer4_0_conv1, resnet18_layer4_0_bn1, resnet18_layer4_0_relu, resnet18_layer4_0_conv2, resnet18_layer4_0_bn2, resnet18_layer4_0_downsample_0, resnet18_layer4_0_downsample_1, add_6, resnet18_layer4_0_relu_1, resnet18_layer4_1_conv1, resnet18_layer4_1_bn1, resnet18_layer4_1_relu, resnet18_layer4_1_conv2, resnet18_layer4_1_bn2, add_7, resnet18_layer4_1_relu_1, resnet18_avgpool, flatten, resnet18_fc, output, Deps: ['heavy_1-stage-1']\n",
            "=== Stage Allocations for Task heavy_2 ===\n",
            "Stage heavy_2-stage-1: Node: CPU-0, Layers: x, resnet18_conv1, resnet18_bn1, resnet18_relu, resnet18_maxpool, resnet18_layer1_0_conv1, resnet18_layer1_0_bn1, resnet18_layer1_0_relu, resnet18_layer1_0_conv2, resnet18_layer1_0_bn2, add, resnet18_layer1_0_relu_1, resnet18_layer1_1_conv1, resnet18_layer1_1_bn1, resnet18_layer1_1_relu, resnet18_layer1_1_conv2, resnet18_layer1_1_bn2, add_1, Deps: []\n",
            "Stage heavy_2-stage-2: Node: CPU-1, Layers: resnet18_layer1_1_relu_1, resnet18_layer2_0_conv1, resnet18_layer2_0_bn1, resnet18_layer2_0_relu, resnet18_layer2_0_conv2, resnet18_layer2_0_bn2, resnet18_layer2_0_downsample_0, resnet18_layer2_0_downsample_1, add_2, resnet18_layer2_0_relu_1, resnet18_layer2_1_conv1, resnet18_layer2_1_bn1, resnet18_layer2_1_relu, resnet18_layer2_1_conv2, resnet18_layer2_1_bn2, add_3, resnet18_layer2_1_relu_1, resnet18_layer3_0_conv1, resnet18_layer3_0_bn1, resnet18_layer3_0_relu, resnet18_layer3_0_conv2, resnet18_layer3_0_bn2, resnet18_layer3_0_downsample_0, resnet18_layer3_0_downsample_1, add_4, resnet18_layer3_0_relu_1, resnet18_layer3_1_conv1, resnet18_layer3_1_bn1, resnet18_layer3_1_relu, resnet18_layer3_1_conv2, resnet18_layer3_1_bn2, add_5, resnet18_layer3_1_relu_1, resnet18_layer4_0_conv1, resnet18_layer4_0_bn1, resnet18_layer4_0_relu, resnet18_layer4_0_conv2, resnet18_layer4_0_bn2, resnet18_layer4_0_downsample_0, resnet18_layer4_0_downsample_1, add_6, resnet18_layer4_0_relu_1, resnet18_layer4_1_conv1, resnet18_layer4_1_bn1, resnet18_layer4_1_relu, resnet18_layer4_1_conv2, resnet18_layer4_1_bn2, add_7, resnet18_layer4_1_relu_1, resnet18_avgpool, flatten, resnet18_fc, output, Deps: ['heavy_2-stage-1']\n",
            "=== Stage Allocations for Task heavy_3 ===\n",
            "Stage heavy_3-stage-1: Node: CPU-0, Layers: x, resnet18_conv1, resnet18_bn1, resnet18_relu, resnet18_maxpool, resnet18_layer1_0_conv1, resnet18_layer1_0_bn1, resnet18_layer1_0_relu, resnet18_layer1_0_conv2, resnet18_layer1_0_bn2, add, resnet18_layer1_0_relu_1, resnet18_layer1_1_conv1, resnet18_layer1_1_bn1, resnet18_layer1_1_relu, resnet18_layer1_1_conv2, resnet18_layer1_1_bn2, add_1, resnet18_layer1_1_relu_1, Deps: []\n",
            "Stage heavy_3-stage-2: Node: CPU-1, Layers: resnet18_layer2_0_conv1, resnet18_layer2_0_bn1, resnet18_layer2_0_relu, resnet18_layer2_0_conv2, resnet18_layer2_0_bn2, resnet18_layer2_0_downsample_0, resnet18_layer2_0_downsample_1, add_2, resnet18_layer2_0_relu_1, resnet18_layer2_1_conv1, resnet18_layer2_1_bn1, resnet18_layer2_1_relu, resnet18_layer2_1_conv2, resnet18_layer2_1_bn2, add_3, resnet18_layer2_1_relu_1, resnet18_layer3_0_conv1, resnet18_layer3_0_bn1, resnet18_layer3_0_relu, resnet18_layer3_0_conv2, resnet18_layer3_0_bn2, resnet18_layer3_0_downsample_0, resnet18_layer3_0_downsample_1, add_4, resnet18_layer3_0_relu_1, resnet18_layer3_1_conv1, resnet18_layer3_1_bn1, resnet18_layer3_1_relu, resnet18_layer3_1_conv2, resnet18_layer3_1_bn2, add_5, resnet18_layer3_1_relu_1, resnet18_layer4_0_conv1, resnet18_layer4_0_bn1, resnet18_layer4_0_relu, resnet18_layer4_0_conv2, resnet18_layer4_0_bn2, resnet18_layer4_0_downsample_0, resnet18_layer4_0_downsample_1, add_6, resnet18_layer4_0_relu_1, resnet18_layer4_1_conv1, resnet18_layer4_1_bn1, resnet18_layer4_1_relu, resnet18_layer4_1_conv2, resnet18_layer4_1_bn2, add_7, resnet18_layer4_1_relu_1, resnet18_avgpool, flatten, resnet18_fc, output, Deps: ['heavy_3-stage-1']\n",
            "=== Stage Allocations for Task heavy_4 ===\n",
            "Stage heavy_4-stage-1: Node: CPU-0, Layers: x, resnet18_conv1, resnet18_bn1, resnet18_relu, resnet18_maxpool, resnet18_layer1_0_conv1, resnet18_layer1_0_bn1, resnet18_layer1_0_relu, resnet18_layer1_0_conv2, resnet18_layer1_0_bn2, add, resnet18_layer1_0_relu_1, resnet18_layer1_1_conv1, resnet18_layer1_1_bn1, resnet18_layer1_1_relu, resnet18_layer1_1_conv2, resnet18_layer1_1_bn2, add_1, resnet18_layer1_1_relu_1, Deps: []\n",
            "Stage heavy_4-stage-2: Node: CPU-1, Layers: resnet18_layer2_0_conv1, resnet18_layer2_0_bn1, resnet18_layer2_0_relu, resnet18_layer2_0_conv2, resnet18_layer2_0_bn2, resnet18_layer2_0_downsample_0, resnet18_layer2_0_downsample_1, add_2, resnet18_layer2_0_relu_1, resnet18_layer2_1_conv1, resnet18_layer2_1_bn1, resnet18_layer2_1_relu, resnet18_layer2_1_conv2, resnet18_layer2_1_bn2, add_3, resnet18_layer2_1_relu_1, resnet18_layer3_0_conv1, resnet18_layer3_0_bn1, resnet18_layer3_0_relu, resnet18_layer3_0_conv2, resnet18_layer3_0_bn2, resnet18_layer3_0_downsample_0, resnet18_layer3_0_downsample_1, add_4, resnet18_layer3_0_relu_1, resnet18_layer3_1_conv1, resnet18_layer3_1_bn1, resnet18_layer3_1_relu, resnet18_layer3_1_conv2, resnet18_layer3_1_bn2, add_5, resnet18_layer3_1_relu_1, resnet18_layer4_0_conv1, resnet18_layer4_0_bn1, resnet18_layer4_0_relu, resnet18_layer4_0_conv2, resnet18_layer4_0_bn2, resnet18_layer4_0_downsample_0, resnet18_layer4_0_downsample_1, add_6, resnet18_layer4_0_relu_1, resnet18_layer4_1_conv1, resnet18_layer4_1_bn1, resnet18_layer4_1_relu, resnet18_layer4_1_conv2, resnet18_layer4_1_bn2, add_7, resnet18_layer4_1_relu_1, resnet18_avgpool, flatten, resnet18_fc, output, Deps: ['heavy_4-stage-1']\n",
            "=== Stage Allocations for Task heavy_5 ===\n",
            "Stage heavy_5-stage-1: Node: CPU-0, Layers: x, resnet18_conv1, resnet18_bn1, resnet18_relu, resnet18_maxpool, resnet18_layer1_0_conv1, resnet18_layer1_0_bn1, resnet18_layer1_0_relu, resnet18_layer1_0_conv2, resnet18_layer1_0_bn2, add, resnet18_layer1_0_relu_1, resnet18_layer1_1_conv1, resnet18_layer1_1_bn1, resnet18_layer1_1_relu, resnet18_layer1_1_conv2, resnet18_layer1_1_bn2, add_1, resnet18_layer1_1_relu_1, Deps: []\n",
            "Stage heavy_5-stage-2: Node: CPU-1, Layers: resnet18_layer2_0_conv1, resnet18_layer2_0_bn1, resnet18_layer2_0_relu, resnet18_layer2_0_conv2, resnet18_layer2_0_bn2, resnet18_layer2_0_downsample_0, resnet18_layer2_0_downsample_1, add_2, resnet18_layer2_0_relu_1, resnet18_layer2_1_conv1, resnet18_layer2_1_bn1, resnet18_layer2_1_relu, resnet18_layer2_1_conv2, resnet18_layer2_1_bn2, add_3, resnet18_layer2_1_relu_1, resnet18_layer3_0_conv1, resnet18_layer3_0_bn1, resnet18_layer3_0_relu, resnet18_layer3_0_conv2, resnet18_layer3_0_bn2, resnet18_layer3_0_downsample_0, resnet18_layer3_0_downsample_1, add_4, resnet18_layer3_0_relu_1, resnet18_layer3_1_conv1, resnet18_layer3_1_bn1, resnet18_layer3_1_relu, resnet18_layer3_1_conv2, resnet18_layer3_1_bn2, add_5, resnet18_layer3_1_relu_1, resnet18_layer4_0_conv1, resnet18_layer4_0_bn1, resnet18_layer4_0_relu, resnet18_layer4_0_conv2, resnet18_layer4_0_bn2, resnet18_layer4_0_downsample_0, resnet18_layer4_0_downsample_1, add_6, resnet18_layer4_0_relu_1, resnet18_layer4_1_conv1, resnet18_layer4_1_bn1, resnet18_layer4_1_relu, resnet18_layer4_1_conv2, resnet18_layer4_1_bn2, add_7, resnet18_layer4_1_relu_1, resnet18_avgpool, flatten, resnet18_fc, output, Deps: ['heavy_5-stage-1']\n",
            "=== Stage Allocations for Task heavy_6 ===\n",
            "Stage heavy_6-stage-1: Node: CPU-0, Layers: x, resnet18_conv1, resnet18_bn1, resnet18_relu, resnet18_maxpool, resnet18_layer1_0_conv1, resnet18_layer1_0_bn1, resnet18_layer1_0_relu, resnet18_layer1_0_conv2, resnet18_layer1_0_bn2, add, resnet18_layer1_0_relu_1, resnet18_layer1_1_conv1, resnet18_layer1_1_bn1, resnet18_layer1_1_relu, resnet18_layer1_1_conv2, resnet18_layer1_1_bn2, add_1, resnet18_layer1_1_relu_1, resnet18_layer2_0_conv1, Deps: []\n",
            "Stage heavy_6-stage-2: Node: CPU-1, Layers: resnet18_layer2_0_bn1, resnet18_layer2_0_relu, resnet18_layer2_0_conv2, resnet18_layer2_0_bn2, resnet18_layer2_0_downsample_0, resnet18_layer2_0_downsample_1, add_2, resnet18_layer2_0_relu_1, resnet18_layer2_1_conv1, resnet18_layer2_1_bn1, resnet18_layer2_1_relu, resnet18_layer2_1_conv2, resnet18_layer2_1_bn2, add_3, resnet18_layer2_1_relu_1, resnet18_layer3_0_conv1, resnet18_layer3_0_bn1, resnet18_layer3_0_relu, resnet18_layer3_0_conv2, resnet18_layer3_0_bn2, resnet18_layer3_0_downsample_0, resnet18_layer3_0_downsample_1, add_4, resnet18_layer3_0_relu_1, resnet18_layer3_1_conv1, resnet18_layer3_1_bn1, resnet18_layer3_1_relu, resnet18_layer3_1_conv2, resnet18_layer3_1_bn2, add_5, resnet18_layer3_1_relu_1, resnet18_layer4_0_conv1, resnet18_layer4_0_bn1, resnet18_layer4_0_relu, resnet18_layer4_0_conv2, resnet18_layer4_0_bn2, resnet18_layer4_0_downsample_0, resnet18_layer4_0_downsample_1, add_6, resnet18_layer4_0_relu_1, resnet18_layer4_1_conv1, resnet18_layer4_1_bn1, resnet18_layer4_1_relu, resnet18_layer4_1_conv2, resnet18_layer4_1_bn2, add_7, resnet18_layer4_1_relu_1, resnet18_avgpool, flatten, resnet18_fc, output, Deps: ['heavy_6-stage-1']\n",
            "=== Stage Allocations for Task heavy_7 ===\n",
            "Stage heavy_7-stage-1: Node: CPU-0, Layers: x, resnet18_conv1, resnet18_bn1, resnet18_relu, resnet18_maxpool, resnet18_layer1_0_conv1, resnet18_layer1_0_bn1, resnet18_layer1_0_relu, resnet18_layer1_0_conv2, resnet18_layer1_0_bn2, add, resnet18_layer1_0_relu_1, resnet18_layer1_1_conv1, resnet18_layer1_1_bn1, resnet18_layer1_1_relu, resnet18_layer1_1_conv2, resnet18_layer1_1_bn2, add_1, resnet18_layer1_1_relu_1, Deps: []\n",
            "Stage heavy_7-stage-2: Node: CPU-1, Layers: resnet18_layer2_0_conv1, resnet18_layer2_0_bn1, resnet18_layer2_0_relu, resnet18_layer2_0_conv2, resnet18_layer2_0_bn2, resnet18_layer2_0_downsample_0, resnet18_layer2_0_downsample_1, add_2, resnet18_layer2_0_relu_1, resnet18_layer2_1_conv1, resnet18_layer2_1_bn1, resnet18_layer2_1_relu, resnet18_layer2_1_conv2, resnet18_layer2_1_bn2, add_3, resnet18_layer2_1_relu_1, resnet18_layer3_0_conv1, resnet18_layer3_0_bn1, resnet18_layer3_0_relu, resnet18_layer3_0_conv2, resnet18_layer3_0_bn2, resnet18_layer3_0_downsample_0, resnet18_layer3_0_downsample_1, add_4, resnet18_layer3_0_relu_1, resnet18_layer3_1_conv1, resnet18_layer3_1_bn1, resnet18_layer3_1_relu, resnet18_layer3_1_conv2, resnet18_layer3_1_bn2, add_5, resnet18_layer3_1_relu_1, resnet18_layer4_0_conv1, resnet18_layer4_0_bn1, resnet18_layer4_0_relu, resnet18_layer4_0_conv2, resnet18_layer4_0_bn2, resnet18_layer4_0_downsample_0, resnet18_layer4_0_downsample_1, add_6, resnet18_layer4_0_relu_1, resnet18_layer4_1_conv1, resnet18_layer4_1_bn1, resnet18_layer4_1_relu, resnet18_layer4_1_conv2, resnet18_layer4_1_bn2, add_7, resnet18_layer4_1_relu_1, resnet18_avgpool, flatten, resnet18_fc, output, Deps: ['heavy_7-stage-1']\n",
            "=== Stage Allocations for Task heavy_8 ===\n",
            "Stage heavy_8-stage-1: Node: CPU-0, Layers: x, resnet18_conv1, resnet18_bn1, resnet18_relu, resnet18_maxpool, resnet18_layer1_0_conv1, resnet18_layer1_0_bn1, resnet18_layer1_0_relu, resnet18_layer1_0_conv2, resnet18_layer1_0_bn2, add, resnet18_layer1_0_relu_1, resnet18_layer1_1_conv1, resnet18_layer1_1_bn1, resnet18_layer1_1_relu, resnet18_layer1_1_conv2, resnet18_layer1_1_bn2, add_1, resnet18_layer1_1_relu_1, resnet18_layer2_0_conv1, resnet18_layer2_0_bn1, resnet18_layer2_0_relu, resnet18_layer2_0_conv2, resnet18_layer2_0_bn2, resnet18_layer2_0_downsample_0, Deps: []\n",
            "Stage heavy_8-stage-2: Node: CPU-1, Layers: resnet18_layer2_0_downsample_1, add_2, resnet18_layer2_0_relu_1, resnet18_layer2_1_conv1, resnet18_layer2_1_bn1, resnet18_layer2_1_relu, resnet18_layer2_1_conv2, resnet18_layer2_1_bn2, add_3, resnet18_layer2_1_relu_1, resnet18_layer3_0_conv1, resnet18_layer3_0_bn1, resnet18_layer3_0_relu, resnet18_layer3_0_conv2, resnet18_layer3_0_bn2, resnet18_layer3_0_downsample_0, resnet18_layer3_0_downsample_1, add_4, resnet18_layer3_0_relu_1, resnet18_layer3_1_conv1, resnet18_layer3_1_bn1, resnet18_layer3_1_relu, resnet18_layer3_1_conv2, resnet18_layer3_1_bn2, add_5, resnet18_layer3_1_relu_1, resnet18_layer4_0_conv1, resnet18_layer4_0_bn1, resnet18_layer4_0_relu, resnet18_layer4_0_conv2, resnet18_layer4_0_bn2, resnet18_layer4_0_downsample_0, resnet18_layer4_0_downsample_1, add_6, resnet18_layer4_0_relu_1, resnet18_layer4_1_conv1, resnet18_layer4_1_bn1, resnet18_layer4_1_relu, resnet18_layer4_1_conv2, resnet18_layer4_1_bn2, add_7, resnet18_layer4_1_relu_1, resnet18_avgpool, flatten, resnet18_fc, output, Deps: ['heavy_8-stage-1']\n",
            "=== Stage Allocations for Task heavy_9 ===\n",
            "Stage heavy_9-stage-1: Node: CPU-0, Layers: x, resnet18_conv1, resnet18_bn1, resnet18_relu, resnet18_maxpool, resnet18_layer1_0_conv1, resnet18_layer1_0_bn1, resnet18_layer1_0_relu, resnet18_layer1_0_conv2, resnet18_layer1_0_bn2, add, resnet18_layer1_0_relu_1, resnet18_layer1_1_conv1, resnet18_layer1_1_bn1, resnet18_layer1_1_relu, resnet18_layer1_1_conv2, resnet18_layer1_1_bn2, add_1, resnet18_layer1_1_relu_1, resnet18_layer2_0_conv1, resnet18_layer2_0_bn1, resnet18_layer2_0_relu, resnet18_layer2_0_conv2, Deps: []\n",
            "Stage heavy_9-stage-2: Node: CPU-1, Layers: resnet18_layer2_0_bn2, resnet18_layer2_0_downsample_0, resnet18_layer2_0_downsample_1, add_2, resnet18_layer2_0_relu_1, resnet18_layer2_1_conv1, resnet18_layer2_1_bn1, resnet18_layer2_1_relu, resnet18_layer2_1_conv2, resnet18_layer2_1_bn2, add_3, resnet18_layer2_1_relu_1, resnet18_layer3_0_conv1, resnet18_layer3_0_bn1, resnet18_layer3_0_relu, resnet18_layer3_0_conv2, resnet18_layer3_0_bn2, resnet18_layer3_0_downsample_0, resnet18_layer3_0_downsample_1, add_4, resnet18_layer3_0_relu_1, resnet18_layer3_1_conv1, resnet18_layer3_1_bn1, resnet18_layer3_1_relu, resnet18_layer3_1_conv2, resnet18_layer3_1_bn2, add_5, resnet18_layer3_1_relu_1, resnet18_layer4_0_conv1, resnet18_layer4_0_bn1, resnet18_layer4_0_relu, resnet18_layer4_0_conv2, resnet18_layer4_0_bn2, resnet18_layer4_0_downsample_0, resnet18_layer4_0_downsample_1, add_6, resnet18_layer4_0_relu_1, resnet18_layer4_1_conv1, resnet18_layer4_1_bn1, resnet18_layer4_1_relu, resnet18_layer4_1_conv2, resnet18_layer4_1_bn2, add_7, resnet18_layer4_1_relu_1, resnet18_avgpool, flatten, resnet18_fc, output, Deps: ['heavy_9-stage-1']\n",
            "=== Stage Allocations for Task light_1 ===\n",
            "Stage light_1-stage-1: Node: CPU-1, Layers: x, conv1, relu, conv2, relu_1, cat, flatten, fc, output, Deps: []\n",
            "=== Stage Allocations for Task light_2 ===\n",
            "Stage light_2-stage-1: Node: CPU-0, Layers: x, conv1, relu, conv2, Deps: []\n",
            "Stage light_2-stage-2: Node: CPU-1, Layers: relu_1, cat, flatten, fc, output, Deps: ['light_2-stage-1']\n",
            "=== Stage Allocations for Task light_3 ===\n",
            "Stage light_3-stage-1: Node: CPU-0, Layers: x, conv1, relu, conv2, Deps: []\n",
            "Stage light_3-stage-2: Node: CPU-1, Layers: relu_1, cat, flatten, fc, output, Deps: ['light_3-stage-1']\n",
            "=== Stage Allocations for Task light_4 ===\n",
            "Stage light_4-stage-1: Node: CPU-0, Layers: x, conv1, relu, conv2, Deps: []\n",
            "Stage light_4-stage-2: Node: CPU-1, Layers: relu_1, cat, flatten, fc, output, Deps: ['light_4-stage-1']\n",
            "=== Stage Allocations for Task light_5 ===\n",
            "Stage light_5-stage-1: Node: CPU-0, Layers: x, conv1, relu, conv2, Deps: []\n",
            "Stage light_5-stage-2: Node: CPU-1, Layers: relu_1, cat, flatten, fc, output, Deps: ['light_5-stage-1']\n",
            "=== Stage Allocations for Task light_6 ===\n",
            "Stage light_6-stage-1: Node: CPU-0, Layers: x, conv1, relu, conv2, Deps: []\n",
            "Stage light_6-stage-2: Node: CPU-1, Layers: relu_1, cat, flatten, fc, output, Deps: ['light_6-stage-1']\n",
            "[Evaluator] Starting Naive Execution.\n",
            "[Evaluator] Task heavy_1: Naive exec time: 0.879179s\n",
            "[Evaluator] Task heavy_2: Naive exec time: 0.869206s\n",
            "[Evaluator] Task heavy_3: Naive exec time: 0.861315s\n",
            "[Evaluator] Task heavy_4: Naive exec time: 0.874732s\n",
            "[Evaluator] Task heavy_5: Naive exec time: 0.868078s\n",
            "[Evaluator] Task heavy_6: Naive exec time: 1.168852s\n",
            "[Evaluator] Task heavy_7: Naive exec time: 1.319895s\n",
            "[Evaluator] Task heavy_8: Naive exec time: 1.320025s\n",
            "[Evaluator] Task heavy_9: Naive exec time: 0.871829s\n",
            "[Evaluator] Task light_1: Naive exec time: 0.002729s\n",
            "[Evaluator] Task light_2: Naive exec time: 0.002547s\n",
            "[Evaluator] Task light_3: Naive exec time: 0.002507s\n",
            "[Evaluator] Task light_4: Naive exec time: 0.002475s\n",
            "[Evaluator] Task light_5: Naive exec time: 0.002542s\n",
            "[Evaluator] Task light_6: Naive exec time: 0.002998s\n",
            "[Evaluator] Naive makespan: 9.078197s\n",
            "\n",
            "[Evaluator] Starting Parallel Execution.\n",
            "[Taskset] Starting Task heavy_1\n",
            "[Taskset] Starting Task heavy_2\n",
            "[Taskset] Starting Task heavy_3\n",
            "[Taskset] Starting Task heavy_4\n",
            "[Taskset] Starting Task heavy_5\n",
            "[Taskset] Starting Task heavy_6\n",
            "[Taskset] Starting Task heavy_7\n",
            "[Taskset] Starting Task heavy_8\n",
            "[Taskset] Starting Task heavy_9\n",
            "[Taskset] Starting Task light_1\n",
            "[Stage] light_1-stage-1: Executed on CPU-1 in 0.004914 s, Transfer: 0.000074 s.\n",
            "[Taskset] Completed Task light_1.\n",
            "[Taskset] Starting Task light_2\n",
            "[Taskset] Starting Task light_3\n",
            "[Taskset] Starting Task light_4\n",
            "[Taskset] Starting Task light_5\n",
            "[Taskset] Starting Task light_6\n",
            "[Stage] heavy_1-stage-1: Executed on CPU-0 in 0.471888 s, Transfer: 0.000325 s.\n",
            "[Stage] heavy_2-stage-1: Executed on CPU-0 in 0.620655 s, Transfer: 0.000294 s.\n",
            "[Stage] heavy_1-stage-2: Executed on CPU-1 in 0.718041 s, Transfer: 0.000638 s.\n",
            "[Taskset] Completed Task heavy_1.\n",
            "[Stage] heavy_3-stage-1: Executed on CPU-0 in 0.596194 s, Transfer: 0.000366 s.\n",
            "[Stage] heavy_2-stage-2: Executed on CPU-1 in 0.835744 s, Transfer: 0.000785 s.\n",
            "[Taskset] Completed Task heavy_2.\n",
            "[Stage] heavy_4-stage-1: Executed on CPU-0 in 0.653036 s, Transfer: 0.000524 s.\n",
            "[Stage] heavy_3-stage-2: Executed on CPU-1 in 0.840173 s, Transfer: 0.000740 s.\n",
            "[Taskset] Completed Task heavy_3.\n",
            "[Stage] heavy_5-stage-1: Executed on CPU-0 in 0.596662 s, Transfer: 0.000323 s.\n",
            "[Stage] heavy_6-stage-1: Executed on CPU-0 in 0.647016 s, Transfer: 0.000346 s.\n",
            "[Stage] heavy_4-stage-2: Executed on CPU-1 in 0.859077 s, Transfer: 0.000759 s.\n",
            "[Taskset] Completed Task heavy_4.\n",
            "[Stage] heavy_7-stage-1: Executed on CPU-0 in 0.691594 s, Transfer: 0.000342 s.\n",
            "[Stage] heavy_5-stage-2: Executed on CPU-1 in 0.842528 s, Transfer: 0.000720 s.\n",
            "[Taskset] Completed Task heavy_5.\n",
            "[Stage] heavy_8-stage-1: Executed on CPU-0 in 0.723773 s, Transfer: 0.000445 s.\n",
            "[Stage] heavy_6-stage-2: Executed on CPU-1 in 0.815026 s, Transfer: 0.000699 s.\n",
            "[Taskset] Completed Task heavy_6.\n",
            "[Stage] heavy_9-stage-1: Executed on CPU-0 in 0.746414 s, Transfer: 0.000382 s.\n",
            "[Stage] light_2-stage-1: Executed on CPU-0 in 0.002653 s, Transfer: 0.000039 s.\n",
            "[Stage] light_3-stage-1: Executed on CPU-0 in 0.002334 s, Transfer: 0.000032 s.\n",
            "[Stage] light_4-stage-1: Executed on CPU-0 in 0.002288 s, Transfer: 0.000030 s.\n",
            "[Stage] light_5-stage-1: Executed on CPU-0 in 0.002234 s, Transfer: 0.000029 s.\n",
            "[Stage] light_6-stage-1: Executed on CPU-0 in 0.002253 s, Transfer: 0.000028 s.\n",
            "[Stage] heavy_7-stage-2: Executed on CPU-1 in 0.662868 s, Transfer: 0.000662 s.\n",
            "[Taskset] Completed Task heavy_7.\n",
            "[Stage] heavy_8-stage-2: Executed on CPU-1 in 0.446859 s, Transfer: 0.000499 s.\n",
            "[Taskset] Completed Task heavy_8.\n",
            "[Stage] heavy_9-stage-2: Executed on CPU-1 in 0.431747 s, Transfer: 0.000569 s.\n",
            "[Taskset] Completed Task heavy_9.\n",
            "[Stage] light_2-stage-2: Executed on CPU-1 in 0.003458 s, Transfer: 0.000037 s.\n",
            "[Taskset] Completed Task light_2.\n",
            "[Stage] light_3-stage-2: Executed on CPU-1 in 0.003062 s, Transfer: 0.000055 s.\n",
            "[Taskset] Completed Task light_3.\n",
            "[Stage] light_4-stage-2: Executed on CPU-1 in 0.003129 s, Transfer: 0.000048 s.\n",
            "[Taskset] Completed Task light_4.\n",
            "[Stage] light_5-stage-2: Executed on CPU-1 in 0.003128 s, Transfer: 0.000039 s.\n",
            "[Taskset] Completed Task light_5.\n",
            "[Stage] light_6-stage-2: Executed on CPU-1 in 0.003032 s, Transfer: 0.000037 s.\n",
            "[Taskset] Completed Task light_6.\n",
            "[Evaluator] Parallel makespan: 6.949883s\n",
            "\n",
            "[Evaluator] Comparing Outputs.\n",
            "[Evaluator] Task heavy_1: Outputs match.\n",
            "[Evaluator] Task heavy_2: Outputs match.\n",
            "[Evaluator] Task heavy_3: Outputs match.\n",
            "[Evaluator] Task heavy_4: Outputs match.\n",
            "[Evaluator] Task heavy_5: Outputs match.\n",
            "[Evaluator] Task heavy_6: Outputs match.\n",
            "[Evaluator] Task heavy_7: Outputs match.\n",
            "[Evaluator] Task heavy_8: Outputs match.\n",
            "[Evaluator] Task heavy_9: Outputs match.\n",
            "[Evaluator] Task light_1: Outputs match.\n",
            "[Evaluator] Task light_2: Outputs match.\n",
            "[Evaluator] Task light_3: Outputs match.\n",
            "[Evaluator] Task light_4: Outputs match.\n",
            "[Evaluator] Task light_5: Outputs match.\n",
            "[Evaluator] Task light_6: Outputs match.\n",
            "[Evaluator] All outputs match.\n",
            "\n",
            "[Evaluator] Analyzing Speedup and Throughput.\n",
            "\n",
            "--- Sum-of-times ---\n",
            "Naive total: 9.048909s, Parallel total: 73.831937s\n",
            "Speedup (sum-of-times): 0.12x\n",
            "Naive Throughput: 1.66 tasks/s, Parallel Throughput: 0.20 tasks/s\n",
            "\n",
            "--- Makespan ---\n",
            "Naive makespan: 9.078197s, Parallel makespan: 6.949883s\n",
            "Speedup (makespan): 1.31x\n",
            "Naive Throughput (makespan): 1.65 tasks/s, Parallel Throughput (makespan): 2.16 tasks/s\n",
            "\n",
            "--- Task Completion Times ---\n",
            "Task heavy_1: Naive finish: 0.886060s, Parallel finish: 1.193395s\n",
            "Task heavy_2: Naive finish: 1.757238s, Parallel finish: 2.030417s\n",
            "Task heavy_3: Naive finish: 2.620471s, Parallel finish: 2.871567s\n",
            "Task heavy_4: Naive finish: 3.497290s, Parallel finish: 3.731508s\n",
            "Task heavy_5: Naive finish: 4.367533s, Parallel finish: 4.575076s\n",
            "Task heavy_6: Naive finish: 5.538575s, Parallel finish: 5.391553s\n",
            "Task heavy_7: Naive finish: 6.860675s, Parallel finish: 6.054198s\n",
            "Task heavy_8: Naive finish: 8.182973s, Parallel finish: 6.501281s\n",
            "Task heavy_9: Naive finish: 9.056602s, Parallel finish: 6.934230s\n",
            "Task light_1: Naive finish: 9.060647s, Parallel finish: 0.018255s\n",
            "Task light_2: Naive finish: 9.064219s, Parallel finish: 6.937467s\n",
            "Task light_3: Naive finish: 9.067867s, Parallel finish: 6.940706s\n",
            "Task light_4: Naive finish: 9.071275s, Parallel finish: 6.943966s\n",
            "Task light_5: Naive finish: 9.074787s, Parallel finish: 6.947209s\n",
            "Task light_6: Naive finish: 9.078107s, Parallel finish: 6.949600s\n",
            "\n",
            "Speedup for ratio 0.6: 1.31\n",
            "Parallel Throughput for ratio 0.6: 2.16\n",
            "Naive Throughput for ratio 0.6: 1.65\n",
            "Makespan for ratio 0.6: 6.95\n",
            "Running experiment with 10 heavy and 5 light tasks.\n",
            "Seed set to 42\n",
            "Using 2 nodes: [Node(CPU-0, cpus=(0,), gpu=None), Node(CPU-1, cpus=(1,), gpu=None)]\n",
            "Creating 10 heavy tasks and 5 light tasks.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
            "  warnings.warn(msg)\n",
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
            "  warnings.warn(msg)\n",
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
            "  warnings.warn(msg)\n",
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
            "  warnings.warn(msg)\n",
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
            "  warnings.warn(msg)\n",
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
            "  warnings.warn(msg)\n",
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
            "  warnings.warn(msg)\n",
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
            "  warnings.warn(msg)\n",
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
            "  warnings.warn(msg)\n",
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Profiler] Starting profiling for Task 'heavy_1' on CPU-0 (device=cpu).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/autograd/profiler.py:263: UserWarning: CUDA is not available, disabling CUDA profiling\n",
            "  warn(\"CUDA is not available, disabling CUDA profiling\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Profiler] Profiling complete. Data saved to profiling_results.csv.\n",
            "[Profiler] Starting profiling for Task 'heavy_1' on CPU-1 (device=cpu).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/autograd/profiler.py:263: UserWarning: CUDA is not available, disabling CUDA profiling\n",
            "  warn(\"CUDA is not available, disabling CUDA profiling\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Profiler] Profiling complete. Data saved to profiling_results.csv.\n",
            "[Profiler] Reused cached profiling data for PretrainedResNet18 on CPU-0.\n",
            "[Profiler] Reused cached profiling data for PretrainedResNet18 on CPU-1.\n",
            "[Profiler] Reused cached profiling data for PretrainedResNet18 on CPU-0.\n",
            "[Profiler] Reused cached profiling data for PretrainedResNet18 on CPU-1.\n",
            "[Profiler] Reused cached profiling data for PretrainedResNet18 on CPU-0.\n",
            "[Profiler] Reused cached profiling data for PretrainedResNet18 on CPU-1.\n",
            "[Profiler] Reused cached profiling data for PretrainedResNet18 on CPU-0.\n",
            "[Profiler] Reused cached profiling data for PretrainedResNet18 on CPU-1.\n",
            "[Profiler] Reused cached profiling data for PretrainedResNet18 on CPU-0.\n",
            "[Profiler] Reused cached profiling data for PretrainedResNet18 on CPU-1.\n",
            "[Profiler] Reused cached profiling data for PretrainedResNet18 on CPU-0.\n",
            "[Profiler] Reused cached profiling data for PretrainedResNet18 on CPU-1.\n",
            "[Profiler] Reused cached profiling data for PretrainedResNet18 on CPU-0.\n",
            "[Profiler] Reused cached profiling data for PretrainedResNet18 on CPU-1.\n",
            "[Profiler] Reused cached profiling data for PretrainedResNet18 on CPU-0.\n",
            "[Profiler] Reused cached profiling data for PretrainedResNet18 on CPU-1.\n",
            "[Profiler] Reused cached profiling data for PretrainedResNet18 on CPU-0.\n",
            "[Profiler] Reused cached profiling data for PretrainedResNet18 on CPU-1.\n",
            "[Profiler] Starting profiling for Task 'light_1' on CPU-0 (device=cpu).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/autograd/profiler.py:263: UserWarning: CUDA is not available, disabling CUDA profiling\n",
            "  warn(\"CUDA is not available, disabling CUDA profiling\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Profiler] Profiling complete. Data saved to profiling_results.csv.\n",
            "[Profiler] Starting profiling for Task 'light_1' on CPU-1 (device=cpu).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/autograd/profiler.py:263: UserWarning: CUDA is not available, disabling CUDA profiling\n",
            "  warn(\"CUDA is not available, disabling CUDA profiling\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Profiler] Profiling complete. Data saved to profiling_results.csv.\n",
            "[Profiler] Reused cached profiling data for SimpleCNN on CPU-0.\n",
            "[Profiler] Reused cached profiling data for SimpleCNN on CPU-1.\n",
            "[Profiler] Reused cached profiling data for SimpleCNN on CPU-0.\n",
            "[Profiler] Reused cached profiling data for SimpleCNN on CPU-1.\n",
            "[Profiler] Reused cached profiling data for SimpleCNN on CPU-0.\n",
            "[Profiler] Reused cached profiling data for SimpleCNN on CPU-1.\n",
            "[Profiler] Reused cached profiling data for SimpleCNN on CPU-0.\n",
            "[Profiler] Reused cached profiling data for SimpleCNN on CPU-1.\n",
            "Measured max transfer penalty: 0.000017 s\n",
            "=== Stage Allocations for Task heavy_1 ===\n",
            "Stage heavy_1-stage-1: Node: CPU-0, Layers: x, resnet18_conv1, resnet18_bn1, resnet18_relu, resnet18_maxpool, resnet18_layer1_0_conv1, resnet18_layer1_0_bn1, resnet18_layer1_0_relu, resnet18_layer1_0_conv2, resnet18_layer1_0_bn2, add, resnet18_layer1_0_relu_1, resnet18_layer1_1_conv1, resnet18_layer1_1_bn1, resnet18_layer1_1_relu, resnet18_layer1_1_conv2, resnet18_layer1_1_bn2, add_1, resnet18_layer1_1_relu_1, resnet18_layer2_0_conv1, resnet18_layer2_0_bn1, resnet18_layer2_0_relu, resnet18_layer2_0_conv2, resnet18_layer2_0_bn2, Deps: []\n",
            "Stage heavy_1-stage-2: Node: CPU-1, Layers: resnet18_layer2_0_downsample_0, resnet18_layer2_0_downsample_1, add_2, resnet18_layer2_0_relu_1, resnet18_layer2_1_conv1, resnet18_layer2_1_bn1, resnet18_layer2_1_relu, resnet18_layer2_1_conv2, resnet18_layer2_1_bn2, add_3, resnet18_layer2_1_relu_1, resnet18_layer3_0_conv1, resnet18_layer3_0_bn1, resnet18_layer3_0_relu, resnet18_layer3_0_conv2, resnet18_layer3_0_bn2, resnet18_layer3_0_downsample_0, resnet18_layer3_0_downsample_1, add_4, resnet18_layer3_0_relu_1, resnet18_layer3_1_conv1, resnet18_layer3_1_bn1, resnet18_layer3_1_relu, resnet18_layer3_1_conv2, resnet18_layer3_1_bn2, add_5, resnet18_layer3_1_relu_1, resnet18_layer4_0_conv1, resnet18_layer4_0_bn1, resnet18_layer4_0_relu, resnet18_layer4_0_conv2, resnet18_layer4_0_bn2, resnet18_layer4_0_downsample_0, resnet18_layer4_0_downsample_1, add_6, resnet18_layer4_0_relu_1, resnet18_layer4_1_conv1, resnet18_layer4_1_bn1, resnet18_layer4_1_relu, resnet18_layer4_1_conv2, resnet18_layer4_1_bn2, add_7, resnet18_layer4_1_relu_1, resnet18_avgpool, flatten, resnet18_fc, output, Deps: ['heavy_1-stage-1']\n",
            "=== Stage Allocations for Task heavy_2 ===\n",
            "Stage heavy_2-stage-1: Node: CPU-0, Layers: x, resnet18_conv1, resnet18_bn1, resnet18_relu, resnet18_maxpool, resnet18_layer1_0_conv1, resnet18_layer1_0_bn1, resnet18_layer1_0_relu, resnet18_layer1_0_conv2, resnet18_layer1_0_bn2, add, resnet18_layer1_0_relu_1, resnet18_layer1_1_conv1, resnet18_layer1_1_bn1, resnet18_layer1_1_relu, resnet18_layer1_1_conv2, resnet18_layer1_1_bn2, add_1, resnet18_layer1_1_relu_1, Deps: []\n",
            "Stage heavy_2-stage-2: Node: CPU-1, Layers: resnet18_layer2_0_conv1, resnet18_layer2_0_bn1, resnet18_layer2_0_relu, resnet18_layer2_0_conv2, resnet18_layer2_0_bn2, resnet18_layer2_0_downsample_0, resnet18_layer2_0_downsample_1, add_2, resnet18_layer2_0_relu_1, resnet18_layer2_1_conv1, resnet18_layer2_1_bn1, resnet18_layer2_1_relu, resnet18_layer2_1_conv2, resnet18_layer2_1_bn2, add_3, resnet18_layer2_1_relu_1, resnet18_layer3_0_conv1, resnet18_layer3_0_bn1, resnet18_layer3_0_relu, resnet18_layer3_0_conv2, resnet18_layer3_0_bn2, resnet18_layer3_0_downsample_0, resnet18_layer3_0_downsample_1, add_4, resnet18_layer3_0_relu_1, resnet18_layer3_1_conv1, resnet18_layer3_1_bn1, resnet18_layer3_1_relu, resnet18_layer3_1_conv2, resnet18_layer3_1_bn2, add_5, resnet18_layer3_1_relu_1, resnet18_layer4_0_conv1, resnet18_layer4_0_bn1, resnet18_layer4_0_relu, resnet18_layer4_0_conv2, resnet18_layer4_0_bn2, resnet18_layer4_0_downsample_0, resnet18_layer4_0_downsample_1, add_6, resnet18_layer4_0_relu_1, resnet18_layer4_1_conv1, resnet18_layer4_1_bn1, resnet18_layer4_1_relu, resnet18_layer4_1_conv2, resnet18_layer4_1_bn2, add_7, resnet18_layer4_1_relu_1, resnet18_avgpool, flatten, resnet18_fc, output, Deps: ['heavy_2-stage-1']\n",
            "=== Stage Allocations for Task heavy_3 ===\n",
            "Stage heavy_3-stage-1: Node: CPU-0, Layers: x, resnet18_conv1, resnet18_bn1, resnet18_relu, resnet18_maxpool, resnet18_layer1_0_conv1, resnet18_layer1_0_bn1, resnet18_layer1_0_relu, resnet18_layer1_0_conv2, resnet18_layer1_0_bn2, add, resnet18_layer1_0_relu_1, resnet18_layer1_1_conv1, resnet18_layer1_1_bn1, resnet18_layer1_1_relu, resnet18_layer1_1_conv2, resnet18_layer1_1_bn2, add_1, resnet18_layer1_1_relu_1, Deps: []\n",
            "Stage heavy_3-stage-2: Node: CPU-1, Layers: resnet18_layer2_0_conv1, resnet18_layer2_0_bn1, resnet18_layer2_0_relu, resnet18_layer2_0_conv2, resnet18_layer2_0_bn2, resnet18_layer2_0_downsample_0, resnet18_layer2_0_downsample_1, add_2, resnet18_layer2_0_relu_1, resnet18_layer2_1_conv1, resnet18_layer2_1_bn1, resnet18_layer2_1_relu, resnet18_layer2_1_conv2, resnet18_layer2_1_bn2, add_3, resnet18_layer2_1_relu_1, resnet18_layer3_0_conv1, resnet18_layer3_0_bn1, resnet18_layer3_0_relu, resnet18_layer3_0_conv2, resnet18_layer3_0_bn2, resnet18_layer3_0_downsample_0, resnet18_layer3_0_downsample_1, add_4, resnet18_layer3_0_relu_1, resnet18_layer3_1_conv1, resnet18_layer3_1_bn1, resnet18_layer3_1_relu, resnet18_layer3_1_conv2, resnet18_layer3_1_bn2, add_5, resnet18_layer3_1_relu_1, resnet18_layer4_0_conv1, resnet18_layer4_0_bn1, resnet18_layer4_0_relu, resnet18_layer4_0_conv2, resnet18_layer4_0_bn2, resnet18_layer4_0_downsample_0, resnet18_layer4_0_downsample_1, add_6, resnet18_layer4_0_relu_1, resnet18_layer4_1_conv1, resnet18_layer4_1_bn1, resnet18_layer4_1_relu, resnet18_layer4_1_conv2, resnet18_layer4_1_bn2, add_7, resnet18_layer4_1_relu_1, resnet18_avgpool, flatten, resnet18_fc, output, Deps: ['heavy_3-stage-1']\n",
            "=== Stage Allocations for Task heavy_4 ===\n",
            "Stage heavy_4-stage-1: Node: CPU-0, Layers: x, resnet18_conv1, resnet18_bn1, resnet18_relu, resnet18_maxpool, resnet18_layer1_0_conv1, resnet18_layer1_0_bn1, resnet18_layer1_0_relu, resnet18_layer1_0_conv2, resnet18_layer1_0_bn2, add, resnet18_layer1_0_relu_1, resnet18_layer1_1_conv1, resnet18_layer1_1_bn1, resnet18_layer1_1_relu, resnet18_layer1_1_conv2, resnet18_layer1_1_bn2, add_1, resnet18_layer1_1_relu_1, Deps: []\n",
            "Stage heavy_4-stage-2: Node: CPU-1, Layers: resnet18_layer2_0_conv1, resnet18_layer2_0_bn1, resnet18_layer2_0_relu, resnet18_layer2_0_conv2, resnet18_layer2_0_bn2, resnet18_layer2_0_downsample_0, resnet18_layer2_0_downsample_1, add_2, resnet18_layer2_0_relu_1, resnet18_layer2_1_conv1, resnet18_layer2_1_bn1, resnet18_layer2_1_relu, resnet18_layer2_1_conv2, resnet18_layer2_1_bn2, add_3, resnet18_layer2_1_relu_1, resnet18_layer3_0_conv1, resnet18_layer3_0_bn1, resnet18_layer3_0_relu, resnet18_layer3_0_conv2, resnet18_layer3_0_bn2, resnet18_layer3_0_downsample_0, resnet18_layer3_0_downsample_1, add_4, resnet18_layer3_0_relu_1, resnet18_layer3_1_conv1, resnet18_layer3_1_bn1, resnet18_layer3_1_relu, resnet18_layer3_1_conv2, resnet18_layer3_1_bn2, add_5, resnet18_layer3_1_relu_1, resnet18_layer4_0_conv1, resnet18_layer4_0_bn1, resnet18_layer4_0_relu, resnet18_layer4_0_conv2, resnet18_layer4_0_bn2, resnet18_layer4_0_downsample_0, resnet18_layer4_0_downsample_1, add_6, resnet18_layer4_0_relu_1, resnet18_layer4_1_conv1, resnet18_layer4_1_bn1, resnet18_layer4_1_relu, resnet18_layer4_1_conv2, resnet18_layer4_1_bn2, add_7, resnet18_layer4_1_relu_1, resnet18_avgpool, flatten, resnet18_fc, output, Deps: ['heavy_4-stage-1']\n",
            "=== Stage Allocations for Task heavy_5 ===\n",
            "Stage heavy_5-stage-1: Node: CPU-0, Layers: x, resnet18_conv1, resnet18_bn1, resnet18_relu, resnet18_maxpool, resnet18_layer1_0_conv1, resnet18_layer1_0_bn1, resnet18_layer1_0_relu, resnet18_layer1_0_conv2, resnet18_layer1_0_bn2, add, resnet18_layer1_0_relu_1, resnet18_layer1_1_conv1, resnet18_layer1_1_bn1, resnet18_layer1_1_relu, resnet18_layer1_1_conv2, resnet18_layer1_1_bn2, add_1, resnet18_layer1_1_relu_1, Deps: []\n",
            "Stage heavy_5-stage-2: Node: CPU-1, Layers: resnet18_layer2_0_conv1, resnet18_layer2_0_bn1, resnet18_layer2_0_relu, resnet18_layer2_0_conv2, resnet18_layer2_0_bn2, resnet18_layer2_0_downsample_0, resnet18_layer2_0_downsample_1, add_2, resnet18_layer2_0_relu_1, resnet18_layer2_1_conv1, resnet18_layer2_1_bn1, resnet18_layer2_1_relu, resnet18_layer2_1_conv2, resnet18_layer2_1_bn2, add_3, resnet18_layer2_1_relu_1, resnet18_layer3_0_conv1, resnet18_layer3_0_bn1, resnet18_layer3_0_relu, resnet18_layer3_0_conv2, resnet18_layer3_0_bn2, resnet18_layer3_0_downsample_0, resnet18_layer3_0_downsample_1, add_4, resnet18_layer3_0_relu_1, resnet18_layer3_1_conv1, resnet18_layer3_1_bn1, resnet18_layer3_1_relu, resnet18_layer3_1_conv2, resnet18_layer3_1_bn2, add_5, resnet18_layer3_1_relu_1, resnet18_layer4_0_conv1, resnet18_layer4_0_bn1, resnet18_layer4_0_relu, resnet18_layer4_0_conv2, resnet18_layer4_0_bn2, resnet18_layer4_0_downsample_0, resnet18_layer4_0_downsample_1, add_6, resnet18_layer4_0_relu_1, resnet18_layer4_1_conv1, resnet18_layer4_1_bn1, resnet18_layer4_1_relu, resnet18_layer4_1_conv2, resnet18_layer4_1_bn2, add_7, resnet18_layer4_1_relu_1, resnet18_avgpool, flatten, resnet18_fc, output, Deps: ['heavy_5-stage-1']\n",
            "=== Stage Allocations for Task heavy_6 ===\n",
            "Stage heavy_6-stage-1: Node: CPU-0, Layers: x, resnet18_conv1, resnet18_bn1, resnet18_relu, resnet18_maxpool, resnet18_layer1_0_conv1, resnet18_layer1_0_bn1, resnet18_layer1_0_relu, resnet18_layer1_0_conv2, resnet18_layer1_0_bn2, add, resnet18_layer1_0_relu_1, resnet18_layer1_1_conv1, resnet18_layer1_1_bn1, resnet18_layer1_1_relu, resnet18_layer1_1_conv2, resnet18_layer1_1_bn2, add_1, resnet18_layer1_1_relu_1, resnet18_layer2_0_conv1, Deps: []\n",
            "Stage heavy_6-stage-2: Node: CPU-1, Layers: resnet18_layer2_0_bn1, resnet18_layer2_0_relu, resnet18_layer2_0_conv2, resnet18_layer2_0_bn2, resnet18_layer2_0_downsample_0, resnet18_layer2_0_downsample_1, add_2, resnet18_layer2_0_relu_1, resnet18_layer2_1_conv1, resnet18_layer2_1_bn1, resnet18_layer2_1_relu, resnet18_layer2_1_conv2, resnet18_layer2_1_bn2, add_3, resnet18_layer2_1_relu_1, resnet18_layer3_0_conv1, resnet18_layer3_0_bn1, resnet18_layer3_0_relu, resnet18_layer3_0_conv2, resnet18_layer3_0_bn2, resnet18_layer3_0_downsample_0, resnet18_layer3_0_downsample_1, add_4, resnet18_layer3_0_relu_1, resnet18_layer3_1_conv1, resnet18_layer3_1_bn1, resnet18_layer3_1_relu, resnet18_layer3_1_conv2, resnet18_layer3_1_bn2, add_5, resnet18_layer3_1_relu_1, resnet18_layer4_0_conv1, resnet18_layer4_0_bn1, resnet18_layer4_0_relu, resnet18_layer4_0_conv2, resnet18_layer4_0_bn2, resnet18_layer4_0_downsample_0, resnet18_layer4_0_downsample_1, add_6, resnet18_layer4_0_relu_1, resnet18_layer4_1_conv1, resnet18_layer4_1_bn1, resnet18_layer4_1_relu, resnet18_layer4_1_conv2, resnet18_layer4_1_bn2, add_7, resnet18_layer4_1_relu_1, resnet18_avgpool, flatten, resnet18_fc, output, Deps: ['heavy_6-stage-1']\n",
            "=== Stage Allocations for Task heavy_7 ===\n",
            "Stage heavy_7-stage-1: Node: CPU-0, Layers: x, resnet18_conv1, resnet18_bn1, resnet18_relu, resnet18_maxpool, resnet18_layer1_0_conv1, resnet18_layer1_0_bn1, resnet18_layer1_0_relu, resnet18_layer1_0_conv2, resnet18_layer1_0_bn2, add, resnet18_layer1_0_relu_1, resnet18_layer1_1_conv1, resnet18_layer1_1_bn1, resnet18_layer1_1_relu, resnet18_layer1_1_conv2, resnet18_layer1_1_bn2, add_1, resnet18_layer1_1_relu_1, Deps: []\n",
            "Stage heavy_7-stage-2: Node: CPU-1, Layers: resnet18_layer2_0_conv1, resnet18_layer2_0_bn1, resnet18_layer2_0_relu, resnet18_layer2_0_conv2, resnet18_layer2_0_bn2, resnet18_layer2_0_downsample_0, resnet18_layer2_0_downsample_1, add_2, resnet18_layer2_0_relu_1, resnet18_layer2_1_conv1, resnet18_layer2_1_bn1, resnet18_layer2_1_relu, resnet18_layer2_1_conv2, resnet18_layer2_1_bn2, add_3, resnet18_layer2_1_relu_1, resnet18_layer3_0_conv1, resnet18_layer3_0_bn1, resnet18_layer3_0_relu, resnet18_layer3_0_conv2, resnet18_layer3_0_bn2, resnet18_layer3_0_downsample_0, resnet18_layer3_0_downsample_1, add_4, resnet18_layer3_0_relu_1, resnet18_layer3_1_conv1, resnet18_layer3_1_bn1, resnet18_layer3_1_relu, resnet18_layer3_1_conv2, resnet18_layer3_1_bn2, add_5, resnet18_layer3_1_relu_1, resnet18_layer4_0_conv1, resnet18_layer4_0_bn1, resnet18_layer4_0_relu, resnet18_layer4_0_conv2, resnet18_layer4_0_bn2, resnet18_layer4_0_downsample_0, resnet18_layer4_0_downsample_1, add_6, resnet18_layer4_0_relu_1, resnet18_layer4_1_conv1, resnet18_layer4_1_bn1, resnet18_layer4_1_relu, resnet18_layer4_1_conv2, resnet18_layer4_1_bn2, add_7, resnet18_layer4_1_relu_1, resnet18_avgpool, flatten, resnet18_fc, output, Deps: ['heavy_7-stage-1']\n",
            "=== Stage Allocations for Task heavy_8 ===\n",
            "Stage heavy_8-stage-1: Node: CPU-0, Layers: x, resnet18_conv1, resnet18_bn1, resnet18_relu, resnet18_maxpool, resnet18_layer1_0_conv1, resnet18_layer1_0_bn1, resnet18_layer1_0_relu, resnet18_layer1_0_conv2, resnet18_layer1_0_bn2, add, resnet18_layer1_0_relu_1, resnet18_layer1_1_conv1, resnet18_layer1_1_bn1, resnet18_layer1_1_relu, resnet18_layer1_1_conv2, resnet18_layer1_1_bn2, add_1, resnet18_layer1_1_relu_1, resnet18_layer2_0_conv1, resnet18_layer2_0_bn1, resnet18_layer2_0_relu, resnet18_layer2_0_conv2, resnet18_layer2_0_bn2, resnet18_layer2_0_downsample_0, Deps: []\n",
            "Stage heavy_8-stage-2: Node: CPU-1, Layers: resnet18_layer2_0_downsample_1, add_2, resnet18_layer2_0_relu_1, resnet18_layer2_1_conv1, resnet18_layer2_1_bn1, resnet18_layer2_1_relu, resnet18_layer2_1_conv2, resnet18_layer2_1_bn2, add_3, resnet18_layer2_1_relu_1, resnet18_layer3_0_conv1, resnet18_layer3_0_bn1, resnet18_layer3_0_relu, resnet18_layer3_0_conv2, resnet18_layer3_0_bn2, resnet18_layer3_0_downsample_0, resnet18_layer3_0_downsample_1, add_4, resnet18_layer3_0_relu_1, resnet18_layer3_1_conv1, resnet18_layer3_1_bn1, resnet18_layer3_1_relu, resnet18_layer3_1_conv2, resnet18_layer3_1_bn2, add_5, resnet18_layer3_1_relu_1, resnet18_layer4_0_conv1, resnet18_layer4_0_bn1, resnet18_layer4_0_relu, resnet18_layer4_0_conv2, resnet18_layer4_0_bn2, resnet18_layer4_0_downsample_0, resnet18_layer4_0_downsample_1, add_6, resnet18_layer4_0_relu_1, resnet18_layer4_1_conv1, resnet18_layer4_1_bn1, resnet18_layer4_1_relu, resnet18_layer4_1_conv2, resnet18_layer4_1_bn2, add_7, resnet18_layer4_1_relu_1, resnet18_avgpool, flatten, resnet18_fc, output, Deps: ['heavy_8-stage-1']\n",
            "=== Stage Allocations for Task heavy_9 ===\n",
            "Stage heavy_9-stage-1: Node: CPU-0, Layers: x, resnet18_conv1, resnet18_bn1, resnet18_relu, resnet18_maxpool, resnet18_layer1_0_conv1, resnet18_layer1_0_bn1, resnet18_layer1_0_relu, resnet18_layer1_0_conv2, resnet18_layer1_0_bn2, add, resnet18_layer1_0_relu_1, resnet18_layer1_1_conv1, resnet18_layer1_1_bn1, resnet18_layer1_1_relu, resnet18_layer1_1_conv2, resnet18_layer1_1_bn2, add_1, resnet18_layer1_1_relu_1, resnet18_layer2_0_conv1, resnet18_layer2_0_bn1, resnet18_layer2_0_relu, resnet18_layer2_0_conv2, Deps: []\n",
            "Stage heavy_9-stage-2: Node: CPU-1, Layers: resnet18_layer2_0_bn2, resnet18_layer2_0_downsample_0, resnet18_layer2_0_downsample_1, add_2, resnet18_layer2_0_relu_1, resnet18_layer2_1_conv1, resnet18_layer2_1_bn1, resnet18_layer2_1_relu, resnet18_layer2_1_conv2, resnet18_layer2_1_bn2, add_3, resnet18_layer2_1_relu_1, resnet18_layer3_0_conv1, resnet18_layer3_0_bn1, resnet18_layer3_0_relu, resnet18_layer3_0_conv2, resnet18_layer3_0_bn2, resnet18_layer3_0_downsample_0, resnet18_layer3_0_downsample_1, add_4, resnet18_layer3_0_relu_1, resnet18_layer3_1_conv1, resnet18_layer3_1_bn1, resnet18_layer3_1_relu, resnet18_layer3_1_conv2, resnet18_layer3_1_bn2, add_5, resnet18_layer3_1_relu_1, resnet18_layer4_0_conv1, resnet18_layer4_0_bn1, resnet18_layer4_0_relu, resnet18_layer4_0_conv2, resnet18_layer4_0_bn2, resnet18_layer4_0_downsample_0, resnet18_layer4_0_downsample_1, add_6, resnet18_layer4_0_relu_1, resnet18_layer4_1_conv1, resnet18_layer4_1_bn1, resnet18_layer4_1_relu, resnet18_layer4_1_conv2, resnet18_layer4_1_bn2, add_7, resnet18_layer4_1_relu_1, resnet18_avgpool, flatten, resnet18_fc, output, Deps: ['heavy_9-stage-1']\n",
            "=== Stage Allocations for Task heavy_10 ===\n",
            "Stage heavy_10-stage-1: Node: CPU-0, Layers: x, resnet18_conv1, resnet18_bn1, resnet18_relu, resnet18_maxpool, resnet18_layer1_0_conv1, resnet18_layer1_0_bn1, resnet18_layer1_0_relu, resnet18_layer1_0_conv2, resnet18_layer1_0_bn2, add, resnet18_layer1_0_relu_1, resnet18_layer1_1_conv1, resnet18_layer1_1_bn1, resnet18_layer1_1_relu, resnet18_layer1_1_conv2, resnet18_layer1_1_bn2, add_1, resnet18_layer1_1_relu_1, resnet18_layer2_0_conv1, resnet18_layer2_0_bn1, resnet18_layer2_0_relu, resnet18_layer2_0_conv2, Deps: []\n",
            "Stage heavy_10-stage-2: Node: CPU-1, Layers: resnet18_layer2_0_bn2, resnet18_layer2_0_downsample_0, resnet18_layer2_0_downsample_1, add_2, resnet18_layer2_0_relu_1, resnet18_layer2_1_conv1, resnet18_layer2_1_bn1, resnet18_layer2_1_relu, resnet18_layer2_1_conv2, resnet18_layer2_1_bn2, add_3, resnet18_layer2_1_relu_1, resnet18_layer3_0_conv1, resnet18_layer3_0_bn1, resnet18_layer3_0_relu, resnet18_layer3_0_conv2, resnet18_layer3_0_bn2, resnet18_layer3_0_downsample_0, resnet18_layer3_0_downsample_1, add_4, resnet18_layer3_0_relu_1, resnet18_layer3_1_conv1, resnet18_layer3_1_bn1, resnet18_layer3_1_relu, resnet18_layer3_1_conv2, resnet18_layer3_1_bn2, add_5, resnet18_layer3_1_relu_1, resnet18_layer4_0_conv1, resnet18_layer4_0_bn1, resnet18_layer4_0_relu, resnet18_layer4_0_conv2, resnet18_layer4_0_bn2, resnet18_layer4_0_downsample_0, resnet18_layer4_0_downsample_1, add_6, resnet18_layer4_0_relu_1, resnet18_layer4_1_conv1, resnet18_layer4_1_bn1, resnet18_layer4_1_relu, resnet18_layer4_1_conv2, resnet18_layer4_1_bn2, add_7, resnet18_layer4_1_relu_1, resnet18_avgpool, flatten, resnet18_fc, output, Deps: ['heavy_10-stage-1']\n",
            "=== Stage Allocations for Task light_1 ===\n",
            "Stage light_1-stage-1: Node: CPU-1, Layers: x, conv1, relu, conv2, relu_1, cat, flatten, fc, output, Deps: []\n",
            "=== Stage Allocations for Task light_2 ===\n",
            "Stage light_2-stage-1: Node: CPU-0, Layers: x, conv1, relu, Deps: []\n",
            "Stage light_2-stage-2: Node: CPU-1, Layers: conv2, relu_1, cat, flatten, fc, output, Deps: ['light_2-stage-1']\n",
            "=== Stage Allocations for Task light_3 ===\n",
            "Stage light_3-stage-1: Node: CPU-0, Layers: x, conv1, relu, conv2, relu_1, Deps: []\n",
            "Stage light_3-stage-2: Node: CPU-1, Layers: cat, flatten, fc, output, Deps: ['light_3-stage-1']\n",
            "=== Stage Allocations for Task light_4 ===\n",
            "Stage light_4-stage-1: Node: CPU-0, Layers: x, conv1, relu, conv2, Deps: []\n",
            "Stage light_4-stage-2: Node: CPU-1, Layers: relu_1, cat, flatten, fc, output, Deps: ['light_4-stage-1']\n",
            "=== Stage Allocations for Task light_5 ===\n",
            "Stage light_5-stage-1: Node: CPU-0, Layers: x, conv1, relu, conv2, Deps: []\n",
            "Stage light_5-stage-2: Node: CPU-1, Layers: relu_1, cat, flatten, fc, output, Deps: ['light_5-stage-1']\n",
            "[Evaluator] Starting Naive Execution.\n",
            "[Evaluator] Task heavy_1: Naive exec time: 1.364698s\n",
            "[Evaluator] Task heavy_2: Naive exec time: 1.150577s\n",
            "[Evaluator] Task heavy_3: Naive exec time: 0.870237s\n",
            "[Evaluator] Task heavy_4: Naive exec time: 0.874327s\n",
            "[Evaluator] Task heavy_5: Naive exec time: 0.868791s\n",
            "[Evaluator] Task heavy_6: Naive exec time: 0.873285s\n",
            "[Evaluator] Task heavy_7: Naive exec time: 0.855128s\n",
            "[Evaluator] Task heavy_8: Naive exec time: 0.857933s\n",
            "[Evaluator] Task heavy_9: Naive exec time: 0.860638s\n",
            "[Evaluator] Task heavy_10: Naive exec time: 0.866313s\n",
            "[Evaluator] Task light_1: Naive exec time: 0.002598s\n",
            "[Evaluator] Task light_2: Naive exec time: 0.002483s\n",
            "[Evaluator] Task light_3: Naive exec time: 0.002501s\n",
            "[Evaluator] Task light_4: Naive exec time: 0.002837s\n",
            "[Evaluator] Task light_5: Naive exec time: 0.002473s\n",
            "[Evaluator] Naive makespan: 9.478464s\n",
            "\n",
            "[Evaluator] Starting Parallel Execution.\n",
            "[Taskset] Starting Task heavy_1\n",
            "[Taskset] Starting Task heavy_2\n",
            "[Taskset] Starting Task heavy_3\n",
            "[Taskset] Starting Task heavy_4\n",
            "[Taskset] Starting Task heavy_5\n",
            "[Taskset] Starting Task heavy_6\n",
            "[Taskset] Starting Task heavy_7\n",
            "[Taskset] Starting Task heavy_8\n",
            "[Taskset] Starting Task heavy_9\n",
            "[Taskset] Starting Task heavy_10\n",
            "[Taskset] Starting Task light_1\n",
            "[Taskset] Starting Task light_2\n",
            "[Stage] light_1-stage-1: Executed on CPU-1 in 0.007521 s, Transfer: 0.000074 s.\n",
            "[Taskset] Starting Task light_3\n",
            "[Taskset] Completed Task light_1.\n",
            "[Taskset] Starting Task light_4[Taskset] Starting Task light_5\n",
            "\n",
            "[Stage] heavy_1-stage-1: Executed on CPU-0 in 0.475101 s, Transfer: 0.000357 s.\n",
            "[Stage] heavy_2-stage-1: Executed on CPU-0 in 0.711744 s, Transfer: 0.000309 s.\n",
            "[Stage] heavy_1-stage-2: Executed on CPU-1 in 0.811579 s, Transfer: 0.000662 s.\n",
            "[Taskset] Completed Task heavy_1.\n",
            "[Stage] heavy_3-stage-1: Executed on CPU-0 in 0.670511 s, Transfer: 0.000311 s.\n",
            "[Stage] heavy_2-stage-2: Executed on CPU-1 in 0.875342 s, Transfer: 0.000788 s.\n",
            "[Taskset] Completed Task heavy_2.\n",
            "[Stage] heavy_4-stage-1: Executed on CPU-0 in 0.647626 s, Transfer: 0.000354 s.\n",
            "[Stage] heavy_3-stage-2: Executed on CPU-1 in 1.060966 s, Transfer: 0.000757 s.\n",
            "[Taskset] Completed Task heavy_3.\n",
            "[Stage] heavy_5-stage-1: Executed on CPU-0 in 0.818179 s, Transfer: 0.000303 s.\n",
            "[Stage] heavy_6-stage-1: Executed on CPU-0 in 0.852940 s, Transfer: 0.000358 s.\n",
            "[Stage] heavy_4-stage-2: Executed on CPU-1 in 1.225075 s, Transfer: 0.000753 s.\n",
            "[Taskset] Completed Task heavy_4.\n",
            "[Stage] heavy_7-stage-1: Executed on CPU-0 in 1.076489 s, Transfer: 0.000338 s.\n",
            "[Stage] heavy_5-stage-2: Executed on CPU-1 in 1.116052 s, Transfer: 0.000732 s.\n",
            "[Taskset] Completed Task heavy_5.\n",
            "[Stage] heavy_8-stage-1: Executed on CPU-0 in 1.238543 s, Transfer: 0.000494 s.\n",
            "[Stage] heavy_6-stage-2: Executed on CPU-1 in 1.109512 s, Transfer: 0.000695 s.\n",
            "[Taskset] Completed Task heavy_6.\n",
            "[Stage] heavy_9-stage-1: Executed on CPU-0 in 1.136672 s, Transfer: 0.000432 s.\n",
            "[Stage] heavy_7-stage-2: Executed on CPU-1 in 1.137999 s, Transfer: 0.000770 s.\n",
            "[Taskset] Completed Task heavy_7.\n",
            "[Stage] heavy_10-stage-1: Executed on CPU-0 in 0.771897 s, Transfer: 0.000383 s.\n",
            "[Stage] light_2-stage-1: Executed on CPU-0 in 0.001638 s, Transfer: 0.000031 s.\n",
            "[Stage] light_3-stage-1: Executed on CPU-0 in 0.003110 s, Transfer: 0.000036 s.\n",
            "[Stage] light_4-stage-1: Executed on CPU-0 in 0.002573 s, Transfer: 0.000029 s.\n",
            "[Stage] light_5-stage-1: Executed on CPU-0 in 0.002693 s, Transfer: 0.000031 s.\n",
            "[Stage] heavy_8-stage-2: Executed on CPU-1 in 0.678336 s, Transfer: 0.000600 s.\n",
            "[Taskset] Completed Task heavy_8.\n",
            "[Stage] heavy_9-stage-2: Executed on CPU-1 in 0.445407 s, Transfer: 0.000493 s.\n",
            "[Taskset] Completed Task heavy_9.\n",
            "[Stage] heavy_10-stage-2: Executed on CPU-1 in 0.445539 s, Transfer: 0.000501 s.\n",
            "[Taskset] Completed Task heavy_10.\n",
            "[Stage] light_2-stage-2: Executed on CPU-1 in 0.004367 s, Transfer: 0.000047 s.\n",
            "[Taskset] Completed Task light_2.\n",
            "[Stage] light_3-stage-2: Executed on CPU-1 in 0.005818 s, Transfer: 0.000036 s.\n",
            "[Taskset] Completed Task light_3.\n",
            "[Stage] light_4-stage-2: Executed on CPU-1 in 0.024670 s, Transfer: 0.000060 s.\n",
            "[Taskset] Completed Task light_4.\n",
            "[Stage] light_5-stage-2: Executed on CPU-1 in 0.003169 s, Transfer: 0.000040 s.\n",
            "[Taskset] Completed Task light_5.\n",
            "[Evaluator] Parallel makespan: 9.434967s\n",
            "\n",
            "[Evaluator] Comparing Outputs.\n",
            "[Evaluator] Task heavy_1: Outputs match.\n",
            "[Evaluator] Task heavy_2: Outputs match.\n",
            "[Evaluator] Task heavy_3: Outputs match.\n",
            "[Evaluator] Task heavy_4: Outputs match.\n",
            "[Evaluator] Task heavy_5: Outputs match.\n",
            "[Evaluator] Task heavy_6: Outputs match.\n",
            "[Evaluator] Task heavy_7: Outputs match.\n",
            "[Evaluator] Task heavy_8: Outputs match.\n",
            "[Evaluator] Task heavy_9: Outputs match.\n",
            "[Evaluator] Task heavy_10: Outputs match.\n",
            "[Evaluator] Task light_1: Outputs match.\n",
            "[Evaluator] Task light_2: Outputs match.\n",
            "[Evaluator] Task light_3: Outputs match.\n",
            "[Evaluator] Task light_4: Outputs match.\n",
            "[Evaluator] Task light_5: Outputs match.\n",
            "[Evaluator] All outputs match.\n",
            "\n",
            "[Evaluator] Analyzing Speedup and Throughput.\n",
            "\n",
            "--- Sum-of-times ---\n",
            "Naive total: 9.454820s, Parallel total: 95.563726s\n",
            "Speedup (sum-of-times): 0.10x\n",
            "Naive Throughput: 1.59 tasks/s, Parallel Throughput: 0.16 tasks/s\n",
            "\n",
            "--- Makespan ---\n",
            "Naive makespan: 9.478464s, Parallel makespan: 9.434967s\n",
            "Speedup (makespan): 1.00x\n",
            "Naive Throughput (makespan): 1.58 tasks/s, Parallel Throughput (makespan): 1.59 tasks/s\n",
            "\n",
            "--- Task Completion Times ---\n",
            "Task heavy_1: Naive finish: 1.365949s, Parallel finish: 1.291264s\n",
            "Task heavy_2: Naive finish: 2.518785s, Parallel finish: 2.167521s\n",
            "Task heavy_3: Naive finish: 3.391012s, Parallel finish: 3.228694s\n",
            "Task heavy_4: Naive finish: 4.267296s, Parallel finish: 4.456520s\n",
            "Task heavy_5: Naive finish: 5.138128s, Parallel finish: 5.571300s\n",
            "Task heavy_6: Naive finish: 6.013326s, Parallel finish: 6.681845s\n",
            "Task heavy_7: Naive finish: 6.870326s, Parallel finish: 7.820752s\n",
            "Task heavy_8: Naive finish: 7.730180s, Parallel finish: 8.499246s\n",
            "Task heavy_9: Naive finish: 8.592776s, Parallel finish: 8.944952s\n",
            "Task heavy_10: Naive finish: 9.461044s, Parallel finish: 9.390622s\n",
            "Task light_1: Naive finish: 9.464915s, Parallel finish: 0.020160s\n",
            "Task light_2: Naive finish: 9.468366s, Parallel finish: 9.395267s\n",
            "Task light_3: Naive finish: 9.471829s, Parallel finish: 9.401996s\n",
            "Task light_4: Naive finish: 9.475005s, Parallel finish: 9.426932s\n",
            "Task light_5: Naive finish: 9.478370s, Parallel finish: 9.429333s\n",
            "\n",
            "Speedup for ratio 0.7: 1.00\n",
            "Parallel Throughput for ratio 0.7: 1.59\n",
            "Naive Throughput for ratio 0.7: 1.58\n",
            "Makespan for ratio 0.7: 9.43\n",
            "Running experiment with 12 heavy and 3 light tasks.\n",
            "Seed set to 42\n",
            "Using 2 nodes: [Node(CPU-0, cpus=(0,), gpu=None), Node(CPU-1, cpus=(1,), gpu=None)]\n",
            "Creating 12 heavy tasks and 3 light tasks.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
            "  warnings.warn(msg)\n",
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
            "  warnings.warn(msg)\n",
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
            "  warnings.warn(msg)\n",
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
            "  warnings.warn(msg)\n",
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
            "  warnings.warn(msg)\n",
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
            "  warnings.warn(msg)\n",
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
            "  warnings.warn(msg)\n",
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
            "  warnings.warn(msg)\n",
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
            "  warnings.warn(msg)\n",
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
            "  warnings.warn(msg)\n",
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
            "  warnings.warn(msg)\n",
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Profiler] Starting profiling for Task 'heavy_1' on CPU-0 (device=cpu).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/autograd/profiler.py:263: UserWarning: CUDA is not available, disabling CUDA profiling\n",
            "  warn(\"CUDA is not available, disabling CUDA profiling\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Profiler] Profiling complete. Data saved to profiling_results.csv.\n",
            "[Profiler] Starting profiling for Task 'heavy_1' on CPU-1 (device=cpu).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/autograd/profiler.py:263: UserWarning: CUDA is not available, disabling CUDA profiling\n",
            "  warn(\"CUDA is not available, disabling CUDA profiling\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Profiler] Profiling complete. Data saved to profiling_results.csv.\n",
            "[Profiler] Reused cached profiling data for PretrainedResNet18 on CPU-0.\n",
            "[Profiler] Reused cached profiling data for PretrainedResNet18 on CPU-1.\n",
            "[Profiler] Reused cached profiling data for PretrainedResNet18 on CPU-0.\n",
            "[Profiler] Reused cached profiling data for PretrainedResNet18 on CPU-1.\n",
            "[Profiler] Reused cached profiling data for PretrainedResNet18 on CPU-0.\n",
            "[Profiler] Reused cached profiling data for PretrainedResNet18 on CPU-1.\n",
            "[Profiler] Reused cached profiling data for PretrainedResNet18 on CPU-0.\n",
            "[Profiler] Reused cached profiling data for PretrainedResNet18 on CPU-1.\n",
            "[Profiler] Reused cached profiling data for PretrainedResNet18 on CPU-0.\n",
            "[Profiler] Reused cached profiling data for PretrainedResNet18 on CPU-1.\n",
            "[Profiler] Reused cached profiling data for PretrainedResNet18 on CPU-0.\n",
            "[Profiler] Reused cached profiling data for PretrainedResNet18 on CPU-1.\n",
            "[Profiler] Reused cached profiling data for PretrainedResNet18 on CPU-0.\n",
            "[Profiler] Reused cached profiling data for PretrainedResNet18 on CPU-1.\n",
            "[Profiler] Reused cached profiling data for PretrainedResNet18 on CPU-0.\n",
            "[Profiler] Reused cached profiling data for PretrainedResNet18 on CPU-1.\n",
            "[Profiler] Reused cached profiling data for PretrainedResNet18 on CPU-0.\n",
            "[Profiler] Reused cached profiling data for PretrainedResNet18 on CPU-1.\n",
            "[Profiler] Reused cached profiling data for PretrainedResNet18 on CPU-0.\n",
            "[Profiler] Reused cached profiling data for PretrainedResNet18 on CPU-1.\n",
            "[Profiler] Reused cached profiling data for PretrainedResNet18 on CPU-0.\n",
            "[Profiler] Reused cached profiling data for PretrainedResNet18 on CPU-1.\n",
            "[Profiler] Starting profiling for Task 'light_1' on CPU-0 (device=cpu).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/autograd/profiler.py:263: UserWarning: CUDA is not available, disabling CUDA profiling\n",
            "  warn(\"CUDA is not available, disabling CUDA profiling\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Profiler] Profiling complete. Data saved to profiling_results.csv.\n",
            "[Profiler] Starting profiling for Task 'light_1' on CPU-1 (device=cpu).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/autograd/profiler.py:263: UserWarning: CUDA is not available, disabling CUDA profiling\n",
            "  warn(\"CUDA is not available, disabling CUDA profiling\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Profiler] Profiling complete. Data saved to profiling_results.csv.\n",
            "[Profiler] Reused cached profiling data for SimpleCNN on CPU-0.\n",
            "[Profiler] Reused cached profiling data for SimpleCNN on CPU-1.\n",
            "[Profiler] Reused cached profiling data for SimpleCNN on CPU-0.\n",
            "[Profiler] Reused cached profiling data for SimpleCNN on CPU-1.\n",
            "Measured max transfer penalty: 0.000049 s\n",
            "=== Stage Allocations for Task heavy_1 ===\n",
            "Stage heavy_1-stage-1: Node: CPU-0, Layers: x, resnet18_conv1, resnet18_bn1, resnet18_relu, resnet18_maxpool, resnet18_layer1_0_conv1, resnet18_layer1_0_bn1, resnet18_layer1_0_relu, resnet18_layer1_0_conv2, resnet18_layer1_0_bn2, add, resnet18_layer1_0_relu_1, resnet18_layer1_1_conv1, resnet18_layer1_1_bn1, resnet18_layer1_1_relu, resnet18_layer1_1_conv2, resnet18_layer1_1_bn2, add_1, resnet18_layer1_1_relu_1, resnet18_layer2_0_conv1, resnet18_layer2_0_bn1, resnet18_layer2_0_relu, resnet18_layer2_0_conv2, Deps: []\n",
            "Stage heavy_1-stage-2: Node: CPU-1, Layers: resnet18_layer2_0_bn2, resnet18_layer2_0_downsample_0, resnet18_layer2_0_downsample_1, add_2, resnet18_layer2_0_relu_1, resnet18_layer2_1_conv1, resnet18_layer2_1_bn1, resnet18_layer2_1_relu, resnet18_layer2_1_conv2, resnet18_layer2_1_bn2, add_3, resnet18_layer2_1_relu_1, resnet18_layer3_0_conv1, resnet18_layer3_0_bn1, resnet18_layer3_0_relu, resnet18_layer3_0_conv2, resnet18_layer3_0_bn2, resnet18_layer3_0_downsample_0, resnet18_layer3_0_downsample_1, add_4, resnet18_layer3_0_relu_1, resnet18_layer3_1_conv1, resnet18_layer3_1_bn1, resnet18_layer3_1_relu, resnet18_layer3_1_conv2, resnet18_layer3_1_bn2, add_5, resnet18_layer3_1_relu_1, resnet18_layer4_0_conv1, resnet18_layer4_0_bn1, resnet18_layer4_0_relu, resnet18_layer4_0_conv2, resnet18_layer4_0_bn2, resnet18_layer4_0_downsample_0, resnet18_layer4_0_downsample_1, add_6, resnet18_layer4_0_relu_1, resnet18_layer4_1_conv1, resnet18_layer4_1_bn1, resnet18_layer4_1_relu, resnet18_layer4_1_conv2, resnet18_layer4_1_bn2, add_7, resnet18_layer4_1_relu_1, resnet18_avgpool, flatten, resnet18_fc, output, Deps: ['heavy_1-stage-1']\n",
            "=== Stage Allocations for Task heavy_2 ===\n",
            "Stage heavy_2-stage-1: Node: CPU-0, Layers: x, resnet18_conv1, resnet18_bn1, resnet18_relu, resnet18_maxpool, resnet18_layer1_0_conv1, resnet18_layer1_0_bn1, resnet18_layer1_0_relu, resnet18_layer1_0_conv2, resnet18_layer1_0_bn2, add, resnet18_layer1_0_relu_1, resnet18_layer1_1_conv1, resnet18_layer1_1_bn1, resnet18_layer1_1_relu, resnet18_layer1_1_conv2, resnet18_layer1_1_bn2, Deps: []\n",
            "Stage heavy_2-stage-2: Node: CPU-1, Layers: add_1, resnet18_layer1_1_relu_1, resnet18_layer2_0_conv1, resnet18_layer2_0_bn1, resnet18_layer2_0_relu, resnet18_layer2_0_conv2, resnet18_layer2_0_bn2, resnet18_layer2_0_downsample_0, resnet18_layer2_0_downsample_1, add_2, resnet18_layer2_0_relu_1, resnet18_layer2_1_conv1, resnet18_layer2_1_bn1, resnet18_layer2_1_relu, resnet18_layer2_1_conv2, resnet18_layer2_1_bn2, add_3, resnet18_layer2_1_relu_1, resnet18_layer3_0_conv1, resnet18_layer3_0_bn1, resnet18_layer3_0_relu, resnet18_layer3_0_conv2, resnet18_layer3_0_bn2, resnet18_layer3_0_downsample_0, resnet18_layer3_0_downsample_1, add_4, resnet18_layer3_0_relu_1, resnet18_layer3_1_conv1, resnet18_layer3_1_bn1, resnet18_layer3_1_relu, resnet18_layer3_1_conv2, resnet18_layer3_1_bn2, add_5, resnet18_layer3_1_relu_1, resnet18_layer4_0_conv1, resnet18_layer4_0_bn1, resnet18_layer4_0_relu, resnet18_layer4_0_conv2, resnet18_layer4_0_bn2, resnet18_layer4_0_downsample_0, resnet18_layer4_0_downsample_1, add_6, resnet18_layer4_0_relu_1, resnet18_layer4_1_conv1, resnet18_layer4_1_bn1, resnet18_layer4_1_relu, resnet18_layer4_1_conv2, resnet18_layer4_1_bn2, add_7, resnet18_layer4_1_relu_1, resnet18_avgpool, flatten, resnet18_fc, output, Deps: ['heavy_2-stage-1']\n",
            "=== Stage Allocations for Task heavy_3 ===\n",
            "Stage heavy_3-stage-1: Node: CPU-0, Layers: x, resnet18_conv1, resnet18_bn1, resnet18_relu, resnet18_maxpool, resnet18_layer1_0_conv1, resnet18_layer1_0_bn1, resnet18_layer1_0_relu, resnet18_layer1_0_conv2, resnet18_layer1_0_bn2, add, resnet18_layer1_0_relu_1, resnet18_layer1_1_conv1, resnet18_layer1_1_bn1, resnet18_layer1_1_relu, resnet18_layer1_1_conv2, resnet18_layer1_1_bn2, add_1, resnet18_layer1_1_relu_1, Deps: []\n",
            "Stage heavy_3-stage-2: Node: CPU-1, Layers: resnet18_layer2_0_conv1, resnet18_layer2_0_bn1, resnet18_layer2_0_relu, resnet18_layer2_0_conv2, resnet18_layer2_0_bn2, resnet18_layer2_0_downsample_0, resnet18_layer2_0_downsample_1, add_2, resnet18_layer2_0_relu_1, resnet18_layer2_1_conv1, resnet18_layer2_1_bn1, resnet18_layer2_1_relu, resnet18_layer2_1_conv2, resnet18_layer2_1_bn2, add_3, resnet18_layer2_1_relu_1, resnet18_layer3_0_conv1, resnet18_layer3_0_bn1, resnet18_layer3_0_relu, resnet18_layer3_0_conv2, resnet18_layer3_0_bn2, resnet18_layer3_0_downsample_0, resnet18_layer3_0_downsample_1, add_4, resnet18_layer3_0_relu_1, resnet18_layer3_1_conv1, resnet18_layer3_1_bn1, resnet18_layer3_1_relu, resnet18_layer3_1_conv2, resnet18_layer3_1_bn2, add_5, resnet18_layer3_1_relu_1, resnet18_layer4_0_conv1, resnet18_layer4_0_bn1, resnet18_layer4_0_relu, resnet18_layer4_0_conv2, resnet18_layer4_0_bn2, resnet18_layer4_0_downsample_0, resnet18_layer4_0_downsample_1, add_6, resnet18_layer4_0_relu_1, resnet18_layer4_1_conv1, resnet18_layer4_1_bn1, resnet18_layer4_1_relu, resnet18_layer4_1_conv2, resnet18_layer4_1_bn2, add_7, resnet18_layer4_1_relu_1, resnet18_avgpool, flatten, resnet18_fc, output, Deps: ['heavy_3-stage-1']\n",
            "=== Stage Allocations for Task heavy_4 ===\n",
            "Stage heavy_4-stage-1: Node: CPU-0, Layers: x, resnet18_conv1, resnet18_bn1, resnet18_relu, resnet18_maxpool, resnet18_layer1_0_conv1, resnet18_layer1_0_bn1, resnet18_layer1_0_relu, resnet18_layer1_0_conv2, resnet18_layer1_0_bn2, add, resnet18_layer1_0_relu_1, resnet18_layer1_1_conv1, resnet18_layer1_1_bn1, resnet18_layer1_1_relu, resnet18_layer1_1_conv2, resnet18_layer1_1_bn2, add_1, resnet18_layer1_1_relu_1, Deps: []\n",
            "Stage heavy_4-stage-2: Node: CPU-1, Layers: resnet18_layer2_0_conv1, resnet18_layer2_0_bn1, resnet18_layer2_0_relu, resnet18_layer2_0_conv2, resnet18_layer2_0_bn2, resnet18_layer2_0_downsample_0, resnet18_layer2_0_downsample_1, add_2, resnet18_layer2_0_relu_1, resnet18_layer2_1_conv1, resnet18_layer2_1_bn1, resnet18_layer2_1_relu, resnet18_layer2_1_conv2, resnet18_layer2_1_bn2, add_3, resnet18_layer2_1_relu_1, resnet18_layer3_0_conv1, resnet18_layer3_0_bn1, resnet18_layer3_0_relu, resnet18_layer3_0_conv2, resnet18_layer3_0_bn2, resnet18_layer3_0_downsample_0, resnet18_layer3_0_downsample_1, add_4, resnet18_layer3_0_relu_1, resnet18_layer3_1_conv1, resnet18_layer3_1_bn1, resnet18_layer3_1_relu, resnet18_layer3_1_conv2, resnet18_layer3_1_bn2, add_5, resnet18_layer3_1_relu_1, resnet18_layer4_0_conv1, resnet18_layer4_0_bn1, resnet18_layer4_0_relu, resnet18_layer4_0_conv2, resnet18_layer4_0_bn2, resnet18_layer4_0_downsample_0, resnet18_layer4_0_downsample_1, add_6, resnet18_layer4_0_relu_1, resnet18_layer4_1_conv1, resnet18_layer4_1_bn1, resnet18_layer4_1_relu, resnet18_layer4_1_conv2, resnet18_layer4_1_bn2, add_7, resnet18_layer4_1_relu_1, resnet18_avgpool, flatten, resnet18_fc, output, Deps: ['heavy_4-stage-1']\n",
            "=== Stage Allocations for Task heavy_5 ===\n",
            "Stage heavy_5-stage-1: Node: CPU-0, Layers: x, resnet18_conv1, resnet18_bn1, resnet18_relu, resnet18_maxpool, resnet18_layer1_0_conv1, resnet18_layer1_0_bn1, resnet18_layer1_0_relu, resnet18_layer1_0_conv2, resnet18_layer1_0_bn2, add, resnet18_layer1_0_relu_1, resnet18_layer1_1_conv1, resnet18_layer1_1_bn1, resnet18_layer1_1_relu, resnet18_layer1_1_conv2, resnet18_layer1_1_bn2, add_1, resnet18_layer1_1_relu_1, Deps: []\n",
            "Stage heavy_5-stage-2: Node: CPU-1, Layers: resnet18_layer2_0_conv1, resnet18_layer2_0_bn1, resnet18_layer2_0_relu, resnet18_layer2_0_conv2, resnet18_layer2_0_bn2, resnet18_layer2_0_downsample_0, resnet18_layer2_0_downsample_1, add_2, resnet18_layer2_0_relu_1, resnet18_layer2_1_conv1, resnet18_layer2_1_bn1, resnet18_layer2_1_relu, resnet18_layer2_1_conv2, resnet18_layer2_1_bn2, add_3, resnet18_layer2_1_relu_1, resnet18_layer3_0_conv1, resnet18_layer3_0_bn1, resnet18_layer3_0_relu, resnet18_layer3_0_conv2, resnet18_layer3_0_bn2, resnet18_layer3_0_downsample_0, resnet18_layer3_0_downsample_1, add_4, resnet18_layer3_0_relu_1, resnet18_layer3_1_conv1, resnet18_layer3_1_bn1, resnet18_layer3_1_relu, resnet18_layer3_1_conv2, resnet18_layer3_1_bn2, add_5, resnet18_layer3_1_relu_1, resnet18_layer4_0_conv1, resnet18_layer4_0_bn1, resnet18_layer4_0_relu, resnet18_layer4_0_conv2, resnet18_layer4_0_bn2, resnet18_layer4_0_downsample_0, resnet18_layer4_0_downsample_1, add_6, resnet18_layer4_0_relu_1, resnet18_layer4_1_conv1, resnet18_layer4_1_bn1, resnet18_layer4_1_relu, resnet18_layer4_1_conv2, resnet18_layer4_1_bn2, add_7, resnet18_layer4_1_relu_1, resnet18_avgpool, flatten, resnet18_fc, output, Deps: ['heavy_5-stage-1']\n",
            "=== Stage Allocations for Task heavy_6 ===\n",
            "Stage heavy_6-stage-1: Node: CPU-0, Layers: x, resnet18_conv1, resnet18_bn1, resnet18_relu, resnet18_maxpool, resnet18_layer1_0_conv1, resnet18_layer1_0_bn1, resnet18_layer1_0_relu, resnet18_layer1_0_conv2, resnet18_layer1_0_bn2, add, resnet18_layer1_0_relu_1, resnet18_layer1_1_conv1, resnet18_layer1_1_bn1, resnet18_layer1_1_relu, resnet18_layer1_1_conv2, resnet18_layer1_1_bn2, add_1, resnet18_layer1_1_relu_1, resnet18_layer2_0_conv1, Deps: []\n",
            "Stage heavy_6-stage-2: Node: CPU-1, Layers: resnet18_layer2_0_bn1, resnet18_layer2_0_relu, resnet18_layer2_0_conv2, resnet18_layer2_0_bn2, resnet18_layer2_0_downsample_0, resnet18_layer2_0_downsample_1, add_2, resnet18_layer2_0_relu_1, resnet18_layer2_1_conv1, resnet18_layer2_1_bn1, resnet18_layer2_1_relu, resnet18_layer2_1_conv2, resnet18_layer2_1_bn2, add_3, resnet18_layer2_1_relu_1, resnet18_layer3_0_conv1, resnet18_layer3_0_bn1, resnet18_layer3_0_relu, resnet18_layer3_0_conv2, resnet18_layer3_0_bn2, resnet18_layer3_0_downsample_0, resnet18_layer3_0_downsample_1, add_4, resnet18_layer3_0_relu_1, resnet18_layer3_1_conv1, resnet18_layer3_1_bn1, resnet18_layer3_1_relu, resnet18_layer3_1_conv2, resnet18_layer3_1_bn2, add_5, resnet18_layer3_1_relu_1, resnet18_layer4_0_conv1, resnet18_layer4_0_bn1, resnet18_layer4_0_relu, resnet18_layer4_0_conv2, resnet18_layer4_0_bn2, resnet18_layer4_0_downsample_0, resnet18_layer4_0_downsample_1, add_6, resnet18_layer4_0_relu_1, resnet18_layer4_1_conv1, resnet18_layer4_1_bn1, resnet18_layer4_1_relu, resnet18_layer4_1_conv2, resnet18_layer4_1_bn2, add_7, resnet18_layer4_1_relu_1, resnet18_avgpool, flatten, resnet18_fc, output, Deps: ['heavy_6-stage-1']\n",
            "=== Stage Allocations for Task heavy_7 ===\n",
            "Stage heavy_7-stage-1: Node: CPU-0, Layers: x, resnet18_conv1, resnet18_bn1, resnet18_relu, resnet18_maxpool, resnet18_layer1_0_conv1, resnet18_layer1_0_bn1, resnet18_layer1_0_relu, resnet18_layer1_0_conv2, resnet18_layer1_0_bn2, add, resnet18_layer1_0_relu_1, resnet18_layer1_1_conv1, resnet18_layer1_1_bn1, resnet18_layer1_1_relu, resnet18_layer1_1_conv2, resnet18_layer1_1_bn2, add_1, resnet18_layer1_1_relu_1, Deps: []\n",
            "Stage heavy_7-stage-2: Node: CPU-1, Layers: resnet18_layer2_0_conv1, resnet18_layer2_0_bn1, resnet18_layer2_0_relu, resnet18_layer2_0_conv2, resnet18_layer2_0_bn2, resnet18_layer2_0_downsample_0, resnet18_layer2_0_downsample_1, add_2, resnet18_layer2_0_relu_1, resnet18_layer2_1_conv1, resnet18_layer2_1_bn1, resnet18_layer2_1_relu, resnet18_layer2_1_conv2, resnet18_layer2_1_bn2, add_3, resnet18_layer2_1_relu_1, resnet18_layer3_0_conv1, resnet18_layer3_0_bn1, resnet18_layer3_0_relu, resnet18_layer3_0_conv2, resnet18_layer3_0_bn2, resnet18_layer3_0_downsample_0, resnet18_layer3_0_downsample_1, add_4, resnet18_layer3_0_relu_1, resnet18_layer3_1_conv1, resnet18_layer3_1_bn1, resnet18_layer3_1_relu, resnet18_layer3_1_conv2, resnet18_layer3_1_bn2, add_5, resnet18_layer3_1_relu_1, resnet18_layer4_0_conv1, resnet18_layer4_0_bn1, resnet18_layer4_0_relu, resnet18_layer4_0_conv2, resnet18_layer4_0_bn2, resnet18_layer4_0_downsample_0, resnet18_layer4_0_downsample_1, add_6, resnet18_layer4_0_relu_1, resnet18_layer4_1_conv1, resnet18_layer4_1_bn1, resnet18_layer4_1_relu, resnet18_layer4_1_conv2, resnet18_layer4_1_bn2, add_7, resnet18_layer4_1_relu_1, resnet18_avgpool, flatten, resnet18_fc, output, Deps: ['heavy_7-stage-1']\n",
            "=== Stage Allocations for Task heavy_8 ===\n",
            "Stage heavy_8-stage-1: Node: CPU-0, Layers: x, resnet18_conv1, resnet18_bn1, resnet18_relu, resnet18_maxpool, resnet18_layer1_0_conv1, resnet18_layer1_0_bn1, resnet18_layer1_0_relu, resnet18_layer1_0_conv2, resnet18_layer1_0_bn2, add, resnet18_layer1_0_relu_1, resnet18_layer1_1_conv1, resnet18_layer1_1_bn1, resnet18_layer1_1_relu, resnet18_layer1_1_conv2, resnet18_layer1_1_bn2, add_1, resnet18_layer1_1_relu_1, resnet18_layer2_0_conv1, resnet18_layer2_0_bn1, resnet18_layer2_0_relu, resnet18_layer2_0_conv2, resnet18_layer2_0_bn2, resnet18_layer2_0_downsample_0, Deps: []\n",
            "Stage heavy_8-stage-2: Node: CPU-1, Layers: resnet18_layer2_0_downsample_1, add_2, resnet18_layer2_0_relu_1, resnet18_layer2_1_conv1, resnet18_layer2_1_bn1, resnet18_layer2_1_relu, resnet18_layer2_1_conv2, resnet18_layer2_1_bn2, add_3, resnet18_layer2_1_relu_1, resnet18_layer3_0_conv1, resnet18_layer3_0_bn1, resnet18_layer3_0_relu, resnet18_layer3_0_conv2, resnet18_layer3_0_bn2, resnet18_layer3_0_downsample_0, resnet18_layer3_0_downsample_1, add_4, resnet18_layer3_0_relu_1, resnet18_layer3_1_conv1, resnet18_layer3_1_bn1, resnet18_layer3_1_relu, resnet18_layer3_1_conv2, resnet18_layer3_1_bn2, add_5, resnet18_layer3_1_relu_1, resnet18_layer4_0_conv1, resnet18_layer4_0_bn1, resnet18_layer4_0_relu, resnet18_layer4_0_conv2, resnet18_layer4_0_bn2, resnet18_layer4_0_downsample_0, resnet18_layer4_0_downsample_1, add_6, resnet18_layer4_0_relu_1, resnet18_layer4_1_conv1, resnet18_layer4_1_bn1, resnet18_layer4_1_relu, resnet18_layer4_1_conv2, resnet18_layer4_1_bn2, add_7, resnet18_layer4_1_relu_1, resnet18_avgpool, flatten, resnet18_fc, output, Deps: ['heavy_8-stage-1']\n",
            "=== Stage Allocations for Task heavy_9 ===\n",
            "Stage heavy_9-stage-1: Node: CPU-0, Layers: x, resnet18_conv1, resnet18_bn1, resnet18_relu, resnet18_maxpool, resnet18_layer1_0_conv1, resnet18_layer1_0_bn1, resnet18_layer1_0_relu, resnet18_layer1_0_conv2, resnet18_layer1_0_bn2, add, resnet18_layer1_0_relu_1, resnet18_layer1_1_conv1, resnet18_layer1_1_bn1, resnet18_layer1_1_relu, resnet18_layer1_1_conv2, resnet18_layer1_1_bn2, add_1, resnet18_layer1_1_relu_1, resnet18_layer2_0_conv1, resnet18_layer2_0_bn1, resnet18_layer2_0_relu, resnet18_layer2_0_conv2, Deps: []\n",
            "Stage heavy_9-stage-2: Node: CPU-1, Layers: resnet18_layer2_0_bn2, resnet18_layer2_0_downsample_0, resnet18_layer2_0_downsample_1, add_2, resnet18_layer2_0_relu_1, resnet18_layer2_1_conv1, resnet18_layer2_1_bn1, resnet18_layer2_1_relu, resnet18_layer2_1_conv2, resnet18_layer2_1_bn2, add_3, resnet18_layer2_1_relu_1, resnet18_layer3_0_conv1, resnet18_layer3_0_bn1, resnet18_layer3_0_relu, resnet18_layer3_0_conv2, resnet18_layer3_0_bn2, resnet18_layer3_0_downsample_0, resnet18_layer3_0_downsample_1, add_4, resnet18_layer3_0_relu_1, resnet18_layer3_1_conv1, resnet18_layer3_1_bn1, resnet18_layer3_1_relu, resnet18_layer3_1_conv2, resnet18_layer3_1_bn2, add_5, resnet18_layer3_1_relu_1, resnet18_layer4_0_conv1, resnet18_layer4_0_bn1, resnet18_layer4_0_relu, resnet18_layer4_0_conv2, resnet18_layer4_0_bn2, resnet18_layer4_0_downsample_0, resnet18_layer4_0_downsample_1, add_6, resnet18_layer4_0_relu_1, resnet18_layer4_1_conv1, resnet18_layer4_1_bn1, resnet18_layer4_1_relu, resnet18_layer4_1_conv2, resnet18_layer4_1_bn2, add_7, resnet18_layer4_1_relu_1, resnet18_avgpool, flatten, resnet18_fc, output, Deps: ['heavy_9-stage-1']\n",
            "=== Stage Allocations for Task heavy_10 ===\n",
            "Stage heavy_10-stage-1: Node: CPU-0, Layers: x, resnet18_conv1, resnet18_bn1, resnet18_relu, resnet18_maxpool, resnet18_layer1_0_conv1, resnet18_layer1_0_bn1, resnet18_layer1_0_relu, resnet18_layer1_0_conv2, resnet18_layer1_0_bn2, add, resnet18_layer1_0_relu_1, resnet18_layer1_1_conv1, resnet18_layer1_1_bn1, resnet18_layer1_1_relu, resnet18_layer1_1_conv2, resnet18_layer1_1_bn2, add_1, resnet18_layer1_1_relu_1, resnet18_layer2_0_conv1, resnet18_layer2_0_bn1, resnet18_layer2_0_relu, resnet18_layer2_0_conv2, Deps: []\n",
            "Stage heavy_10-stage-2: Node: CPU-1, Layers: resnet18_layer2_0_bn2, resnet18_layer2_0_downsample_0, resnet18_layer2_0_downsample_1, add_2, resnet18_layer2_0_relu_1, resnet18_layer2_1_conv1, resnet18_layer2_1_bn1, resnet18_layer2_1_relu, resnet18_layer2_1_conv2, resnet18_layer2_1_bn2, add_3, resnet18_layer2_1_relu_1, resnet18_layer3_0_conv1, resnet18_layer3_0_bn1, resnet18_layer3_0_relu, resnet18_layer3_0_conv2, resnet18_layer3_0_bn2, resnet18_layer3_0_downsample_0, resnet18_layer3_0_downsample_1, add_4, resnet18_layer3_0_relu_1, resnet18_layer3_1_conv1, resnet18_layer3_1_bn1, resnet18_layer3_1_relu, resnet18_layer3_1_conv2, resnet18_layer3_1_bn2, add_5, resnet18_layer3_1_relu_1, resnet18_layer4_0_conv1, resnet18_layer4_0_bn1, resnet18_layer4_0_relu, resnet18_layer4_0_conv2, resnet18_layer4_0_bn2, resnet18_layer4_0_downsample_0, resnet18_layer4_0_downsample_1, add_6, resnet18_layer4_0_relu_1, resnet18_layer4_1_conv1, resnet18_layer4_1_bn1, resnet18_layer4_1_relu, resnet18_layer4_1_conv2, resnet18_layer4_1_bn2, add_7, resnet18_layer4_1_relu_1, resnet18_avgpool, flatten, resnet18_fc, output, Deps: ['heavy_10-stage-1']\n",
            "=== Stage Allocations for Task heavy_11 ===\n",
            "Stage heavy_11-stage-1: Node: CPU-0, Layers: x, resnet18_conv1, resnet18_bn1, resnet18_relu, resnet18_maxpool, resnet18_layer1_0_conv1, resnet18_layer1_0_bn1, resnet18_layer1_0_relu, resnet18_layer1_0_conv2, resnet18_layer1_0_bn2, add, resnet18_layer1_0_relu_1, resnet18_layer1_1_conv1, resnet18_layer1_1_bn1, resnet18_layer1_1_relu, resnet18_layer1_1_conv2, resnet18_layer1_1_bn2, add_1, resnet18_layer1_1_relu_1, resnet18_layer2_0_conv1, resnet18_layer2_0_bn1, resnet18_layer2_0_relu, resnet18_layer2_0_conv2, Deps: []\n",
            "Stage heavy_11-stage-2: Node: CPU-1, Layers: resnet18_layer2_0_bn2, resnet18_layer2_0_downsample_0, resnet18_layer2_0_downsample_1, add_2, resnet18_layer2_0_relu_1, resnet18_layer2_1_conv1, resnet18_layer2_1_bn1, resnet18_layer2_1_relu, resnet18_layer2_1_conv2, resnet18_layer2_1_bn2, add_3, resnet18_layer2_1_relu_1, resnet18_layer3_0_conv1, resnet18_layer3_0_bn1, resnet18_layer3_0_relu, resnet18_layer3_0_conv2, resnet18_layer3_0_bn2, resnet18_layer3_0_downsample_0, resnet18_layer3_0_downsample_1, add_4, resnet18_layer3_0_relu_1, resnet18_layer3_1_conv1, resnet18_layer3_1_bn1, resnet18_layer3_1_relu, resnet18_layer3_1_conv2, resnet18_layer3_1_bn2, add_5, resnet18_layer3_1_relu_1, resnet18_layer4_0_conv1, resnet18_layer4_0_bn1, resnet18_layer4_0_relu, resnet18_layer4_0_conv2, resnet18_layer4_0_bn2, resnet18_layer4_0_downsample_0, resnet18_layer4_0_downsample_1, add_6, resnet18_layer4_0_relu_1, resnet18_layer4_1_conv1, resnet18_layer4_1_bn1, resnet18_layer4_1_relu, resnet18_layer4_1_conv2, resnet18_layer4_1_bn2, add_7, resnet18_layer4_1_relu_1, resnet18_avgpool, flatten, resnet18_fc, output, Deps: ['heavy_11-stage-1']\n",
            "=== Stage Allocations for Task heavy_12 ===\n",
            "Stage heavy_12-stage-1: Node: CPU-0, Layers: x, resnet18_conv1, resnet18_bn1, resnet18_relu, resnet18_maxpool, resnet18_layer1_0_conv1, resnet18_layer1_0_bn1, resnet18_layer1_0_relu, resnet18_layer1_0_conv2, resnet18_layer1_0_bn2, add, resnet18_layer1_0_relu_1, resnet18_layer1_1_conv1, resnet18_layer1_1_bn1, resnet18_layer1_1_relu, resnet18_layer1_1_conv2, resnet18_layer1_1_bn2, add_1, resnet18_layer1_1_relu_1, resnet18_layer2_0_conv1, resnet18_layer2_0_bn1, resnet18_layer2_0_relu, resnet18_layer2_0_conv2, Deps: []\n",
            "Stage heavy_12-stage-2: Node: CPU-1, Layers: resnet18_layer2_0_bn2, resnet18_layer2_0_downsample_0, resnet18_layer2_0_downsample_1, add_2, resnet18_layer2_0_relu_1, resnet18_layer2_1_conv1, resnet18_layer2_1_bn1, resnet18_layer2_1_relu, resnet18_layer2_1_conv2, resnet18_layer2_1_bn2, add_3, resnet18_layer2_1_relu_1, resnet18_layer3_0_conv1, resnet18_layer3_0_bn1, resnet18_layer3_0_relu, resnet18_layer3_0_conv2, resnet18_layer3_0_bn2, resnet18_layer3_0_downsample_0, resnet18_layer3_0_downsample_1, add_4, resnet18_layer3_0_relu_1, resnet18_layer3_1_conv1, resnet18_layer3_1_bn1, resnet18_layer3_1_relu, resnet18_layer3_1_conv2, resnet18_layer3_1_bn2, add_5, resnet18_layer3_1_relu_1, resnet18_layer4_0_conv1, resnet18_layer4_0_bn1, resnet18_layer4_0_relu, resnet18_layer4_0_conv2, resnet18_layer4_0_bn2, resnet18_layer4_0_downsample_0, resnet18_layer4_0_downsample_1, add_6, resnet18_layer4_0_relu_1, resnet18_layer4_1_conv1, resnet18_layer4_1_bn1, resnet18_layer4_1_relu, resnet18_layer4_1_conv2, resnet18_layer4_1_bn2, add_7, resnet18_layer4_1_relu_1, resnet18_avgpool, flatten, resnet18_fc, output, Deps: ['heavy_12-stage-1']\n",
            "=== Stage Allocations for Task light_1 ===\n",
            "Stage light_1-stage-1: Node: CPU-1, Layers: x, conv1, relu, conv2, relu_1, cat, flatten, fc, output, Deps: []\n",
            "=== Stage Allocations for Task light_2 ===\n",
            "Stage light_2-stage-1: Node: CPU-1, Layers: x, conv1, relu, conv2, relu_1, cat, flatten, fc, output, Deps: []\n",
            "=== Stage Allocations for Task light_3 ===\n",
            "Stage light_3-stage-1: Node: CPU-1, Layers: x, conv1, relu, conv2, relu_1, cat, flatten, fc, output, Deps: []\n",
            "[Evaluator] Starting Naive Execution.\n",
            "[Evaluator] Task heavy_1: Naive exec time: 0.897329s\n",
            "[Evaluator] Task heavy_2: Naive exec time: 1.246468s\n",
            "[Evaluator] Task heavy_3: Naive exec time: 1.306978s\n",
            "[Evaluator] Task heavy_4: Naive exec time: 1.232948s\n",
            "[Evaluator] Task heavy_5: Naive exec time: 0.866974s\n",
            "[Evaluator] Task heavy_6: Naive exec time: 0.882630s\n",
            "[Evaluator] Task heavy_7: Naive exec time: 0.863889s\n",
            "[Evaluator] Task heavy_8: Naive exec time: 0.862587s\n",
            "[Evaluator] Task heavy_9: Naive exec time: 0.862521s\n",
            "[Evaluator] Task heavy_10: Naive exec time: 0.856504s\n",
            "[Evaluator] Task heavy_11: Naive exec time: 0.882996s\n",
            "[Evaluator] Task heavy_12: Naive exec time: 0.887506s\n",
            "[Evaluator] Task light_1: Naive exec time: 0.003473s\n",
            "[Evaluator] Task light_2: Naive exec time: 0.002948s\n",
            "[Evaluator] Task light_3: Naive exec time: 0.003053s\n",
            "[Evaluator] Naive makespan: 11.687113s\n",
            "\n",
            "[Evaluator] Starting Parallel Execution.\n",
            "[Taskset] Starting Task heavy_1\n",
            "[Taskset] Starting Task heavy_2\n",
            "[Taskset] Starting Task heavy_3\n",
            "[Taskset] Starting Task heavy_4\n",
            "[Taskset] Starting Task heavy_5\n",
            "[Taskset] Starting Task heavy_6\n",
            "[Taskset] Starting Task heavy_7\n",
            "[Taskset] Starting Task heavy_8\n",
            "[Taskset] Starting Task heavy_9\n",
            "[Taskset] Starting Task heavy_10\n",
            "[Taskset] Starting Task heavy_11\n",
            "[Taskset] Starting Task heavy_12\n",
            "[Taskset] Starting Task light_1\n",
            "[Taskset] Starting Task light_2\n",
            "[Stage] light_1-stage-1: Executed on CPU-1 in 0.004839 s, Transfer: 0.000068 s.\n",
            "[Stage] light_2-stage-1: Executed on CPU-1 in 0.003889 s, Transfer: 0.000063 s.[Taskset] Starting Task light_3\n",
            "\n",
            "[Taskset] Completed Task light_1.\n",
            "[Taskset] Completed Task light_2.\n",
            "[Stage] light_3-stage-1: Executed on CPU-1 in 0.004950 s, Transfer: 0.000067 s.\n",
            "[Taskset] Completed Task light_3.\n",
            "[Stage] heavy_1-stage-1: Executed on CPU-0 in 0.629761 s, Transfer: 0.000420 s.\n",
            "[Stage] heavy_2-stage-1: Executed on CPU-0 in 0.805138 s, Transfer: 0.000312 s.\n",
            "[Stage] heavy_1-stage-2: Executed on CPU-1 in 1.113647 s, Transfer: 0.000721 s.\n",
            "[Taskset] Completed Task heavy_1.\n",
            "[Stage] heavy_3-stage-1: Executed on CPU-0 in 0.913194 s, Transfer: 0.000302 s.\n",
            "[Stage] heavy_2-stage-2: Executed on CPU-1 in 1.235591 s, Transfer: 0.000807 s.\n",
            "[Taskset] Completed Task heavy_2.\n",
            "[Stage] heavy_4-stage-1: Executed on CPU-0 in 0.950012 s, Transfer: 0.000376 s.\n",
            "[Stage] heavy_5-stage-1: Executed on CPU-0 in 1.177168 s, Transfer: 0.000365 s.\n",
            "[Stage] heavy_3-stage-2: Executed on CPU-1 in 1.568649 s, Transfer: 0.000858 s.\n",
            "[Taskset] Completed Task heavy_3.\n",
            "[Stage] heavy_6-stage-1: Executed on CPU-0 in 1.327397 s, Transfer: 0.000396 s.\n",
            "[Stage] heavy_4-stage-2: Executed on CPU-1 in 1.579339 s, Transfer: 0.001893 s.\n",
            "[Taskset] Completed Task heavy_4.\n",
            "[Stage] heavy_7-stage-1: Executed on CPU-0 in 0.967530 s, Transfer: 0.000356 s.\n",
            "[Stage] heavy_5-stage-2: Executed on CPU-1 in 1.186779 s, Transfer: 0.000808 s.\n",
            "[Taskset] Completed Task heavy_5.\n",
            "[Stage] heavy_8-stage-1: Executed on CPU-0 in 1.174402 s, Transfer: 0.000517 s.\n",
            "[Stage] heavy_6-stage-2: Executed on CPU-1 in 1.168269 s, Transfer: 0.000855 s.\n",
            "[Taskset] Completed Task heavy_6.\n",
            "[Stage] heavy_9-stage-1: Executed on CPU-0 in 0.821978 s, Transfer: 0.000386 s.\n",
            "[Stage] heavy_7-stage-2: Executed on CPU-1 in 0.857885 s, Transfer: 0.000766 s.\n",
            "[Taskset] Completed Task heavy_7.\n",
            "[Stage] heavy_10-stage-1: Executed on CPU-0 in 0.747389 s, Transfer: 0.000416 s.\n",
            "[Stage] heavy_8-stage-2: Executed on CPU-1 in 0.700496 s, Transfer: 0.000687 s.\n",
            "[Taskset] Completed Task heavy_8.\n",
            "[Stage] heavy_11-stage-1: Executed on CPU-0 in 0.769691 s, Transfer: 0.000401 s.\n",
            "[Stage] heavy_9-stage-2: Executed on CPU-1 in 0.714647 s, Transfer: 0.000674 s.\n",
            "[Taskset] Completed Task heavy_9.\n",
            "[Stage] heavy_12-stage-1: Executed on CPU-0 in 0.741565 s, Transfer: 0.000430 s.\n",
            "[Stage] heavy_10-stage-2: Executed on CPU-1 in 0.560291 s, Transfer: 0.000608 s.\n",
            "[Taskset] Completed Task heavy_10.\n",
            "[Stage] heavy_11-stage-2: Executed on CPU-1 in 0.431979 s, Transfer: 0.000491 s.\n",
            "[Taskset] Completed Task heavy_11.\n",
            "[Stage] heavy_12-stage-2: Executed on CPU-1 in 0.453638 s, Transfer: 0.000572 s.\n",
            "[Taskset] Completed Task heavy_12.\n",
            "[Evaluator] Parallel makespan: 12.222958s\n",
            "\n",
            "[Evaluator] Comparing Outputs.\n",
            "[Evaluator] Task heavy_1: Outputs match.\n",
            "[Evaluator] Task heavy_2: Outputs match.\n",
            "[Evaluator] Task heavy_3: Outputs match.\n",
            "[Evaluator] Task heavy_4: Outputs match.\n",
            "[Evaluator] Task heavy_5: Outputs match.\n",
            "[Evaluator] Task heavy_6: Outputs match.\n",
            "[Evaluator] Task heavy_7: Outputs match.\n",
            "[Evaluator] Task heavy_8: Outputs match.\n",
            "[Evaluator] Task heavy_9: Outputs match.\n",
            "[Evaluator] Task heavy_10: Outputs match.\n",
            "[Evaluator] Task heavy_11: Outputs match.\n",
            "[Evaluator] Task heavy_12: Outputs match.\n",
            "[Evaluator] Task light_1: Outputs match.\n",
            "[Evaluator] Task light_2: Outputs match.\n",
            "[Evaluator] Task light_3: Outputs match.\n",
            "[Evaluator] All outputs match.\n",
            "\n",
            "[Evaluator] Analyzing Speedup and Throughput.\n",
            "\n",
            "--- Sum-of-times ---\n",
            "Naive total: 11.658803s, Parallel total: 96.720412s\n",
            "Speedup (sum-of-times): 0.12x\n",
            "Naive Throughput: 1.29 tasks/s, Parallel Throughput: 0.16 tasks/s\n",
            "\n",
            "--- Makespan ---\n",
            "Naive makespan: 11.687113s, Parallel makespan: 12.222958s\n",
            "Speedup (makespan): 0.96x\n",
            "Naive Throughput (makespan): 1.28 tasks/s, Parallel Throughput (makespan): 1.23 tasks/s\n",
            "\n",
            "--- Task Completion Times ---\n",
            "Task heavy_1: Naive finish: 0.898904s, Parallel finish: 1.752761s\n",
            "Task heavy_2: Naive finish: 2.147247s, Parallel finish: 2.991897s\n",
            "Task heavy_3: Naive finish: 3.456450s, Parallel finish: 4.560411s\n",
            "Task heavy_4: Naive finish: 4.691785s, Parallel finish: 6.140902s\n",
            "Task heavy_5: Naive finish: 5.560981s, Parallel finish: 7.328792s\n",
            "Task heavy_6: Naive finish: 6.445668s, Parallel finish: 8.498365s\n",
            "Task heavy_7: Naive finish: 7.311424s, Parallel finish: 9.357488s\n",
            "Task heavy_8: Naive finish: 8.176016s, Parallel finish: 10.059198s\n",
            "Task heavy_9: Naive finish: 9.040369s, Parallel finish: 10.775423s\n",
            "Task heavy_10: Naive finish: 9.898743s, Parallel finish: 11.335924s\n",
            "Task heavy_11: Naive finish: 10.783610s, Parallel finish: 11.768088s\n",
            "Task heavy_12: Naive finish: 11.673137s, Parallel finish: 12.222610s\n",
            "Task light_1: Naive finish: 11.678263s, Parallel finish: 0.026215s\n",
            "Task light_2: Naive finish: 11.682250s, Parallel finish: 0.027591s\n",
            "Task light_3: Naive finish: 11.686327s, Parallel finish: 0.033300s\n",
            "\n",
            "Speedup for ratio 0.8: 0.96\n",
            "Parallel Throughput for ratio 0.8: 1.23\n",
            "Naive Throughput for ratio 0.8: 1.28\n",
            "Makespan for ratio 0.8: 12.22\n",
            "Running experiment with 13 heavy and 2 light tasks.\n",
            "Seed set to 42\n",
            "Using 2 nodes: [Node(CPU-0, cpus=(0,), gpu=None), Node(CPU-1, cpus=(1,), gpu=None)]\n",
            "Creating 13 heavy tasks and 2 light tasks.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
            "  warnings.warn(msg)\n",
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
            "  warnings.warn(msg)\n",
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
            "  warnings.warn(msg)\n",
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
            "  warnings.warn(msg)\n",
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
            "  warnings.warn(msg)\n",
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
            "  warnings.warn(msg)\n",
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
            "  warnings.warn(msg)\n",
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
            "  warnings.warn(msg)\n",
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
            "  warnings.warn(msg)\n",
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
            "  warnings.warn(msg)\n",
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
            "  warnings.warn(msg)\n",
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
            "  warnings.warn(msg)\n",
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Profiler] Starting profiling for Task 'heavy_1' on CPU-0 (device=cpu).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/autograd/profiler.py:263: UserWarning: CUDA is not available, disabling CUDA profiling\n",
            "  warn(\"CUDA is not available, disabling CUDA profiling\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Profiler] Profiling complete. Data saved to profiling_results.csv.\n",
            "[Profiler] Starting profiling for Task 'heavy_1' on CPU-1 (device=cpu).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/autograd/profiler.py:263: UserWarning: CUDA is not available, disabling CUDA profiling\n",
            "  warn(\"CUDA is not available, disabling CUDA profiling\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Profiler] Profiling complete. Data saved to profiling_results.csv.\n",
            "[Profiler] Reused cached profiling data for PretrainedResNet18 on CPU-0.\n",
            "[Profiler] Reused cached profiling data for PretrainedResNet18 on CPU-1.\n",
            "[Profiler] Reused cached profiling data for PretrainedResNet18 on CPU-0.\n",
            "[Profiler] Reused cached profiling data for PretrainedResNet18 on CPU-1.\n",
            "[Profiler] Reused cached profiling data for PretrainedResNet18 on CPU-0.\n",
            "[Profiler] Reused cached profiling data for PretrainedResNet18 on CPU-1.\n",
            "[Profiler] Reused cached profiling data for PretrainedResNet18 on CPU-0.\n",
            "[Profiler] Reused cached profiling data for PretrainedResNet18 on CPU-1.\n",
            "[Profiler] Reused cached profiling data for PretrainedResNet18 on CPU-0.\n",
            "[Profiler] Reused cached profiling data for PretrainedResNet18 on CPU-1.\n",
            "[Profiler] Reused cached profiling data for PretrainedResNet18 on CPU-0.\n",
            "[Profiler] Reused cached profiling data for PretrainedResNet18 on CPU-1.\n",
            "[Profiler] Reused cached profiling data for PretrainedResNet18 on CPU-0.\n",
            "[Profiler] Reused cached profiling data for PretrainedResNet18 on CPU-1.\n",
            "[Profiler] Reused cached profiling data for PretrainedResNet18 on CPU-0.\n",
            "[Profiler] Reused cached profiling data for PretrainedResNet18 on CPU-1.\n",
            "[Profiler] Reused cached profiling data for PretrainedResNet18 on CPU-0.\n",
            "[Profiler] Reused cached profiling data for PretrainedResNet18 on CPU-1.\n",
            "[Profiler] Reused cached profiling data for PretrainedResNet18 on CPU-0.\n",
            "[Profiler] Reused cached profiling data for PretrainedResNet18 on CPU-1.\n",
            "[Profiler] Reused cached profiling data for PretrainedResNet18 on CPU-0.\n",
            "[Profiler] Reused cached profiling data for PretrainedResNet18 on CPU-1.\n",
            "[Profiler] Reused cached profiling data for PretrainedResNet18 on CPU-0.\n",
            "[Profiler] Reused cached profiling data for PretrainedResNet18 on CPU-1.\n",
            "[Profiler] Starting profiling for Task 'light_1' on CPU-0 (device=cpu).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/autograd/profiler.py:263: UserWarning: CUDA is not available, disabling CUDA profiling\n",
            "  warn(\"CUDA is not available, disabling CUDA profiling\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Profiler] Profiling complete. Data saved to profiling_results.csv.\n",
            "[Profiler] Starting profiling for Task 'light_1' on CPU-1 (device=cpu).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/autograd/profiler.py:263: UserWarning: CUDA is not available, disabling CUDA profiling\n",
            "  warn(\"CUDA is not available, disabling CUDA profiling\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Profiler] Profiling complete. Data saved to profiling_results.csv.\n",
            "[Profiler] Reused cached profiling data for SimpleCNN on CPU-0.\n",
            "[Profiler] Reused cached profiling data for SimpleCNN on CPU-1.\n",
            "Measured max transfer penalty: 0.000152 s\n",
            "=== Stage Allocations for Task heavy_1 ===\n",
            "Stage heavy_1-stage-1: Node: CPU-0, Layers: x, resnet18_conv1, resnet18_bn1, resnet18_relu, resnet18_maxpool, resnet18_layer1_0_conv1, resnet18_layer1_0_bn1, resnet18_layer1_0_relu, resnet18_layer1_0_conv2, resnet18_layer1_0_bn2, add, resnet18_layer1_0_relu_1, resnet18_layer1_1_conv1, resnet18_layer1_1_bn1, resnet18_layer1_1_relu, resnet18_layer1_1_conv2, resnet18_layer1_1_bn2, add_1, resnet18_layer1_1_relu_1, resnet18_layer2_0_conv1, resnet18_layer2_0_bn1, resnet18_layer2_0_relu, resnet18_layer2_0_conv2, Deps: []\n",
            "Stage heavy_1-stage-2: Node: CPU-1, Layers: resnet18_layer2_0_bn2, resnet18_layer2_0_downsample_0, resnet18_layer2_0_downsample_1, add_2, resnet18_layer2_0_relu_1, resnet18_layer2_1_conv1, resnet18_layer2_1_bn1, resnet18_layer2_1_relu, resnet18_layer2_1_conv2, resnet18_layer2_1_bn2, add_3, resnet18_layer2_1_relu_1, resnet18_layer3_0_conv1, resnet18_layer3_0_bn1, resnet18_layer3_0_relu, resnet18_layer3_0_conv2, resnet18_layer3_0_bn2, resnet18_layer3_0_downsample_0, resnet18_layer3_0_downsample_1, add_4, resnet18_layer3_0_relu_1, resnet18_layer3_1_conv1, resnet18_layer3_1_bn1, resnet18_layer3_1_relu, resnet18_layer3_1_conv2, resnet18_layer3_1_bn2, add_5, resnet18_layer3_1_relu_1, resnet18_layer4_0_conv1, resnet18_layer4_0_bn1, resnet18_layer4_0_relu, resnet18_layer4_0_conv2, resnet18_layer4_0_bn2, resnet18_layer4_0_downsample_0, resnet18_layer4_0_downsample_1, add_6, resnet18_layer4_0_relu_1, resnet18_layer4_1_conv1, resnet18_layer4_1_bn1, resnet18_layer4_1_relu, resnet18_layer4_1_conv2, resnet18_layer4_1_bn2, add_7, resnet18_layer4_1_relu_1, resnet18_avgpool, flatten, resnet18_fc, output, Deps: ['heavy_1-stage-1']\n",
            "=== Stage Allocations for Task heavy_2 ===\n",
            "Stage heavy_2-stage-1: Node: CPU-0, Layers: x, resnet18_conv1, resnet18_bn1, resnet18_relu, resnet18_maxpool, resnet18_layer1_0_conv1, resnet18_layer1_0_bn1, resnet18_layer1_0_relu, resnet18_layer1_0_conv2, resnet18_layer1_0_bn2, add, resnet18_layer1_0_relu_1, resnet18_layer1_1_conv1, resnet18_layer1_1_bn1, resnet18_layer1_1_relu, resnet18_layer1_1_conv2, resnet18_layer1_1_bn2, add_1, Deps: []\n",
            "Stage heavy_2-stage-2: Node: CPU-1, Layers: resnet18_layer1_1_relu_1, resnet18_layer2_0_conv1, resnet18_layer2_0_bn1, resnet18_layer2_0_relu, resnet18_layer2_0_conv2, resnet18_layer2_0_bn2, resnet18_layer2_0_downsample_0, resnet18_layer2_0_downsample_1, add_2, resnet18_layer2_0_relu_1, resnet18_layer2_1_conv1, resnet18_layer2_1_bn1, resnet18_layer2_1_relu, resnet18_layer2_1_conv2, resnet18_layer2_1_bn2, add_3, resnet18_layer2_1_relu_1, resnet18_layer3_0_conv1, resnet18_layer3_0_bn1, resnet18_layer3_0_relu, resnet18_layer3_0_conv2, resnet18_layer3_0_bn2, resnet18_layer3_0_downsample_0, resnet18_layer3_0_downsample_1, add_4, resnet18_layer3_0_relu_1, resnet18_layer3_1_conv1, resnet18_layer3_1_bn1, resnet18_layer3_1_relu, resnet18_layer3_1_conv2, resnet18_layer3_1_bn2, add_5, resnet18_layer3_1_relu_1, resnet18_layer4_0_conv1, resnet18_layer4_0_bn1, resnet18_layer4_0_relu, resnet18_layer4_0_conv2, resnet18_layer4_0_bn2, resnet18_layer4_0_downsample_0, resnet18_layer4_0_downsample_1, add_6, resnet18_layer4_0_relu_1, resnet18_layer4_1_conv1, resnet18_layer4_1_bn1, resnet18_layer4_1_relu, resnet18_layer4_1_conv2, resnet18_layer4_1_bn2, add_7, resnet18_layer4_1_relu_1, resnet18_avgpool, flatten, resnet18_fc, output, Deps: ['heavy_2-stage-1']\n",
            "=== Stage Allocations for Task heavy_3 ===\n",
            "Stage heavy_3-stage-1: Node: CPU-0, Layers: x, resnet18_conv1, resnet18_bn1, resnet18_relu, resnet18_maxpool, resnet18_layer1_0_conv1, resnet18_layer1_0_bn1, resnet18_layer1_0_relu, resnet18_layer1_0_conv2, resnet18_layer1_0_bn2, add, resnet18_layer1_0_relu_1, resnet18_layer1_1_conv1, resnet18_layer1_1_bn1, resnet18_layer1_1_relu, resnet18_layer1_1_conv2, resnet18_layer1_1_bn2, add_1, resnet18_layer1_1_relu_1, Deps: []\n",
            "Stage heavy_3-stage-2: Node: CPU-1, Layers: resnet18_layer2_0_conv1, resnet18_layer2_0_bn1, resnet18_layer2_0_relu, resnet18_layer2_0_conv2, resnet18_layer2_0_bn2, resnet18_layer2_0_downsample_0, resnet18_layer2_0_downsample_1, add_2, resnet18_layer2_0_relu_1, resnet18_layer2_1_conv1, resnet18_layer2_1_bn1, resnet18_layer2_1_relu, resnet18_layer2_1_conv2, resnet18_layer2_1_bn2, add_3, resnet18_layer2_1_relu_1, resnet18_layer3_0_conv1, resnet18_layer3_0_bn1, resnet18_layer3_0_relu, resnet18_layer3_0_conv2, resnet18_layer3_0_bn2, resnet18_layer3_0_downsample_0, resnet18_layer3_0_downsample_1, add_4, resnet18_layer3_0_relu_1, resnet18_layer3_1_conv1, resnet18_layer3_1_bn1, resnet18_layer3_1_relu, resnet18_layer3_1_conv2, resnet18_layer3_1_bn2, add_5, resnet18_layer3_1_relu_1, resnet18_layer4_0_conv1, resnet18_layer4_0_bn1, resnet18_layer4_0_relu, resnet18_layer4_0_conv2, resnet18_layer4_0_bn2, resnet18_layer4_0_downsample_0, resnet18_layer4_0_downsample_1, add_6, resnet18_layer4_0_relu_1, resnet18_layer4_1_conv1, resnet18_layer4_1_bn1, resnet18_layer4_1_relu, resnet18_layer4_1_conv2, resnet18_layer4_1_bn2, add_7, resnet18_layer4_1_relu_1, resnet18_avgpool, flatten, resnet18_fc, output, Deps: ['heavy_3-stage-1']\n",
            "=== Stage Allocations for Task heavy_4 ===\n",
            "Stage heavy_4-stage-1: Node: CPU-0, Layers: x, resnet18_conv1, resnet18_bn1, resnet18_relu, resnet18_maxpool, resnet18_layer1_0_conv1, resnet18_layer1_0_bn1, resnet18_layer1_0_relu, resnet18_layer1_0_conv2, resnet18_layer1_0_bn2, add, resnet18_layer1_0_relu_1, resnet18_layer1_1_conv1, resnet18_layer1_1_bn1, resnet18_layer1_1_relu, resnet18_layer1_1_conv2, resnet18_layer1_1_bn2, add_1, resnet18_layer1_1_relu_1, Deps: []\n",
            "Stage heavy_4-stage-2: Node: CPU-1, Layers: resnet18_layer2_0_conv1, resnet18_layer2_0_bn1, resnet18_layer2_0_relu, resnet18_layer2_0_conv2, resnet18_layer2_0_bn2, resnet18_layer2_0_downsample_0, resnet18_layer2_0_downsample_1, add_2, resnet18_layer2_0_relu_1, resnet18_layer2_1_conv1, resnet18_layer2_1_bn1, resnet18_layer2_1_relu, resnet18_layer2_1_conv2, resnet18_layer2_1_bn2, add_3, resnet18_layer2_1_relu_1, resnet18_layer3_0_conv1, resnet18_layer3_0_bn1, resnet18_layer3_0_relu, resnet18_layer3_0_conv2, resnet18_layer3_0_bn2, resnet18_layer3_0_downsample_0, resnet18_layer3_0_downsample_1, add_4, resnet18_layer3_0_relu_1, resnet18_layer3_1_conv1, resnet18_layer3_1_bn1, resnet18_layer3_1_relu, resnet18_layer3_1_conv2, resnet18_layer3_1_bn2, add_5, resnet18_layer3_1_relu_1, resnet18_layer4_0_conv1, resnet18_layer4_0_bn1, resnet18_layer4_0_relu, resnet18_layer4_0_conv2, resnet18_layer4_0_bn2, resnet18_layer4_0_downsample_0, resnet18_layer4_0_downsample_1, add_6, resnet18_layer4_0_relu_1, resnet18_layer4_1_conv1, resnet18_layer4_1_bn1, resnet18_layer4_1_relu, resnet18_layer4_1_conv2, resnet18_layer4_1_bn2, add_7, resnet18_layer4_1_relu_1, resnet18_avgpool, flatten, resnet18_fc, output, Deps: ['heavy_4-stage-1']\n",
            "=== Stage Allocations for Task heavy_5 ===\n",
            "Stage heavy_5-stage-1: Node: CPU-0, Layers: x, resnet18_conv1, resnet18_bn1, resnet18_relu, resnet18_maxpool, resnet18_layer1_0_conv1, resnet18_layer1_0_bn1, resnet18_layer1_0_relu, resnet18_layer1_0_conv2, resnet18_layer1_0_bn2, add, resnet18_layer1_0_relu_1, resnet18_layer1_1_conv1, resnet18_layer1_1_bn1, resnet18_layer1_1_relu, resnet18_layer1_1_conv2, resnet18_layer1_1_bn2, add_1, resnet18_layer1_1_relu_1, Deps: []\n",
            "Stage heavy_5-stage-2: Node: CPU-1, Layers: resnet18_layer2_0_conv1, resnet18_layer2_0_bn1, resnet18_layer2_0_relu, resnet18_layer2_0_conv2, resnet18_layer2_0_bn2, resnet18_layer2_0_downsample_0, resnet18_layer2_0_downsample_1, add_2, resnet18_layer2_0_relu_1, resnet18_layer2_1_conv1, resnet18_layer2_1_bn1, resnet18_layer2_1_relu, resnet18_layer2_1_conv2, resnet18_layer2_1_bn2, add_3, resnet18_layer2_1_relu_1, resnet18_layer3_0_conv1, resnet18_layer3_0_bn1, resnet18_layer3_0_relu, resnet18_layer3_0_conv2, resnet18_layer3_0_bn2, resnet18_layer3_0_downsample_0, resnet18_layer3_0_downsample_1, add_4, resnet18_layer3_0_relu_1, resnet18_layer3_1_conv1, resnet18_layer3_1_bn1, resnet18_layer3_1_relu, resnet18_layer3_1_conv2, resnet18_layer3_1_bn2, add_5, resnet18_layer3_1_relu_1, resnet18_layer4_0_conv1, resnet18_layer4_0_bn1, resnet18_layer4_0_relu, resnet18_layer4_0_conv2, resnet18_layer4_0_bn2, resnet18_layer4_0_downsample_0, resnet18_layer4_0_downsample_1, add_6, resnet18_layer4_0_relu_1, resnet18_layer4_1_conv1, resnet18_layer4_1_bn1, resnet18_layer4_1_relu, resnet18_layer4_1_conv2, resnet18_layer4_1_bn2, add_7, resnet18_layer4_1_relu_1, resnet18_avgpool, flatten, resnet18_fc, output, Deps: ['heavy_5-stage-1']\n",
            "=== Stage Allocations for Task heavy_6 ===\n",
            "Stage heavy_6-stage-1: Node: CPU-0, Layers: x, resnet18_conv1, resnet18_bn1, resnet18_relu, resnet18_maxpool, resnet18_layer1_0_conv1, resnet18_layer1_0_bn1, resnet18_layer1_0_relu, resnet18_layer1_0_conv2, resnet18_layer1_0_bn2, add, resnet18_layer1_0_relu_1, resnet18_layer1_1_conv1, resnet18_layer1_1_bn1, resnet18_layer1_1_relu, resnet18_layer1_1_conv2, resnet18_layer1_1_bn2, add_1, resnet18_layer1_1_relu_1, resnet18_layer2_0_conv1, Deps: []\n",
            "Stage heavy_6-stage-2: Node: CPU-1, Layers: resnet18_layer2_0_bn1, resnet18_layer2_0_relu, resnet18_layer2_0_conv2, resnet18_layer2_0_bn2, resnet18_layer2_0_downsample_0, resnet18_layer2_0_downsample_1, add_2, resnet18_layer2_0_relu_1, resnet18_layer2_1_conv1, resnet18_layer2_1_bn1, resnet18_layer2_1_relu, resnet18_layer2_1_conv2, resnet18_layer2_1_bn2, add_3, resnet18_layer2_1_relu_1, resnet18_layer3_0_conv1, resnet18_layer3_0_bn1, resnet18_layer3_0_relu, resnet18_layer3_0_conv2, resnet18_layer3_0_bn2, resnet18_layer3_0_downsample_0, resnet18_layer3_0_downsample_1, add_4, resnet18_layer3_0_relu_1, resnet18_layer3_1_conv1, resnet18_layer3_1_bn1, resnet18_layer3_1_relu, resnet18_layer3_1_conv2, resnet18_layer3_1_bn2, add_5, resnet18_layer3_1_relu_1, resnet18_layer4_0_conv1, resnet18_layer4_0_bn1, resnet18_layer4_0_relu, resnet18_layer4_0_conv2, resnet18_layer4_0_bn2, resnet18_layer4_0_downsample_0, resnet18_layer4_0_downsample_1, add_6, resnet18_layer4_0_relu_1, resnet18_layer4_1_conv1, resnet18_layer4_1_bn1, resnet18_layer4_1_relu, resnet18_layer4_1_conv2, resnet18_layer4_1_bn2, add_7, resnet18_layer4_1_relu_1, resnet18_avgpool, flatten, resnet18_fc, output, Deps: ['heavy_6-stage-1']\n",
            "=== Stage Allocations for Task heavy_7 ===\n",
            "Stage heavy_7-stage-1: Node: CPU-0, Layers: x, resnet18_conv1, resnet18_bn1, resnet18_relu, resnet18_maxpool, resnet18_layer1_0_conv1, resnet18_layer1_0_bn1, resnet18_layer1_0_relu, resnet18_layer1_0_conv2, resnet18_layer1_0_bn2, add, resnet18_layer1_0_relu_1, resnet18_layer1_1_conv1, resnet18_layer1_1_bn1, resnet18_layer1_1_relu, resnet18_layer1_1_conv2, resnet18_layer1_1_bn2, add_1, resnet18_layer1_1_relu_1, Deps: []\n",
            "Stage heavy_7-stage-2: Node: CPU-1, Layers: resnet18_layer2_0_conv1, resnet18_layer2_0_bn1, resnet18_layer2_0_relu, resnet18_layer2_0_conv2, resnet18_layer2_0_bn2, resnet18_layer2_0_downsample_0, resnet18_layer2_0_downsample_1, add_2, resnet18_layer2_0_relu_1, resnet18_layer2_1_conv1, resnet18_layer2_1_bn1, resnet18_layer2_1_relu, resnet18_layer2_1_conv2, resnet18_layer2_1_bn2, add_3, resnet18_layer2_1_relu_1, resnet18_layer3_0_conv1, resnet18_layer3_0_bn1, resnet18_layer3_0_relu, resnet18_layer3_0_conv2, resnet18_layer3_0_bn2, resnet18_layer3_0_downsample_0, resnet18_layer3_0_downsample_1, add_4, resnet18_layer3_0_relu_1, resnet18_layer3_1_conv1, resnet18_layer3_1_bn1, resnet18_layer3_1_relu, resnet18_layer3_1_conv2, resnet18_layer3_1_bn2, add_5, resnet18_layer3_1_relu_1, resnet18_layer4_0_conv1, resnet18_layer4_0_bn1, resnet18_layer4_0_relu, resnet18_layer4_0_conv2, resnet18_layer4_0_bn2, resnet18_layer4_0_downsample_0, resnet18_layer4_0_downsample_1, add_6, resnet18_layer4_0_relu_1, resnet18_layer4_1_conv1, resnet18_layer4_1_bn1, resnet18_layer4_1_relu, resnet18_layer4_1_conv2, resnet18_layer4_1_bn2, add_7, resnet18_layer4_1_relu_1, resnet18_avgpool, flatten, resnet18_fc, output, Deps: ['heavy_7-stage-1']\n",
            "=== Stage Allocations for Task heavy_8 ===\n",
            "Stage heavy_8-stage-1: Node: CPU-0, Layers: x, resnet18_conv1, resnet18_bn1, resnet18_relu, resnet18_maxpool, resnet18_layer1_0_conv1, resnet18_layer1_0_bn1, resnet18_layer1_0_relu, resnet18_layer1_0_conv2, resnet18_layer1_0_bn2, add, resnet18_layer1_0_relu_1, resnet18_layer1_1_conv1, resnet18_layer1_1_bn1, resnet18_layer1_1_relu, resnet18_layer1_1_conv2, resnet18_layer1_1_bn2, add_1, resnet18_layer1_1_relu_1, resnet18_layer2_0_conv1, resnet18_layer2_0_bn1, resnet18_layer2_0_relu, resnet18_layer2_0_conv2, resnet18_layer2_0_bn2, resnet18_layer2_0_downsample_0, Deps: []\n",
            "Stage heavy_8-stage-2: Node: CPU-1, Layers: resnet18_layer2_0_downsample_1, add_2, resnet18_layer2_0_relu_1, resnet18_layer2_1_conv1, resnet18_layer2_1_bn1, resnet18_layer2_1_relu, resnet18_layer2_1_conv2, resnet18_layer2_1_bn2, add_3, resnet18_layer2_1_relu_1, resnet18_layer3_0_conv1, resnet18_layer3_0_bn1, resnet18_layer3_0_relu, resnet18_layer3_0_conv2, resnet18_layer3_0_bn2, resnet18_layer3_0_downsample_0, resnet18_layer3_0_downsample_1, add_4, resnet18_layer3_0_relu_1, resnet18_layer3_1_conv1, resnet18_layer3_1_bn1, resnet18_layer3_1_relu, resnet18_layer3_1_conv2, resnet18_layer3_1_bn2, add_5, resnet18_layer3_1_relu_1, resnet18_layer4_0_conv1, resnet18_layer4_0_bn1, resnet18_layer4_0_relu, resnet18_layer4_0_conv2, resnet18_layer4_0_bn2, resnet18_layer4_0_downsample_0, resnet18_layer4_0_downsample_1, add_6, resnet18_layer4_0_relu_1, resnet18_layer4_1_conv1, resnet18_layer4_1_bn1, resnet18_layer4_1_relu, resnet18_layer4_1_conv2, resnet18_layer4_1_bn2, add_7, resnet18_layer4_1_relu_1, resnet18_avgpool, flatten, resnet18_fc, output, Deps: ['heavy_8-stage-1']\n",
            "=== Stage Allocations for Task heavy_9 ===\n",
            "Stage heavy_9-stage-1: Node: CPU-0, Layers: x, resnet18_conv1, resnet18_bn1, resnet18_relu, resnet18_maxpool, resnet18_layer1_0_conv1, resnet18_layer1_0_bn1, resnet18_layer1_0_relu, resnet18_layer1_0_conv2, resnet18_layer1_0_bn2, add, resnet18_layer1_0_relu_1, resnet18_layer1_1_conv1, resnet18_layer1_1_bn1, resnet18_layer1_1_relu, resnet18_layer1_1_conv2, resnet18_layer1_1_bn2, add_1, resnet18_layer1_1_relu_1, resnet18_layer2_0_conv1, resnet18_layer2_0_bn1, resnet18_layer2_0_relu, resnet18_layer2_0_conv2, Deps: []\n",
            "Stage heavy_9-stage-2: Node: CPU-1, Layers: resnet18_layer2_0_bn2, resnet18_layer2_0_downsample_0, resnet18_layer2_0_downsample_1, add_2, resnet18_layer2_0_relu_1, resnet18_layer2_1_conv1, resnet18_layer2_1_bn1, resnet18_layer2_1_relu, resnet18_layer2_1_conv2, resnet18_layer2_1_bn2, add_3, resnet18_layer2_1_relu_1, resnet18_layer3_0_conv1, resnet18_layer3_0_bn1, resnet18_layer3_0_relu, resnet18_layer3_0_conv2, resnet18_layer3_0_bn2, resnet18_layer3_0_downsample_0, resnet18_layer3_0_downsample_1, add_4, resnet18_layer3_0_relu_1, resnet18_layer3_1_conv1, resnet18_layer3_1_bn1, resnet18_layer3_1_relu, resnet18_layer3_1_conv2, resnet18_layer3_1_bn2, add_5, resnet18_layer3_1_relu_1, resnet18_layer4_0_conv1, resnet18_layer4_0_bn1, resnet18_layer4_0_relu, resnet18_layer4_0_conv2, resnet18_layer4_0_bn2, resnet18_layer4_0_downsample_0, resnet18_layer4_0_downsample_1, add_6, resnet18_layer4_0_relu_1, resnet18_layer4_1_conv1, resnet18_layer4_1_bn1, resnet18_layer4_1_relu, resnet18_layer4_1_conv2, resnet18_layer4_1_bn2, add_7, resnet18_layer4_1_relu_1, resnet18_avgpool, flatten, resnet18_fc, output, Deps: ['heavy_9-stage-1']\n",
            "=== Stage Allocations for Task heavy_10 ===\n",
            "Stage heavy_10-stage-1: Node: CPU-0, Layers: x, resnet18_conv1, resnet18_bn1, resnet18_relu, resnet18_maxpool, resnet18_layer1_0_conv1, resnet18_layer1_0_bn1, resnet18_layer1_0_relu, resnet18_layer1_0_conv2, resnet18_layer1_0_bn2, add, resnet18_layer1_0_relu_1, resnet18_layer1_1_conv1, resnet18_layer1_1_bn1, resnet18_layer1_1_relu, resnet18_layer1_1_conv2, resnet18_layer1_1_bn2, add_1, resnet18_layer1_1_relu_1, resnet18_layer2_0_conv1, resnet18_layer2_0_bn1, resnet18_layer2_0_relu, resnet18_layer2_0_conv2, Deps: []\n",
            "Stage heavy_10-stage-2: Node: CPU-1, Layers: resnet18_layer2_0_bn2, resnet18_layer2_0_downsample_0, resnet18_layer2_0_downsample_1, add_2, resnet18_layer2_0_relu_1, resnet18_layer2_1_conv1, resnet18_layer2_1_bn1, resnet18_layer2_1_relu, resnet18_layer2_1_conv2, resnet18_layer2_1_bn2, add_3, resnet18_layer2_1_relu_1, resnet18_layer3_0_conv1, resnet18_layer3_0_bn1, resnet18_layer3_0_relu, resnet18_layer3_0_conv2, resnet18_layer3_0_bn2, resnet18_layer3_0_downsample_0, resnet18_layer3_0_downsample_1, add_4, resnet18_layer3_0_relu_1, resnet18_layer3_1_conv1, resnet18_layer3_1_bn1, resnet18_layer3_1_relu, resnet18_layer3_1_conv2, resnet18_layer3_1_bn2, add_5, resnet18_layer3_1_relu_1, resnet18_layer4_0_conv1, resnet18_layer4_0_bn1, resnet18_layer4_0_relu, resnet18_layer4_0_conv2, resnet18_layer4_0_bn2, resnet18_layer4_0_downsample_0, resnet18_layer4_0_downsample_1, add_6, resnet18_layer4_0_relu_1, resnet18_layer4_1_conv1, resnet18_layer4_1_bn1, resnet18_layer4_1_relu, resnet18_layer4_1_conv2, resnet18_layer4_1_bn2, add_7, resnet18_layer4_1_relu_1, resnet18_avgpool, flatten, resnet18_fc, output, Deps: ['heavy_10-stage-1']\n",
            "=== Stage Allocations for Task heavy_11 ===\n",
            "Stage heavy_11-stage-1: Node: CPU-0, Layers: x, resnet18_conv1, resnet18_bn1, resnet18_relu, resnet18_maxpool, resnet18_layer1_0_conv1, resnet18_layer1_0_bn1, resnet18_layer1_0_relu, resnet18_layer1_0_conv2, resnet18_layer1_0_bn2, add, resnet18_layer1_0_relu_1, resnet18_layer1_1_conv1, resnet18_layer1_1_bn1, resnet18_layer1_1_relu, resnet18_layer1_1_conv2, resnet18_layer1_1_bn2, add_1, resnet18_layer1_1_relu_1, resnet18_layer2_0_conv1, resnet18_layer2_0_bn1, resnet18_layer2_0_relu, resnet18_layer2_0_conv2, Deps: []\n",
            "Stage heavy_11-stage-2: Node: CPU-1, Layers: resnet18_layer2_0_bn2, resnet18_layer2_0_downsample_0, resnet18_layer2_0_downsample_1, add_2, resnet18_layer2_0_relu_1, resnet18_layer2_1_conv1, resnet18_layer2_1_bn1, resnet18_layer2_1_relu, resnet18_layer2_1_conv2, resnet18_layer2_1_bn2, add_3, resnet18_layer2_1_relu_1, resnet18_layer3_0_conv1, resnet18_layer3_0_bn1, resnet18_layer3_0_relu, resnet18_layer3_0_conv2, resnet18_layer3_0_bn2, resnet18_layer3_0_downsample_0, resnet18_layer3_0_downsample_1, add_4, resnet18_layer3_0_relu_1, resnet18_layer3_1_conv1, resnet18_layer3_1_bn1, resnet18_layer3_1_relu, resnet18_layer3_1_conv2, resnet18_layer3_1_bn2, add_5, resnet18_layer3_1_relu_1, resnet18_layer4_0_conv1, resnet18_layer4_0_bn1, resnet18_layer4_0_relu, resnet18_layer4_0_conv2, resnet18_layer4_0_bn2, resnet18_layer4_0_downsample_0, resnet18_layer4_0_downsample_1, add_6, resnet18_layer4_0_relu_1, resnet18_layer4_1_conv1, resnet18_layer4_1_bn1, resnet18_layer4_1_relu, resnet18_layer4_1_conv2, resnet18_layer4_1_bn2, add_7, resnet18_layer4_1_relu_1, resnet18_avgpool, flatten, resnet18_fc, output, Deps: ['heavy_11-stage-1']\n",
            "=== Stage Allocations for Task heavy_12 ===\n",
            "Stage heavy_12-stage-1: Node: CPU-0, Layers: x, resnet18_conv1, resnet18_bn1, resnet18_relu, resnet18_maxpool, resnet18_layer1_0_conv1, resnet18_layer1_0_bn1, resnet18_layer1_0_relu, resnet18_layer1_0_conv2, resnet18_layer1_0_bn2, add, resnet18_layer1_0_relu_1, resnet18_layer1_1_conv1, resnet18_layer1_1_bn1, resnet18_layer1_1_relu, resnet18_layer1_1_conv2, resnet18_layer1_1_bn2, add_1, resnet18_layer1_1_relu_1, resnet18_layer2_0_conv1, resnet18_layer2_0_bn1, resnet18_layer2_0_relu, resnet18_layer2_0_conv2, Deps: []\n",
            "Stage heavy_12-stage-2: Node: CPU-1, Layers: resnet18_layer2_0_bn2, resnet18_layer2_0_downsample_0, resnet18_layer2_0_downsample_1, add_2, resnet18_layer2_0_relu_1, resnet18_layer2_1_conv1, resnet18_layer2_1_bn1, resnet18_layer2_1_relu, resnet18_layer2_1_conv2, resnet18_layer2_1_bn2, add_3, resnet18_layer2_1_relu_1, resnet18_layer3_0_conv1, resnet18_layer3_0_bn1, resnet18_layer3_0_relu, resnet18_layer3_0_conv2, resnet18_layer3_0_bn2, resnet18_layer3_0_downsample_0, resnet18_layer3_0_downsample_1, add_4, resnet18_layer3_0_relu_1, resnet18_layer3_1_conv1, resnet18_layer3_1_bn1, resnet18_layer3_1_relu, resnet18_layer3_1_conv2, resnet18_layer3_1_bn2, add_5, resnet18_layer3_1_relu_1, resnet18_layer4_0_conv1, resnet18_layer4_0_bn1, resnet18_layer4_0_relu, resnet18_layer4_0_conv2, resnet18_layer4_0_bn2, resnet18_layer4_0_downsample_0, resnet18_layer4_0_downsample_1, add_6, resnet18_layer4_0_relu_1, resnet18_layer4_1_conv1, resnet18_layer4_1_bn1, resnet18_layer4_1_relu, resnet18_layer4_1_conv2, resnet18_layer4_1_bn2, add_7, resnet18_layer4_1_relu_1, resnet18_avgpool, flatten, resnet18_fc, output, Deps: ['heavy_12-stage-1']\n",
            "=== Stage Allocations for Task heavy_13 ===\n",
            "Stage heavy_13-stage-1: Node: CPU-0, Layers: x, resnet18_conv1, resnet18_bn1, resnet18_relu, resnet18_maxpool, resnet18_layer1_0_conv1, resnet18_layer1_0_bn1, resnet18_layer1_0_relu, resnet18_layer1_0_conv2, resnet18_layer1_0_bn2, add, resnet18_layer1_0_relu_1, resnet18_layer1_1_conv1, resnet18_layer1_1_bn1, resnet18_layer1_1_relu, resnet18_layer1_1_conv2, resnet18_layer1_1_bn2, add_1, resnet18_layer1_1_relu_1, resnet18_layer2_0_conv1, resnet18_layer2_0_bn1, resnet18_layer2_0_relu, resnet18_layer2_0_conv2, Deps: []\n",
            "Stage heavy_13-stage-2: Node: CPU-1, Layers: resnet18_layer2_0_bn2, resnet18_layer2_0_downsample_0, resnet18_layer2_0_downsample_1, add_2, resnet18_layer2_0_relu_1, resnet18_layer2_1_conv1, resnet18_layer2_1_bn1, resnet18_layer2_1_relu, resnet18_layer2_1_conv2, resnet18_layer2_1_bn2, add_3, resnet18_layer2_1_relu_1, resnet18_layer3_0_conv1, resnet18_layer3_0_bn1, resnet18_layer3_0_relu, resnet18_layer3_0_conv2, resnet18_layer3_0_bn2, resnet18_layer3_0_downsample_0, resnet18_layer3_0_downsample_1, add_4, resnet18_layer3_0_relu_1, resnet18_layer3_1_conv1, resnet18_layer3_1_bn1, resnet18_layer3_1_relu, resnet18_layer3_1_conv2, resnet18_layer3_1_bn2, add_5, resnet18_layer3_1_relu_1, resnet18_layer4_0_conv1, resnet18_layer4_0_bn1, resnet18_layer4_0_relu, resnet18_layer4_0_conv2, resnet18_layer4_0_bn2, resnet18_layer4_0_downsample_0, resnet18_layer4_0_downsample_1, add_6, resnet18_layer4_0_relu_1, resnet18_layer4_1_conv1, resnet18_layer4_1_bn1, resnet18_layer4_1_relu, resnet18_layer4_1_conv2, resnet18_layer4_1_bn2, add_7, resnet18_layer4_1_relu_1, resnet18_avgpool, flatten, resnet18_fc, output, Deps: ['heavy_13-stage-1']\n",
            "=== Stage Allocations for Task light_1 ===\n",
            "Stage light_1-stage-1: Node: CPU-1, Layers: x, conv1, relu, conv2, relu_1, cat, flatten, fc, output, Deps: []\n",
            "=== Stage Allocations for Task light_2 ===\n",
            "Stage light_2-stage-1: Node: CPU-1, Layers: x, conv1, relu, conv2, relu_1, cat, flatten, fc, output, Deps: []\n",
            "[Evaluator] Starting Naive Execution.\n",
            "[Evaluator] Task heavy_1: Naive exec time: 0.881185s\n",
            "[Evaluator] Task heavy_2: Naive exec time: 0.868935s\n",
            "[Evaluator] Task heavy_3: Naive exec time: 0.864288s\n",
            "[Evaluator] Task heavy_4: Naive exec time: 0.865925s\n",
            "[Evaluator] Task heavy_5: Naive exec time: 0.868484s\n",
            "[Evaluator] Task heavy_6: Naive exec time: 1.025038s\n",
            "[Evaluator] Task heavy_7: Naive exec time: 1.293615s\n",
            "[Evaluator] Task heavy_8: Naive exec time: 1.340011s\n",
            "[Evaluator] Task heavy_9: Naive exec time: 0.944475s\n",
            "[Evaluator] Task heavy_10: Naive exec time: 0.876368s\n",
            "[Evaluator] Task heavy_11: Naive exec time: 0.866133s\n",
            "[Evaluator] Task heavy_12: Naive exec time: 0.888421s\n",
            "[Evaluator] Task heavy_13: Naive exec time: 0.902451s\n",
            "[Evaluator] Task light_1: Naive exec time: 0.003480s\n",
            "[Evaluator] Task light_2: Naive exec time: 0.002662s\n",
            "[Evaluator] Naive makespan: 12.524919s\n",
            "\n",
            "[Evaluator] Starting Parallel Execution.\n",
            "[Taskset] Starting Task heavy_1\n",
            "[Taskset] Starting Task heavy_2\n",
            "[Taskset] Starting Task heavy_3\n",
            "[Taskset] Starting Task heavy_4\n",
            "[Taskset] Starting Task heavy_5\n",
            "[Taskset] Starting Task heavy_6\n",
            "[Taskset] Starting Task heavy_7\n",
            "[Taskset] Starting Task heavy_8\n",
            "[Taskset] Starting Task heavy_9\n",
            "[Taskset] Starting Task heavy_10\n",
            "[Taskset] Starting Task heavy_11\n",
            "[Taskset] Starting Task heavy_12\n",
            "[Taskset] Starting Task heavy_13\n",
            "[Taskset] Starting Task light_1\n",
            "[Stage] light_1-stage-1: Executed on CPU-1 in 0.005367 s, Transfer: 0.000070 s.\n",
            "[Taskset] Completed Task light_1.\n",
            "[Taskset] Starting Task light_2\n",
            "[Stage] light_2-stage-1: Executed on CPU-1 in 0.004056 s, Transfer: 0.000066 s.\n",
            "[Taskset] Completed Task light_2.\n",
            "[Stage] heavy_1-stage-1: Executed on CPU-0 in 0.483517 s, Transfer: 0.000312 s.\n",
            "[Stage] heavy_2-stage-1: Executed on CPU-0 in 0.696018 s, Transfer: 0.000305 s.\n",
            "[Stage] heavy_1-stage-2: Executed on CPU-1 in 0.714611 s, Transfer: 0.000679 s.\n",
            "[Taskset] Completed Task heavy_1.\n",
            "[Stage] heavy_3-stage-1: Executed on CPU-0 in 0.701572 s, Transfer: 0.000330 s.\n",
            "[Stage] heavy_2-stage-2: Executed on CPU-1 in 0.839796 s, Transfer: 0.000731 s.\n",
            "[Taskset] Completed Task heavy_2.\n",
            "[Stage] heavy_4-stage-1: Executed on CPU-0 in 0.770792 s, Transfer: 0.000353 s.\n",
            "[Stage] heavy_3-stage-2: Executed on CPU-1 in 0.848997 s, Transfer: 0.000755 s.\n",
            "[Taskset] Completed Task heavy_3.\n",
            "[Stage] heavy_5-stage-1: Executed on CPU-0 in 0.610324 s, Transfer: 0.000326 s.\n",
            "[Stage] heavy_4-stage-2: Executed on CPU-1 in 0.845925 s, Transfer: 0.000730 s.\n",
            "[Taskset] Completed Task heavy_4.\n",
            "[Stage] heavy_6-stage-1: Executed on CPU-0 in 0.735231 s, Transfer: 0.000335 s.\n",
            "[Stage] heavy_5-stage-2: Executed on CPU-1 in 0.856264 s, Transfer: 0.000695 s.\n",
            "[Taskset] Completed Task heavy_5.\n",
            "[Stage] heavy_7-stage-1: Executed on CPU-0 in 0.703901 s, Transfer: 0.000323 s.\n",
            "[Stage] heavy_6-stage-2: Executed on CPU-1 in 0.791616 s, Transfer: 0.000770 s.\n",
            "[Taskset] Completed Task heavy_6.\n",
            "[Stage] heavy_8-stage-1: Executed on CPU-0 in 0.733669 s, Transfer: 0.000393 s.\n",
            "[Stage] heavy_7-stage-2: Executed on CPU-1 in 0.998719 s, Transfer: 0.000759 s.\n",
            "[Taskset] Completed Task heavy_7.\n",
            "[Stage] heavy_9-stage-1: Executed on CPU-0 in 0.977177 s, Transfer: 0.000394 s.\n",
            "[Stage] heavy_8-stage-2: Executed on CPU-1 in 0.953521 s, Transfer: 0.000628 s.\n",
            "[Taskset] Completed Task heavy_8.\n",
            "[Stage] heavy_10-stage-1: Executed on CPU-0 in 1.231941 s, Transfer: 0.000383 s.\n",
            "[Stage] heavy_9-stage-2: Executed on CPU-1 in 0.999348 s, Transfer: 0.000682 s.\n",
            "[Taskset] Completed Task heavy_9.\n",
            "[Stage] heavy_11-stage-1: Executed on CPU-0 in 0.981123 s, Transfer: 0.000416 s.\n",
            "[Stage] heavy_10-stage-2: Executed on CPU-1 in 1.194151 s, Transfer: 0.000719 s.\n",
            "[Taskset] Completed Task heavy_10.\n",
            "[Stage] heavy_12-stage-1: Executed on CPU-0 in 1.048241 s, Transfer: 0.000402 s.\n",
            "[Stage] heavy_13-stage-1: Executed on CPU-0 in 1.021497 s, Transfer: 0.000421 s.\n",
            "[Stage] heavy_11-stage-2: Executed on CPU-1 in 1.203359 s, Transfer: 0.000705 s.\n",
            "[Taskset] Completed Task heavy_11.\n",
            "[Stage] heavy_12-stage-2: Executed on CPU-1 in 0.446785 s, Transfer: 0.000556 s.\n",
            "[Taskset] Completed Task heavy_12.\n",
            "[Stage] heavy_13-stage-2: Executed on CPU-1 in 0.434003 s, Transfer: 0.000538 s.\n",
            "[Taskset] Completed Task heavy_13.\n",
            "[Evaluator] Parallel makespan: 11.629208s\n",
            "\n",
            "[Evaluator] Comparing Outputs.\n",
            "[Evaluator] Task heavy_1: Outputs match.\n",
            "[Evaluator] Task heavy_2: Outputs match.\n",
            "[Evaluator] Task heavy_3: Outputs match.\n",
            "[Evaluator] Task heavy_4: Outputs match.\n",
            "[Evaluator] Task heavy_5: Outputs match.\n",
            "[Evaluator] Task heavy_6: Outputs match.\n",
            "[Evaluator] Task heavy_7: Outputs match.\n",
            "[Evaluator] Task heavy_8: Outputs match.\n",
            "[Evaluator] Task heavy_9: Outputs match.\n",
            "[Evaluator] Task heavy_10: Outputs match.\n",
            "[Evaluator] Task heavy_11: Outputs match.\n",
            "[Evaluator] Task heavy_12: Outputs match.\n",
            "[Evaluator] Task heavy_13: Outputs match.\n",
            "[Evaluator] Task light_1: Outputs match.\n",
            "[Evaluator] Task light_2: Outputs match.\n",
            "[Evaluator] All outputs match.\n",
            "\n",
            "[Evaluator] Analyzing Speedup and Throughput.\n",
            "\n",
            "--- Sum-of-times ---\n",
            "Naive total: 12.491469s, Parallel total: 84.968418s\n",
            "Speedup (sum-of-times): 0.15x\n",
            "Naive Throughput: 1.20 tasks/s, Parallel Throughput: 0.18 tasks/s\n",
            "\n",
            "--- Makespan ---\n",
            "Naive makespan: 12.524919s, Parallel makespan: 11.629208s\n",
            "Speedup (makespan): 1.08x\n",
            "Naive Throughput (makespan): 1.20 tasks/s, Parallel Throughput (makespan): 1.29 tasks/s\n",
            "\n",
            "--- Task Completion Times ---\n",
            "Task heavy_1: Naive finish: 0.886020s, Parallel finish: 1.203431s\n",
            "Task heavy_2: Naive finish: 1.756919s, Parallel finish: 2.043165s\n",
            "Task heavy_3: Naive finish: 2.623165s, Parallel finish: 2.893166s\n",
            "Task heavy_4: Naive finish: 3.490981s, Parallel finish: 3.739341s\n",
            "Task heavy_5: Naive finish: 4.361447s, Parallel finish: 4.596551s\n",
            "Task heavy_6: Naive finish: 5.388393s, Parallel finish: 5.389463s\n",
            "Task heavy_7: Naive finish: 6.684473s, Parallel finish: 6.392770s\n",
            "Task heavy_8: Naive finish: 8.028913s, Parallel finish: 7.347394s\n",
            "Task heavy_9: Naive finish: 8.975512s, Parallel finish: 8.347988s\n",
            "Task heavy_10: Naive finish: 9.853806s, Parallel finish: 9.544981s\n",
            "Task heavy_11: Naive finish: 10.721939s, Parallel finish: 10.747555s\n",
            "Task heavy_12: Naive finish: 11.612280s, Parallel finish: 11.193912s\n",
            "Task heavy_13: Naive finish: 12.517166s, Parallel finish: 11.628871s\n",
            "Task light_1: Naive finish: 12.521076s, Parallel finish: 0.021561s\n",
            "Task light_2: Naive finish: 12.524814s, Parallel finish: 0.029399s\n",
            "\n",
            "Speedup for ratio 0.9: 1.08\n",
            "Parallel Throughput for ratio 0.9: 1.29\n",
            "Naive Throughput for ratio 0.9: 1.20\n",
            "Makespan for ratio 0.9: 11.63\n",
            "\n",
            "Speedups: [0.6089016190010897, 0.8452805440712027, 0.7443797589122157, 1.3241518405730022, 1.1324409774008455, 1.306237275480968, 1.0046101746688112, 0.9561607707132102, 1.0770225259608985]\n",
            "Parallel Throughputs: [8.507441071208111, 4.747520183421647, 3.147592161457623, 3.071090369424759, 2.493582591608044, 2.1583096463067846, 1.589830632767125, 1.2271988889495735, 1.2898557237642967]\n",
            "Naive Throughputs: [13.971782642267675, 5.616502375123564, 4.228476290189961, 2.319288676210881, 2.20195369239575, 1.6523105616566303, 1.582534870594201, 1.2834650056121768, 1.1976125778924749]\n",
            "Makespans: [1.76316237449646, 3.1595442295074463, 4.765547513961792, 4.88425874710083, 6.015441417694092, 6.949883222579956, 9.434967279434204, 12.222957611083984, 11.629207611083984]\n",
            "\n",
            "The best configuration is {'speedup': 1.3241518405730022, 'parallel_throughput': 3.071090369424759, 'naive_throughput': 2.319288676210881, 'makespan': 4.88425874710083}\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1200x400 with 3 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import torch\n",
        "import threading\n",
        "import queue\n",
        "import os\n",
        "\n",
        "# Suppose this is your existing Node class (abbreviated here).\n",
        "# from your_code import Node\n",
        "\n",
        "def torch_long_running_task(name: str, size: int = 2000, device: str = 'cpu'):\n",
        "    \"\"\"\n",
        "    A sample function to simulate a time-consuming PyTorch operation.\n",
        "    Performs a large matrix multiplication on the specified device.\n",
        "    Prints start/end times so we can see parallel behavior.\n",
        "\n",
        "    Args:\n",
        "        name (str): A label for logging.\n",
        "        size (int): Dimension for the square matrices to multiply.\n",
        "        device (str): 'cpu' or 'cuda' (if available).\n",
        "    \"\"\"\n",
        "    start = time.time()\n",
        "    print(f\"[{name}] Starting at {start:.2f} on device={device}\")\n",
        "\n",
        "    # Create two random Tensors of shape (size, size) on 'device'\n",
        "    a = torch.randn(size, size, device=device)\n",
        "    b = torch.randn(size, size, device=device)\n",
        "\n",
        "    # Perform a matrix multiplication to simulate a heavy compute load\n",
        "    c = torch.matmul(a, b)\n",
        "\n",
        "    # If running on GPU, synchronize to ensure the operation completes\n",
        "    if device == 'cuda':\n",
        "        torch.cuda.synchronize()\n",
        "\n",
        "    end = time.time()\n",
        "    elapsed = end - start\n",
        "    print(f\"[{name}] Finished at {end:.2f}, Elapsed: {elapsed:.2f} seconds (device={device})\")\n",
        "\n",
        "    return f\"[{name}] done (shape={c.shape}, device={device}, time={elapsed:.2f}s)\"\n",
        "\n",
        "def test_parallel_nodes():\n",
        "    print(\"Discovering nodes...\")\n",
        "    nodes = Node.discover_nodes(disjoint=True)\n",
        "\n",
        "    if len(nodes) < 2:\n",
        "        print(\"Not enough nodes discovered. Exiting.\")\n",
        "        return\n",
        "\n",
        "    node1 = nodes[0]\n",
        "    node2 = nodes[1]\n",
        "    print(f\"Using Node1: {node1}, Node2: {node2}\")\n",
        "\n",
        "    # Decide on device for each node (Node might have GPU set, or just CPU)\n",
        "    device1 = 'cuda' if node1.gpu is not None and torch.cuda.is_available() else 'cpu'\n",
        "    device2 = 'cuda' if node2.gpu is not None and torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "    # Assign a PyTorch matrix-mult task to each node\n",
        "    print(\"\\nAssigning PyTorch tasks...\")\n",
        "    res_queue1 = node1.assign_task(torch_long_running_task, \"TorchTask-A\", 2000, device1)\n",
        "    res_queue2 = node2.assign_task(torch_long_running_task, \"TorchTask-B\", 2000, device2)\n",
        "\n",
        "    print(\"Waiting for tasks to complete...\")\n",
        "    result1 = res_queue1.get()\n",
        "    result2 = res_queue2.get()\n",
        "\n",
        "    node1.stop()\n",
        "    node2.stop()\n",
        "\n",
        "    print(\"\\nResults:\")\n",
        "    print(\" -\", result1)\n",
        "    print(\" -\", result2)\n",
        "\n",
        "\n",
        "def main():\n",
        "    test_parallel_nodes()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "A7GtmG03xhKl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "540e3077-b7c4-438d-d9fc-e9e8ae09a0f5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Discovering nodes...\n",
            "Using Node1: Node(CPU-0, cpus=(0,), gpu=None), Node2: Node(CPU-1, cpus=(1,), gpu=None)\n",
            "\n",
            "Assigning PyTorch tasks...\n",
            "Waiting for tasks to complete...\n",
            "[TorchTask-A] Starting at 1739180018.84 on device=cpu\n",
            "[TorchTask-B] Starting at 1739180018.85 on device=cpu\n",
            "[TorchTask-B] Finished at 1739180019.75, Elapsed: 0.91 seconds (device=cpu)\n",
            "[TorchTask-A] Finished at 1739180019.91, Elapsed: 1.07 seconds (device=cpu)\n",
            "\n",
            "Results:\n",
            " - [TorchTask-A] done (shape=torch.Size([2000, 2000]), device=cpu, time=1.07s)\n",
            " - [TorchTask-B] done (shape=torch.Size([2000, 2000]), device=cpu, time=0.91s)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import time\n",
        "import torch\n",
        "import threading\n",
        "import queue\n",
        "from typing import Callable, List\n",
        "import random\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "########################################\n",
        "# Heavy Torch Function for demonstration\n",
        "########################################\n",
        "def heavy_torch_op(name: str, size: int = 2000, device: str = 'cpu'):\n",
        "    \"\"\"\n",
        "    Performs a large matrix multiplication to simulate a heavy compute workload.\n",
        "    \"\"\"\n",
        "    start = time.time()\n",
        "    print(f\"[{name}] Starting on device={device} at {start:.2f}\")\n",
        "\n",
        "    # Create random Tensors\n",
        "    a = torch.randn(size, size, device=device)\n",
        "    b = torch.randn(size, size, device=device)\n",
        "\n",
        "    # Do a matrix multiply\n",
        "    c = torch.matmul(a, b)\n",
        "\n",
        "    if device.startswith('cuda'):\n",
        "        torch.cuda.synchronize()  # ensure kernel completes before timing\n",
        "\n",
        "    end = time.time()\n",
        "    elapsed = end - start\n",
        "    print(f\"[{name}] Finished at {end:.2f}, Elapsed: {elapsed:.2f}s (device={device})\")\n",
        "\n",
        "    # Return some info\n",
        "    return f\"{name} done (shape={c.shape}, device={device}, time={elapsed:.2f}s)\"\n",
        "\n",
        "\n",
        "########################################\n",
        "# Compare Naive vs. Parallel Node Execution\n",
        "########################################\n",
        "def run_naive(tasks):\n",
        "    \"\"\"\n",
        "    Runs tasks sequentially in the main thread (naive approach).\n",
        "    tasks: list of (function, args, kwargs)\n",
        "    \"\"\"\n",
        "    print(\"\\n[Naive Execution] Starting...\")\n",
        "    start = time.time()\n",
        "    results = []\n",
        "    for i, (func, args, kwargs) in enumerate(tasks):\n",
        "        r = func(*args, **kwargs)  # run inline, blocking\n",
        "        results.append(r)\n",
        "    end = time.time()\n",
        "    elapsed = end - start\n",
        "    print(f\"[Naive Execution] Done. Total elapsed: {elapsed:.2f}s\\n\")\n",
        "    return results, elapsed\n",
        "\n",
        "\n",
        "def run_parallel_nodes(tasks, nodes):\n",
        "    \"\"\"\n",
        "    Runs tasks in parallel using multiple Node objects.\n",
        "    tasks: list of (function, args, kwargs)\n",
        "    nodes: list of Node objects\n",
        "    \"\"\"\n",
        "    print(\"\\n[Parallel Execution] Starting...\")\n",
        "    start = time.time()\n",
        "\n",
        "    # We'll assign each task to one node (if # tasks <= # nodes)\n",
        "    # or reuse nodes if we have more tasks than nodes.\n",
        "    result_queues = []\n",
        "    for i, (func, args, kwargs) in enumerate(tasks):\n",
        "        node = nodes[i % len(nodes)]  # pick a node in round-robin\n",
        "        rq = node.assign_task(func, *args, **kwargs)\n",
        "        result_queues.append(rq)\n",
        "\n",
        "    # Collect results\n",
        "    results = []\n",
        "    for rq in result_queues:\n",
        "        r = rq.get()  # block until the task is done\n",
        "        results.append(r)\n",
        "\n",
        "    end = time.time()\n",
        "    elapsed = end - start\n",
        "    print(f\"[Parallel Execution] Done. Total elapsed: {elapsed:.2f}s\\n\")\n",
        "    return results, elapsed\n",
        "\n",
        "\n",
        "def main():\n",
        "    # 1) Create a set of tasks (function + args)\n",
        "    # For demonstration, let's generate 4 tasks that do a matrix multiply.\n",
        "    # We'll randomize the matrix size a bit so they differ slightly.\n",
        "    tasks = []\n",
        "    for i in range(4):\n",
        "        size = random.randint(1800, 2200)  # a bit of variety\n",
        "        name = f\"Task-{i+1}\"\n",
        "        tasks.append((heavy_torch_op, (name, size), {'device': 'cuda'}))  # We'll keep device='cpu' for the naive test\n",
        "\n",
        "    # 2) Run them naively (sequentially)\n",
        "    naive_results, naive_time = run_naive(tasks)\n",
        "\n",
        "    # 3) Discover nodes for parallel approach\n",
        "    # If you have a GPU, it will appear here. Otherwise, you'll see multiple CPU nodes.\n",
        "    nodes = Node.discover_nodes(disjoint=True)\n",
        "    if not nodes:\n",
        "        print(\"No nodes discovered! Exiting parallel test.\")\n",
        "        return\n",
        "\n",
        "    # For parallel test, let's do the same tasks but allow them to run on their assigned nodes\n",
        "    # We can choose device based on the node's GPU, or keep CPU.\n",
        "    # Let's create the tasks again but pick device by node's GPU availability.\n",
        "    # We'll create a new tasks list for the parallel approach:\n",
        "    parallel_tasks = []\n",
        "    for i in range(4):\n",
        "        size = random.randint(1800, 2200)\n",
        "        name = f\"Par-Task-{i+1}\"\n",
        "        # We'll decide device at runtime, so we'll store 'device'='cpu' or 'cuda' in the node assign call:\n",
        "        # Actually, let's just define the function+args now, but we won't set device here.\n",
        "        # We'll specify device in a moment by partial application, or a closure.\n",
        "        # For simplicity, let's just do CPU again, or we can do a small trick:\n",
        "        parallel_tasks.append((heavy_torch_op, (name, size), {}))  # device to be set in the code.\n",
        "\n",
        "    # We'll pick a node for each task. If it has a GPU, we set device='cuda'.\n",
        "    # We'll do that in run_parallel_nodes to keep it consistent.\n",
        "\n",
        "    # Actually let's do a custom approach:\n",
        "    # We'll modify the tasks list so that each item has the correct device\n",
        "    final_parallel_tasks = []\n",
        "    for i, tinfo in enumerate(parallel_tasks):\n",
        "        func, args, kwargs = tinfo\n",
        "        node = nodes[i % len(nodes)]  # node to be used\n",
        "        device = 'cuda' if (node.gpu is not None and torch.cuda.is_available()) else 'cpu'\n",
        "        # Make a new args tuple\n",
        "        name, size = args\n",
        "        new_args = (name, size)\n",
        "        new_kwargs = {'device': device}\n",
        "        final_parallel_tasks.append((func, new_args, new_kwargs))\n",
        "\n",
        "    # 4) Run them in parallel on the discovered nodes\n",
        "    parallel_results, parallel_time = run_parallel_nodes(final_parallel_tasks, nodes)\n",
        "\n",
        "    # 5) Compare times\n",
        "    print(f\"Naive total time:    {naive_time:.2f}s\")\n",
        "    print(f\"Parallel total time: {parallel_time:.2f}s\")\n",
        "\n",
        "    # 6) Shut down nodes\n",
        "    for n in nodes:\n",
        "        n.stop()\n",
        "\n",
        "    # Print final results\n",
        "    print(\"\\nNaive results:\")\n",
        "    for r in naive_results:\n",
        "        print(\"  \", r)\n",
        "\n",
        "    print(\"\\nParallel results:\")\n",
        "    for r in parallel_results:\n",
        "        print(\"  \", r)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DnmgOpHaKV-X",
        "outputId": "01132ab4-a73e-4926-e358-8531039f538d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "[Naive Execution] Starting...\n",
            "[Task-1] Starting on device=cuda at 1739180524.36\n",
            "[Task-1] Finished at 1739180524.38, Elapsed: 0.02s (device=cuda)\n",
            "[Task-2] Starting on device=cuda at 1739180524.38\n",
            "[Task-2] Finished at 1739180524.39, Elapsed: 0.00s (device=cuda)\n",
            "[Task-3] Starting on device=cuda at 1739180524.39\n",
            "[Task-3] Finished at 1739180524.39, Elapsed: 0.01s (device=cuda)\n",
            "[Task-4] Starting on device=cuda at 1739180524.39\n",
            "[Task-4] Finished at 1739180524.40, Elapsed: 0.01s (device=cuda)\n",
            "[Naive Execution] Done. Total elapsed: 0.05s\n",
            "\n",
            "\n",
            "[Parallel Execution] Starting...\n",
            "[Par-Task-2] Starting on device=cpu at 1739180524.42\n",
            "[Par-Task-1] Starting on device=cuda at 1739180524.43\n",
            "[Par-Task-1] Finished at 1739180524.44, Elapsed: 0.01s (device=cuda)\n",
            "[Par-Task-3] Starting on device=cuda at 1739180524.44\n",
            "[Par-Task-3] Finished at 1739180524.46, Elapsed: 0.01s (device=cuda)\n",
            "[Par-Task-2] Finished at 1739180524.99, Elapsed: 0.57s (device=cpu)\n",
            "[Par-Task-4] Starting on device=cpu at 1739180525.02\n",
            "[Par-Task-4] Finished at 1739180525.43, Elapsed: 0.41s (device=cpu)\n",
            "[Parallel Execution] Done. Total elapsed: 1.02s\n",
            "\n",
            "Naive total time:    0.05s\n",
            "Parallel total time: 1.02s\n",
            "\n",
            "Naive results:\n",
            "   Task-1 done (shape=torch.Size([2131, 2131]), device=cuda, time=0.02s)\n",
            "   Task-2 done (shape=torch.Size([1850, 1850]), device=cuda, time=0.00s)\n",
            "   Task-3 done (shape=torch.Size([1817, 1817]), device=cuda, time=0.01s)\n",
            "   Task-4 done (shape=torch.Size([2024, 2024]), device=cuda, time=0.01s)\n",
            "\n",
            "Parallel results:\n",
            "   Par-Task-1 done (shape=torch.Size([1957, 1957]), device=cuda, time=0.01s)\n",
            "   Par-Task-2 done (shape=torch.Size([2197, 2197]), device=cpu, time=0.57s)\n",
            "   Par-Task-3 done (shape=torch.Size([2134, 2134]), device=cuda, time=0.01s)\n",
            "   Par-Task-4 done (shape=torch.Size([1994, 1994]), device=cpu, time=0.41s)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import time\n",
        "import torch\n",
        "import threading\n",
        "import queue\n",
        "from typing import Callable, List\n",
        "import random\n",
        "# import seed\n",
        "\n",
        "random.seed(42)\n",
        "\n",
        "\n",
        "\n",
        "########################################\n",
        "# Heavy Torch Function for demonstration\n",
        "########################################\n",
        "def heavy_torch_op(name: str, size: int = 2000, device: str = 'cpu'):\n",
        "    \"\"\"\n",
        "    Performs a large matrix multiplication to simulate a heavy compute workload.\n",
        "    \"\"\"\n",
        "    start = time.time()\n",
        "    print(f\"[{name}] Starting on device={device} at {start:.2f}\")\n",
        "\n",
        "    # Create random Tensors\n",
        "    a = torch.randn(size, size, device=device)\n",
        "    b = torch.randn(size, size, device=device)\n",
        "\n",
        "    # Do a matrix multiply\n",
        "    c = torch.matmul(a, b)\n",
        "\n",
        "    if device.startswith('cuda'):\n",
        "        torch.cuda.synchronize()  # ensure kernel completes before timing\n",
        "\n",
        "    end = time.time()\n",
        "    elapsed = end - start\n",
        "    print(f\"[{name}] Finished at {end:.2f}, Elapsed: {elapsed:.2f}s (device={device})\")\n",
        "\n",
        "    # Return some info\n",
        "    return f\"{name} done (shape={c.shape}, device={device}, time={elapsed:.2f}s)\"\n",
        "\n",
        "\n",
        "########################################\n",
        "# Compare Naive vs. Parallel Node Execution\n",
        "########################################\n",
        "def run_naive(tasks):\n",
        "    \"\"\"\n",
        "    Runs tasks sequentially in the main thread (naive approach).\n",
        "    tasks: list of (function, args, kwargs)\n",
        "    \"\"\"\n",
        "    print(\"\\n[Naive Execution] Starting...\")\n",
        "    start = time.time()\n",
        "    results = []\n",
        "    for i, (func, args, kwargs) in enumerate(tasks):\n",
        "        r = func(*args, **kwargs)  # run inline, blocking\n",
        "        results.append(r)\n",
        "    end = time.time()\n",
        "    elapsed = end - start\n",
        "    print(f\"[Naive Execution] Done. Total elapsed: {elapsed:.2f}s\\n\")\n",
        "    return results, elapsed\n",
        "\n",
        "\n",
        "def run_parallel_nodes(tasks, nodes):\n",
        "    \"\"\"\n",
        "    Runs tasks in parallel using multiple Node objects.\n",
        "    tasks: list of (function, args, kwargs)\n",
        "    nodes: list of Node objects\n",
        "    \"\"\"\n",
        "    print(\"\\n[Parallel Execution] Starting...\")\n",
        "    start = time.time()\n",
        "\n",
        "    # We'll assign each task to one node (if # tasks <= # nodes)\n",
        "    # or reuse nodes if we have more tasks than nodes.\n",
        "    result_queues = []\n",
        "    for i, (func, args, kwargs) in enumerate(tasks):\n",
        "        node = nodes[i % len(nodes)]  # pick a node in round-robin\n",
        "        rq = node.assign_task(func, *args, **kwargs)\n",
        "        result_queues.append(rq)\n",
        "\n",
        "    # Collect results\n",
        "    results = []\n",
        "    for rq in result_queues:\n",
        "        r = rq.get()  # block until the task is done\n",
        "        results.append(r)\n",
        "\n",
        "    end = time.time()\n",
        "    elapsed = end - start\n",
        "    print(f\"[Parallel Execution] Done. Total elapsed: {elapsed:.2f}s\\n\")\n",
        "    return results, elapsed\n",
        "\n",
        "\n",
        "def main():\n",
        "    # 1) Create a set of tasks (function + args)\n",
        "    # For demonstration, let's generate 4 tasks that do a matrix multiply.\n",
        "    # We'll randomize the matrix size a bit so they differ slightly.\n",
        "    tasks = []\n",
        "    for i in range(4):\n",
        "        size = random.randint(1000, 3200)  # a bit of variety\n",
        "        # size = random\n",
        "        name = f\"Task-{i+1}\"\n",
        "        tasks.append((heavy_torch_op, (name, size), {'device': 'cpu'}))  # We'll keep device='cpu' for the naive test\n",
        "\n",
        "    # 2) Run them naively (sequentially)\n",
        "    naive_results, naive_time = run_naive(tasks)\n",
        "\n",
        "    # 3) Discover nodes for parallel approach\n",
        "    # If you have a GPU, it will appear here. Otherwise, you'll see multiple CPU nodes.\n",
        "    nodes = Node.discover_nodes(disjoint=True)\n",
        "    print(nodes)\n",
        "    if not nodes:\n",
        "        print(\"No nodes discovered! Exiting parallel test.\")\n",
        "        return\n",
        "\n",
        "    # For parallel test, let's do the same tasks but allow them to run on their assigned nodes\n",
        "    # We can choose device based on the node's GPU, or keep CPU.\n",
        "    # Let's create the tasks again but pick device by node's GPU availability.\n",
        "    # We'll create a new tasks list for the parallel approach:\n",
        "    parallel_tasks = []\n",
        "    for i in range(4):\n",
        "        size = random.randint(1000, 3200)\n",
        "        name = f\"Par-Task-{i+1}\"\n",
        "        # We'll decide device at runtime, so we'll store 'device'='cpu' or 'cuda' in the node assign call:\n",
        "        # Actually, let's just define the function+args now, but we won't set device here.\n",
        "        # We'll specify device in a moment by partial application, or a closure.\n",
        "        # For simplicity, let's just do CPU again, or we can do a small trick:\n",
        "        parallel_tasks.append((heavy_torch_op, (name, size), {}))  # device to be set in the code.\n",
        "\n",
        "    # We'll pick a node for each task. If it has a GPU, we set device='cuda'.\n",
        "    # We'll do that in run_parallel_nodes to keep it consistent.\n",
        "\n",
        "    # Actually let's do a custom approach:\n",
        "    # We'll modify the tasks list so that each item has the correct device\n",
        "    final_parallel_tasks = []\n",
        "    for i, tinfo in enumerate(parallel_tasks):\n",
        "        func, args, kwargs = tinfo\n",
        "        node = nodes[i % len(nodes)]  # node to be used\n",
        "        device = 'cuda' if (node.gpu is not None and torch.cuda.is_available()) else 'cpu'\n",
        "        # Make a new args tuple\n",
        "        name, size = args\n",
        "        new_args = (name, size)\n",
        "        new_kwargs = {'device': device}\n",
        "        final_parallel_tasks.append((func, new_args, new_kwargs))\n",
        "\n",
        "    # 4) Run them in parallel on the discovered nodes\n",
        "    parallel_results, parallel_time = run_parallel_nodes(final_parallel_tasks, nodes)\n",
        "\n",
        "    # 5) Compare times\n",
        "    print(f\"Naive total time:    {naive_time:.2f}s\")\n",
        "    print(f\"Parallel total time: {parallel_time:.2f}s\")\n",
        "\n",
        "    # 6) Shut down nodes\n",
        "    for n in nodes:\n",
        "        n.stop()\n",
        "\n",
        "    # Print final results\n",
        "    print(\"\\nNaive results:\")\n",
        "    for r in naive_results:\n",
        "        print(\"  \", r)\n",
        "\n",
        "    print(\"\\nParallel results:\")\n",
        "    for r in parallel_results:\n",
        "        print(\"  \", r)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "id": "dwdbWu_eOpUB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "26119693-f1ff-4745-f75b-0a3337b7a7d4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "[Naive Execution] Starting...\n",
            "[Task-1] Starting on device=cpu at 1739264751.07\n",
            "[Task-1] Finished at 1739264751.30, Elapsed: 0.22s (device=cpu)\n",
            "[Task-2] Starting on device=cpu at 1739264751.30\n",
            "[Task-2] Finished at 1739264751.36, Elapsed: 0.06s (device=cpu)\n",
            "[Task-3] Starting on device=cpu at 1739264751.36\n",
            "[Task-3] Finished at 1739264751.72, Elapsed: 0.36s (device=cpu)\n",
            "[Task-4] Starting on device=cpu at 1739264751.73\n",
            "[Task-4] Finished at 1739264752.04, Elapsed: 0.31s (device=cpu)\n",
            "[Naive Execution] Done. Total elapsed: 0.97s\n",
            "\n",
            "[Node(CPU-0, cpus=(0,), gpu=None), Node(CPU-1, cpus=(1,), gpu=None)]\n",
            "\n",
            "[Parallel Execution] Starting...\n",
            "[Par-Task-2] Starting on device=cpu at 1739264752.04\n",
            "[Par-Task-1] Starting on device=cpu at 1739264752.05\n",
            "[Par-Task-2] Finished at 1739264752.34, Elapsed: 0.30s (device=cpu)\n",
            "[Par-Task-4] Starting on device=cpu at 1739264752.35\n",
            "[Par-Task-4] Finished at 1739264752.53, Elapsed: 0.18s (device=cpu)\n",
            "[Par-Task-1] Finished at 1739264752.55, Elapsed: 0.51s (device=cpu)\n",
            "[Par-Task-3] Starting on device=cpu at 1739264752.55\n",
            "[Par-Task-3] Finished at 1739264752.66, Elapsed: 0.11s (device=cpu)\n",
            "[Parallel Execution] Done. Total elapsed: 0.62s\n",
            "\n",
            "Naive total time:    0.97s\n",
            "Parallel total time: 0.62s\n",
            "\n",
            "Naive results:\n",
            "   Task-1 done (shape=torch.Size([1456, 1456]), device=cpu, time=0.22s)\n",
            "   Task-2 done (shape=torch.Size([1102, 1102]), device=cpu, time=0.06s)\n",
            "   Task-3 done (shape=torch.Size([2126, 2126]), device=cpu, time=0.36s)\n",
            "   Task-4 done (shape=torch.Size([2003, 2003]), device=cpu, time=0.31s)\n",
            "\n",
            "Parallel results:\n",
            "   Par-Task-1 done (shape=torch.Size([1914, 1914]), device=cpu, time=0.51s)\n",
            "   Par-Task-2 done (shape=torch.Size([1571, 1571]), device=cpu, time=0.30s)\n",
            "   Par-Task-3 done (shape=torch.Size([1419, 1419]), device=cpu, time=0.11s)\n",
            "   Par-Task-4 done (shape=torch.Size([1356, 1356]), device=cpu, time=0.18s)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "GIX1m3XnQXIB"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}